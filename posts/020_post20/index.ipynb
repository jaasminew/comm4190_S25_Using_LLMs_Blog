{
 "cells": [
  {
   "cell_type": "raw",
   "id": "5c5987ae-b4fa-40c6-b512-4348324268e3",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Experimenting with a smaller LLM for ideating\"\n",
    "description: \"I decided to run the same task using a smaller LLM on Ollama and compare the quality of its output to that generated by GPT-4o.\"\n",
    "author: \"Jasmine Wang\"\n",
    "date: \"5/07/2025\"\n",
    "categories:\n",
    "  - product development\n",
    "  - creativity\n",
    "  - ideation\n",
    "  - reflections\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fcad57-4bc5-40c3-8868-f07981eb53e0",
   "metadata": {},
   "source": [
    "## To make the cost manageable, what else can I do?\n",
    "Throughout this course, I dedicated a significant amount of time to building my ideation tool—a project I became deeply invested in and plan to continue developing even after the semester ends. As the course wraps up, I began considering a shift to a smaller, more cost-effective LLM, especially for long-term development and scalability.\n",
    "\n",
    "![](hero-img.jpg)\n",
    "\n",
    "Before making that switch, however, I wanted to evaluate whether the quality of task completion from a smaller model would be comparable to GPT-4o. In this post, I’ll be documenting my testing process with the Mistral model, examining its performance in detail to see if it meets the standards I’ve set during my work with GPT-4o.\n",
    "\n",
    "#### Choosing the model\n",
    "There are several high-quality small-scale LLMs available on Ollama—such as Llama2, Mistral, Gemma, and Phi-2—and I wanted to choose the one most aligned with the goals of my ideation tool. After evaluating their respective strengths, I ultimately decided to use Mistral 7B.\n",
    "\n",
    "Mistral 7B stands out as an excellent all-purpose model, offering a strong combination of reasoning power and language generation quality, especially given its relatively compact size. Among the models I considered, it offered the best balance between performance, creativity, and local efficiency, which is crucial for generating diverse and innovative product ideas.\n",
    "\n",
    "#### Running Mistral 7B locally\n",
    "I wrote a simplified version of my original code using the LangGraph framework. This version replicates the essential functionality of the original, including idea generation, concept expansion, and concept combination, while being more lightweight and easier to test and refine.\n",
    "\n",
    "![](code-snippet.png)\n",
    "\n",
    "Then I started running Mistral locally.\n",
    "\n",
    "![](running-mistral.png)\n",
    "\n",
    "#### Formating problem\n",
    "Problem quickly showed up when I asked the LLM to do ideation through exploring emotional root causes. Based on error message, it seems that Mistral was able to generate output but it didn't perfectly follow the formating requirements.\n",
    "\n",
    "```\n",
    "    ValueError: Invalid format specifier ' \"Fear of letting others down\",\n",
    "      \"explanation\": \"Social and professional pressures can make people fear judgment from peers.\",\n",
    "      \"productDirection\": \"Use progress-sharing with supportive feedback loops instead of performance scores.\"\n",
    "    ' for object of type 'str'\n",
    "```\n",
    "\n",
    "#### Simplier output but similar content overall\n",
    "Once I fixed the formatting issue by providing a more explicit sample output, the code ran smoothly. Since I was working with a simplified version of the original code, Mistral wasn’t able to produce highly detailed responses. However, after comparing the actual content generated, I noticed that LLMs tend to produce fairly similar outputs, regardless of model size—especially when the task is well-structured.\n",
    "\n",
    "However, the combined product feature didn’t perform as well. Due to the simplified structure of the concept branches, the resulting product ideas felt a bit cliché and lacked originality—nowhere near the level of quality you’d expect from a successful, commercially viable concept.\n",
    "\n",
    "```\n",
    "- b8: Real-time Feedback Via Virtual Tutor\n",
    "- b9: Instant Quiz Feature\n",
    "- b10: Gamified Reward System\n",
    "- b11: Point System\n",
    "- b12: Level System\n",
    "- b13: Competitive Leaderboards\n",
    "\n",
    "Enter branch IDs to combine (space-separated, e.g., 'b1 b2 b3'): b1 b8 b11\n",
    "Created product branch b14: Lecture Quest: Gamified Virtual Tutor\n",
    "\n",
    "Created combined product: Lecture Quest: Gamified Virtual Tutor\n",
    "Explanation: Lecture Quest gamifies learning by integrating quizzes, mini-games, augmented reality elements, and an AI-powered virtual tutor that provides real-time feedback. Earning points through active engagement fuels students' motivation to stay focused during lectures.\n",
    "Features:\n",
    "- Interactive Quizzes\n",
    "- Mini-Games\n",
    "- Augmented Reality Elements\n",
    "- AI-Powered Virtual Tutor for Real-Time Feedback\n",
    "- Dynamic Point System\n",
    "```\n",
    "\n",
    "#### Mistral has the potential to produce higher quality ideas with better prompt engineering\n",
    "Based on my current tests, I was genuinely impressed by Mistral’s ability to generate output of comparable quality to larger models—despite its smaller scale. This demonstrates that with the right setup, smaller LLMs can be highly effective for creative tasks like product ideation.\n",
    "\n",
    "There’s still plenty of room for further exploration—refining prompts, enforcing more structured outputs, and experimenting with other lightweight models. But this trial reinforces my belief that with well-crafted prompts, smaller LLMs like Mistral are more than capable of handling complex tasks efficiently and affordably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd6642e-3869-44fa-9a0c-6b3a5000432d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "citation-manager": {
   "items": {}
  },
  "kernelspec": {
   "display_name": "Python 3-12 (COMM4190)",
   "language": "python",
   "name": "python3-12_comm4190"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
