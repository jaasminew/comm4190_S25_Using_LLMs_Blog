[
  {
    "objectID": "posts/002_post2/index.html",
    "href": "posts/002_post2/index.html",
    "title": "An Investigation Into the Creativity of LLMs Through Ideation 2",
    "section": "",
    "text": "After experimenting with different collaborative roles for LLMs, my confidence in human creativity has only grown stronger. While Artificial Intelligence might be exceptionally good at synthesizing information and generating small, creative ideas, it still struggles to produce truly unique business concepts with real commercial potential.\nHowever, LLMs are excellent prompters–they are the perfect guide for brainstorming, pushing us to explore ideas more deeply and venture further down the rabbit hole.\n\n\n\nThis time, rather than directly asking LLM to generate ideas, I switched our roles and told it to become my prompter by asking thought-proviking questions. With base prompt the same, I added the following:\n\nI want you to do the following: ask me 5 questions that you think might be inspiring in this field and have the potential to expand my thoughts. Based on my answers, you can ask follow-up questions if needed. The questions can be about the context, my personal experience, creative process, interdisciplinary fields, or any aspects that you think might help with the idea generation in the context I provided above.\n\nInitially, GPT started with some generic questions, following a typical brainstorming procedure. From top to bottom, each question digs deeper and asks something more specific and tangible. One interesting point worth mentioning is the word templatable in the fifth question. It precisely captures the essence of the context I provided, even though I never used it in my original prompt. I was impressed by how well the LLM understood and summarized my content, and inferred something implicit from our conversation.\n\nAfter I answered the five questions, GPT provided additional feedback by highlighting key points from my responses and posing follow-up questions. Unlike the previous experiment, where GPT took the dominant role in ideation, this time I contributed more information infused with human perspectives and loosely structured ideas. Rather than generating a flood of generic “AI-powered platform” concepts, GPT followed my train of thought, engaging in a more nuanced dialogue and addressing specific questions I had raised in my previous response.\n\nI gave very positive feedback about the questions it prompted me. But this time, I asked it to not only ask me follow-up questions, but also respond to some of the questions I have left in my response and think together with me.\nThis time, each bullet point header was framed as a question. Below each question, GPT provided insightful responses, followed by several follow-up questions for me to consider. In the end, without being prompted, GPT offered a summary of final thoughts along with potential next steps.\n\n\n\n\n\nI was very satisfied with the result this time. Although we haven’t reached an ideal outcome yet, the human-machine collaboration took a promising direction. This made me reflect—what was the game changer?\nLarge Language Models integrate vast amounts of data from the internet, which means their outputs tend to be the most average—the statistically most probable responses. However, the most probable idea isn’t necessarily the most feasible, let alone the most creative. This is where human thought and contribution become essential. Unlike AI, our thinking process isn’t constrained by “word embeddings.” Some seemingly unrelated ideas—those that don’t follow predictable patterns—are often the ones that lead to true breakthroughs.\nSo, how exactly can AI support us in this space? As the saying goes, two minds are better than one. By leveraging AI’s ability to expand on concrete ideas and provide tangible solutions, it helps counteract biases and blind spots in human thinking. By flexibly combining the strengths of both human creativity and AI’s analytical capabilities, I found a more natural and effective approach to human-machine collaboration.\nIn the next post, I’ll be exploring LLMs’ strengths and weaknesses as idea refiner."
  },
  {
    "objectID": "posts/002_post2/index.html#what-is-the-game-changer",
    "href": "posts/002_post2/index.html#what-is-the-game-changer",
    "title": "An Investigation Into the Creativity of LLMs Through Ideation 2",
    "section": "",
    "text": "I was very satisfied with the result this time. Although we haven’t reached an ideal outcome yet, the human-machine collaboration took a promising direction. This made me reflect—what was the game changer?\nLarge Language Models integrate vast amounts of data from the internet, which means their outputs tend to be the most average—the statistically most probable responses. However, the most probable idea isn’t necessarily the most feasible, let alone the most creative. This is where human thought and contribution become essential. Unlike AI, our thinking process isn’t constrained by “word embeddings.” Some seemingly unrelated ideas—those that don’t follow predictable patterns—are often the ones that lead to true breakthroughs.\nSo, how exactly can AI support us in this space? As the saying goes, two minds are better than one. By leveraging AI’s ability to expand on concrete ideas and provide tangible solutions, it helps counteract biases and blind spots in human thinking. By flexibly combining the strengths of both human creativity and AI’s analytical capabilities, I found a more natural and effective approach to human-machine collaboration.\nIn the next post, I’ll be exploring LLMs’ strengths and weaknesses as idea refiner."
  },
  {
    "objectID": "posts/003_post3/index.html",
    "href": "posts/003_post3/index.html",
    "title": "An Investigation Into the Creativity of LLMs Through Ideation 3",
    "section": "",
    "text": "In my third attempt, I focused on using LLMs as an idea refiner. Initially, I was disappointed by the lack of variation in GPT’s output. However, as the conversation progressed, I was once again impressed by its ability to address my concerns and support its viewpoints with relevant market insights drawn from its data.\nBeyond generating a promising commercial idea, we collaborated to craft a compelling pitch statement—one that could be used for marketing and presenting to early investors.\n\n\n\nTo let LLM plays the refiner role, I reworked my prompt (the base prompt remains the same):\n\nI want to come up with a profitable product idea and want you to be my helper for this brainstorming session. I’m the one who will be providing the ideas, but you are responsible for improving how ideas are expressed, in order to maximize our product attraction to potential customers. You are also responsible for filling in the details, such as features or unique selling points, if needed. Your goal is to effectively communicate the idea and persuade the potential customers and investors to buy the product or make the investment.\n\nThis time, the dynamic shifted—the responsibility of generating ideas fell on me. Unlike before, the idea pool remained within the two concepts I provided. Instead of prioritizing breadth, we focused on depth, thoroughly examining and defending every possible argument that users or investors might raise.\n\n\nI wasn’t very satisfied at first, as the output didn’t differ much from what I had received in my initial attempt. Even after refinement, the ideas remained generic and unconvincing. Based on my previous interactions with LLMs, I decided not to ask for further elaboration or expansion. Instead, I structured my prompt to focus on the skepticism raised by the “imaginary” investors, and asked for its feedback:\n\nI presented these ideas to early investors, but they expressed skepticism about several aspects: the high level of market competition, my ability to sustain a large network of designers who are consistently available to take orders, and AI’s effectiveness in generating branding assets and UI in a truly valuable way. Please help me craft responses to address these concerns, using data and reasoning to demonstrate the potential of both ideas.\n\nThis time, it worked much better as we shifted from a broad landscape to a few highly specific concerns—an area where GPT seems to excel. By addressing each concern directly, GPT provided insightful feedback with clear reasoning, market validation, and key points to help persuade investors. For example, when countering concerns about sustaining a large network of designers, it argued that “we don’t need a huge designer pool to succeed–we just need smart workflow management to match demand and supply efficiently.”\n\nThen I asked a couple follow-up questions, pushing back once again by stating some potential skepticisms from investors.\n\nGPT maintained a structured and clear approach in its responses. As I reviewed them, I found most to be compelling and well-reasoned—strong enough to be included in a pitch deck with some refinements. In the end, GPT delivered a closing statement that effectively addressed the three key concerns I had raised.\n\n\n\n\n\nBased on my three rounds of experiments, I summarized several key takeaways when using LLMs as ideating partner:\n\nLLMs can be valuable ideation partners, especially when acting as prompters and refiners. By leveraging both roles at different stages—initial brainstorming and refinement—humans can generate more compelling and feasible ideas.\nThe base prompt has to be concrete and specific, providing the LLMs with sufficient context about target market and problem space.\nHumans should remain the driving force in ideation. The experiments have shown that AI tends to produce safe, average choices, while the ability to generate truly innovative, breakthrough ideas remains unique to human creativity due to our free-flowing thinking patterns.\nDuring the ideating process, think actively with LLM and ask specific, rather than open-ended, questions. The more information and personal insights you feed LLMs, the more ideal your output might be.\nWhen prompting LLMs, focus on concrete elements such as features, challenges, and actionable suggestions to get the most useful insights.\nLeverage LLMs to identify blind spots by constantly challenging their responses and prompting them with counterarguments and imaginary refutations."
  },
  {
    "objectID": "posts/003_post3/index.html#summary-how-should-we-use-llms-in-ideation",
    "href": "posts/003_post3/index.html#summary-how-should-we-use-llms-in-ideation",
    "title": "An Investigation Into the Creativity of LLMs Through Ideation 3",
    "section": "",
    "text": "Based on my three rounds of experiments, I summarized several key takeaways when using LLMs as ideating partner:\n\nLLMs can be valuable ideation partners, especially when acting as prompters and refiners. By leveraging both roles at different stages—initial brainstorming and refinement—humans can generate more compelling and feasible ideas.\nThe base prompt has to be concrete and specific, providing the LLMs with sufficient context about target market and problem space.\nHumans should remain the driving force in ideation. The experiments have shown that AI tends to produce safe, average choices, while the ability to generate truly innovative, breakthrough ideas remains unique to human creativity due to our free-flowing thinking patterns.\nDuring the ideating process, think actively with LLM and ask specific, rather than open-ended, questions. The more information and personal insights you feed LLMs, the more ideal your output might be.\nWhen prompting LLMs, focus on concrete elements such as features, challenges, and actionable suggestions to get the most useful insights.\nLeverage LLMs to identify blind spots by constantly challenging their responses and prompting them with counterarguments and imaginary refutations."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Explorations with LLMs",
    "section": "",
    "text": "An Investigation Into the Creativity of LLMs Through Ideation 3\n\n\n\n\n\n\nLLMs\n\n\nprompting\n\n\ncreativity\n\n\nideation\n\n\n\nTesting the strengths and constraints of LLMs ideating capability\n\n\n\n\n\nFeb 2, 2025\n\n\nJasmine Wang\n\n\n\n\n\n\n\n\n\n\n\n\nAn Investigation Into the Creativity of LLMs Through Ideation 2\n\n\n\n\n\n\nLLMs\n\n\nprompting\n\n\ncreativity\n\n\nideation\n\n\n\nTesting the strengths and constraints of LLMs ideating capability\n\n\n\n\n\nFeb 1, 2025\n\n\nJasmine Wang\n\n\n\n\n\n\n\n\n\n\n\n\nAn Investigation Into the Creativity of LLMs Through Ideation\n\n\n\n\n\n\nLLMs\n\n\nprompting\n\n\ncreativity\n\n\nideation\n\n\n\nTesting the strengths and constraints of LLMs ideating capability\n\n\n\n\n\nJan 30, 2025\n\n\nJasmine Wang\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/001_post1/index.html",
    "href": "posts/001_post1/index.html",
    "title": "An Investigation Into the Creativity of LLMs Through Ideation",
    "section": "",
    "text": "Large Language Models (LLMs) have been intensively used in a lot of productivity settings, and they are notably good at data analysis, problem solving, and knowledge retrieval and synthesis. While they are often assumed to be useful for idea generation, their actual strengths and limitations in producing original and feasible ideas remain underexplored.\nDrawing inspiration from research on creativity in Artificial Intelligence tools, I set out to investigate LLMs’ capabilities in creative idea generation through a human-machine collaborative ideation session. Through multiple experiments, I aim to understand which collaborative role played by LLM is the most effective and inspring in the ideation setting.\n\n\n\nFor the experiments, I will be testing a mix of models to compare their capabilities, biases, and effectiveness in different ideation roles.\n\nGPT-4 (OpenAI)\nClaude 3 (Anthropic)\nDeepSeek-V2 (DeepSeek AI)\n\n\n\n\nTo break down LLMs’ contributions to idea generation, I categorize their roles into Generator, Prompter, and Refiner.\n\nThe Generator role puts the LLM in the driver’s seat, allowing it to take full control of idea creation with minimal human input.\nThe Prompter role flips the dynamic, positioning the LLM as a guide that stimulates human creativity by posing thought-provoking questions and suggestions.\nThe Refiner role focuses on polishing and enhancing existing ideas given by human, helping to improve clarity, coherence, and feasibility.\n\nBy testing these roles across different models, I aim to uncover which approach is the most effective in a collaborative ideation setting.\n\n\n\nThe base prompt:\n\nMany small-scale startup founders have strong technical skills but lack design expertise within their teams. They need cost-effective, high-quality product and graphic design support but have a limited budget. Hiring a full-time product or graphic designer is not cost-effective for them at this stage, as they are still testing their MVP and validating product-market fit.\nTheir primary goal is to get fundamental design work done—whether for their product UI, branding, or marketing—so they can present a functional and visually coherent version to early users and investors. They are not looking for perfect, highly polished design but rather a solid starting point that can be refined later once they achieve growth.\nPlease generate profitable product ideas (tools, platforms, or services) that effectively address this gap. The solution should be: - Descent quality of design that is visually appealing and credible enough to engage early users and investors - Affordable compared to hiring a dedicated designer - Fast and efficient for founders who need quick, functional results - Scalable so it remains profitable while serving multiple startups\nThe target customers are small-scale startup founders who prioritize functionality over perfection but still want fair-quality design to establish credibility in the early stages.\n\n\n\nOn top of the base prompt, I told the LLM that &gt;“I want you to take the dominant role in the idea generation process, meaning that you are the one who’s responsible for thinking of as many ideas as possible, while also trying to maintain the originality and feasibility of the idea. I’m going to be the judge, and will reward you if the ideas you are generating satisfy the standards I described above.”\nI started with GPT-4. For the first run, it came up with 8 ideas. All ideas are of fair quality, meaning that they possess the basic feasibility and attractiveness to early users and investors. The response follows a well-organized structure, consisting a description, key features, a revenue model, and an explanation of why it works.\nHowever, most ideas are very generic and homogeneous. Other than being framed differently, the core of the ideas remains the same. Boiling down to its core, the 8 ideas can be reorganized and combined into 3. All of them except 2 are heavily AI-focused, with very similar p/ roduct offerings.\nHere’s a glimpse of GPT-4’s response.\n\nThen I manually selected two ideas and prompted the model to expand on them. This time, I provided a clear structure and asked the model to self-evaluate its own responses based on originality, feasibility, and attractiveness. Additionally, I requested a feasibility analysis, outlining the top technical or social challenges and potential solutions. In the end, the model generated an overall idea quality summary, synthesizing the ratings across different aspects.\n\n\n\n\nI tested the same prompt in Claude 3 and Deepseek-v2, each yielding very similar ideas and responses.\n\n\nBased on the test results, I found that LLMs are more effective at expanding concrete ideas—answering questions and elaborating on details—rather than generating truly innovative or groundbreaking concepts. During the ideation process, the ideas generated by different Large Language Models tended to be quite homogeneous, with similar core offerings and technologies appearing across multiple suggestions.\nHowever, LLMs are exceptionally good at idea expansion, helping to identify blind spots that humans might initially overlook. When I asked the model to list key and stretch features, along with potential challenges, its insights were particularly valuable in providing a comprehensive and nuanced understanding of the market landscape.\nFrom this observation, my key takeaways are: * When crafting prompts, place strong emphasis on defining your target market and customers. The more niche and specific the problem space, the more original and unique the generated ideas will be. * Embrace LLMs as great helpers in the initial brainstorming stage. While they may not be the best at generating breakthrough ideas, they provide valuable insights on product development that humans might easily overlook.\nIn the next post, I’ll be introducing LLMs’ strengths and weaknesses as prompter."
  },
  {
    "objectID": "posts/001_post1/index.html#the-models-im-testing-with",
    "href": "posts/001_post1/index.html#the-models-im-testing-with",
    "title": "An Investigation Into the Creativity of LLMs Through Ideation",
    "section": "",
    "text": "For the experiments, I will be testing a mix of models to compare their capabilities, biases, and effectiveness in different ideation roles.\n\nGPT-4 (OpenAI)\nClaude 3 (Anthropic)\nDeepSeek-V2 (DeepSeek AI)"
  },
  {
    "objectID": "posts/001_post1/index.html#llm-collaborative-roles-in-human-machine-collaboration",
    "href": "posts/001_post1/index.html#llm-collaborative-roles-in-human-machine-collaboration",
    "title": "An Investigation Into the Creativity of LLMs Through Ideation",
    "section": "",
    "text": "To break down LLMs’ contributions to idea generation, I categorize their roles into Generator, Prompter, and Refiner.\n\nThe Generator role puts the LLM in the driver’s seat, allowing it to take full control of idea creation with minimal human input.\nThe Prompter role flips the dynamic, positioning the LLM as a guide that stimulates human creativity by posing thought-provoking questions and suggestions.\nThe Refiner role focuses on polishing and enhancing existing ideas given by human, helping to improve clarity, coherence, and feasibility.\n\nBy testing these roles across different models, I aim to uncover which approach is the most effective in a collaborative ideation setting."
  },
  {
    "objectID": "posts/001_post1/index.html#experiments-with-llms",
    "href": "posts/001_post1/index.html#experiments-with-llms",
    "title": "An Investigation Into the Creativity of LLMs Through Ideation",
    "section": "",
    "text": "The base prompt:\n\nMany small-scale startup founders have strong technical skills but lack design expertise within their teams. They need cost-effective, high-quality product and graphic design support but have a limited budget. Hiring a full-time product or graphic designer is not cost-effective for them at this stage, as they are still testing their MVP and validating product-market fit.\nTheir primary goal is to get fundamental design work done—whether for their product UI, branding, or marketing—so they can present a functional and visually coherent version to early users and investors. They are not looking for perfect, highly polished design but rather a solid starting point that can be refined later once they achieve growth.\nPlease generate profitable product ideas (tools, platforms, or services) that effectively address this gap. The solution should be: - Descent quality of design that is visually appealing and credible enough to engage early users and investors - Affordable compared to hiring a dedicated designer - Fast and efficient for founders who need quick, functional results - Scalable so it remains profitable while serving multiple startups\nThe target customers are small-scale startup founders who prioritize functionality over perfection but still want fair-quality design to establish credibility in the early stages.\n\n\n\nOn top of the base prompt, I told the LLM that &gt;“I want you to take the dominant role in the idea generation process, meaning that you are the one who’s responsible for thinking of as many ideas as possible, while also trying to maintain the originality and feasibility of the idea. I’m going to be the judge, and will reward you if the ideas you are generating satisfy the standards I described above.”\nI started with GPT-4. For the first run, it came up with 8 ideas. All ideas are of fair quality, meaning that they possess the basic feasibility and attractiveness to early users and investors. The response follows a well-organized structure, consisting a description, key features, a revenue model, and an explanation of why it works.\nHowever, most ideas are very generic and homogeneous. Other than being framed differently, the core of the ideas remains the same. Boiling down to its core, the 8 ideas can be reorganized and combined into 3. All of them except 2 are heavily AI-focused, with very similar p/ roduct offerings.\nHere’s a glimpse of GPT-4’s response.\n\nThen I manually selected two ideas and prompted the model to expand on them. This time, I provided a clear structure and asked the model to self-evaluate its own responses based on originality, feasibility, and attractiveness. Additionally, I requested a feasibility analysis, outlining the top technical or social challenges and potential solutions. In the end, the model generated an overall idea quality summary, synthesizing the ratings across different aspects.\n\n\n\n\nI tested the same prompt in Claude 3 and Deepseek-v2, each yielding very similar ideas and responses.\n\n\nBased on the test results, I found that LLMs are more effective at expanding concrete ideas—answering questions and elaborating on details—rather than generating truly innovative or groundbreaking concepts. During the ideation process, the ideas generated by different Large Language Models tended to be quite homogeneous, with similar core offerings and technologies appearing across multiple suggestions.\nHowever, LLMs are exceptionally good at idea expansion, helping to identify blind spots that humans might initially overlook. When I asked the model to list key and stretch features, along with potential challenges, its insights were particularly valuable in providing a comprehensive and nuanced understanding of the market landscape.\nFrom this observation, my key takeaways are: * When crafting prompts, place strong emphasis on defining your target market and customers. The more niche and specific the problem space, the more original and unique the generated ideas will be. * Embrace LLMs as great helpers in the initial brainstorming stage. While they may not be the best at generating breakthrough ideas, they provide valuable insights on product development that humans might easily overlook.\nIn the next post, I’ll be introducing LLMs’ strengths and weaknesses as prompter."
  }
]