[
  {
    "objectID": "posts/000_test_post/index.html",
    "href": "posts/000_test_post/index.html",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purple‚Ä¶\nBut it is possible you first said green (don‚Äôt worry if you did it is quite a common response!)\n\n\nIt‚Äôs a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | ‚Äî | ‚Äî | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt get‚Äôs it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. üü™\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, I‚Äôll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  },
  {
    "objectID": "posts/000_test_post/index.html#a-visualization-problem-for-llms",
    "href": "posts/000_test_post/index.html#a-visualization-problem-for-llms",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purple‚Ä¶\nBut it is possible you first said green (don‚Äôt worry if you did it is quite a common response!)\n\n\nIt‚Äôs a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | ‚Äî | ‚Äî | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt get‚Äôs it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. üü™\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, I‚Äôll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Explorations with LLMs",
    "section": "",
    "text": "An Investigation Into the Creativity of LLMs Through Ideation\n\n\n\n\n\n\nLLMs\n\n\nprompting\n\n\ncreativity\n\n\nideation\n\n\n\nTesting the strengths and constraints of LLMs ideating capability\n\n\n\n\n\nJan 30, 2025\n\n\nJasmine Wang\n\n\n\n\n\n\n\n\n\n\n\n\nA test post\n\n\n\n\n\n\nLLMs\n\n\nprompting\n\n\nlogic\n\n\n\nAn example post from a Jupyter notebook\n\n\n\n\n\nFeb 2, 2024\n\n\nAn LLM User\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/001_post1/index.html",
    "href": "posts/001_post1/index.html",
    "title": "An Investigation Into the Creativity of LLMs Through Ideation",
    "section": "",
    "text": "Large Language Models (LLMs) have been intensively used in a lot of productivity settings, and they are notably good at data analysis, problem solving, and knowledge retrieval and synthesis. While they are often assumed to be useful for idea generation, their actual strengths and limitations in producing original and feasible ideas remain underexplored.\nDrawing inspiration from research on creativity in Artificial Intelligence tools, I set out to investigate LLMs‚Äô capabilities in creative idea generation through a human-machine collaborative ideation session. Through multiple experiments, I aim to understand which collaborative role played by LLM is the most effective and inspring in the ideation setting.\n\n\n\nFor the experiments, I will be testing a mix of models to compare their capabilities, biases, and effectiveness in different ideation roles.\n\nGPT-4 (OpenAI)\nClaude 3 (Anthropic)\nDeepSeek-V2 (DeepSeek AI)\n\n\n\n\nTo break down LLMs‚Äô contributions to idea generation, I categorize their roles into Generator, Prompter, and Refiner.\n\nThe Generator role puts the LLM in the driver‚Äôs seat, allowing it to take full control of idea creation with minimal human input.\nThe Prompter role flips the dynamic, positioning the LLM as a guide that stimulates human creativity by posing thought-provoking questions and suggestions.\nThe Refiner role focuses on polishing and enhancing existing ideas given by human, helping to improve clarity, coherence, and feasibility.\n\nBy testing these roles across different models, I aim to uncover which approach is the most effective in a collaborative ideation setting.\n\n\n\nThe base prompt:\n\nMany small-scale startup founders have strong technical skills but lack design expertise within their teams. They need cost-effective, high-quality product and graphic design support but have a limited budget. Hiring a full-time product or graphic designer is not cost-effective for them at this stage, as they are still testing their MVP and validating product-market fit.\nTheir primary goal is to get fundamental design work done‚Äîwhether for their product UI, branding, or marketing‚Äîso they can present a functional and visually coherent version to early users and investors. They are not looking for perfect, highly polished design but rather a solid starting point that can be refined later once they achieve growth.\nPlease generate profitable product ideas (tools, platforms, or services) that effectively address this gap. The solution should be: - Descent quality of design that is visually appealing and credible enough to engage early users and investors - Affordable compared to hiring a dedicated designer - Fast and efficient for founders who need quick, functional results - Scalable so it remains profitable while serving multiple startups\nThe target customers are small-scale startup founders who prioritize functionality over perfection but still want fair-quality design to establish credibility in the early stages.\n\n\n\nOn top of the base prompt, I told the LLM that &gt;‚ÄúI want you to take the dominant role in the idea generation process, meaning that you are the one who‚Äôs responsible for thinking of as many ideas as possible, while also trying to maintain the originality and feasibility of the idea. I‚Äôm going to be the judge, and will reward you if the ideas you are generating satisfy the standards I described above.‚Äù\nI started with GPT-4. For the first run, it came up with 8 ideas. All ideas are of fair quality, meaning that they possess the basic feasibility and attractiveness to early users and investors. The response follows a well-organized structure, consisting a description, key features, a revenue model, and an explanation of why it works.\nHowever, most ideas are very generic and homogeneous. Other than being framed differently, the core of the ideas remains the same. Boiling down to its core, the 8 ideas can be reorganized and combined into 3. All of them except 2 are heavily AI-focused, with very similar product offerings.\nHere‚Äôs a glimpse of GPT-4‚Äôs response.\n\nThen I manually selected two ideas and prompted the model to expand on them. This time, I provided a clear structure and asked the model to self-evaluate its own responses based on originality, feasibility, and attractiveness. Additionally, I requested a feasibility analysis, outlining the top technical or social challenges and potential solutions. In the end, the model generated an overall idea quality summary, synthesizing the ratings across different aspects.\n\n\n\n\nI tested the same prompt in Claude 3 and Deepseek-v2, each yielding very similar ideas and responses.\n\n\nBased on the test results, I found that LLMs are more effective at expanding concrete ideas‚Äîanswering questions and elaborating on details‚Äîrather than generating truly innovative or groundbreaking concepts. During the ideation process, the ideas generated by different Large Language Models tended to be quite homogeneous, with similar core offerings and technologies appearing across multiple suggestions.\nHowever, LLMs are exceptionally good at idea expansion, helping to identify blind spots that humans might initially overlook. When I asked the model to list key and stretch features, along with potential challenges, its insights were particularly valuable in providing a comprehensive and nuanced understanding of the market landscape.\nFrom this observation, my key takeaways are: * When crafting prompts, place strong emphasis on defining your target market and customers. The more niche and specific the problem space, the more original and unique the generated ideas will be. * Embrace LLMs as great helpers in the initial brainstorming stage. While they may not be the best at generating breakthrough ideas, they provide valuable insights on product development that humans might easily overlook.\nIn the next post, I‚Äôll be introducing LLMs‚Äô strengths and weaknesses as prompter."
  },
  {
    "objectID": "posts/001_post1/index.html#the-models-im-testing-with",
    "href": "posts/001_post1/index.html#the-models-im-testing-with",
    "title": "An Investigation Into the Creativity of LLMs Through Ideation",
    "section": "",
    "text": "For the experiments, I will be testing a mix of models to compare their capabilities, biases, and effectiveness in different ideation roles.\n\nGPT-4 (OpenAI)\nClaude 3 (Anthropic)\nDeepSeek-V2 (DeepSeek AI)"
  },
  {
    "objectID": "posts/001_post1/index.html#llm-collaborative-roles-in-human-machine-collaboration",
    "href": "posts/001_post1/index.html#llm-collaborative-roles-in-human-machine-collaboration",
    "title": "An Investigation Into the Creativity of LLMs Through Ideation",
    "section": "",
    "text": "To break down LLMs‚Äô contributions to idea generation, I categorize their roles into Generator, Prompter, and Refiner.\n\nThe Generator role puts the LLM in the driver‚Äôs seat, allowing it to take full control of idea creation with minimal human input.\nThe Prompter role flips the dynamic, positioning the LLM as a guide that stimulates human creativity by posing thought-provoking questions and suggestions.\nThe Refiner role focuses on polishing and enhancing existing ideas given by human, helping to improve clarity, coherence, and feasibility.\n\nBy testing these roles across different models, I aim to uncover which approach is the most effective in a collaborative ideation setting."
  },
  {
    "objectID": "posts/001_post1/index.html#experiments-with-llms",
    "href": "posts/001_post1/index.html#experiments-with-llms",
    "title": "An Investigation Into the Creativity of LLMs Through Ideation",
    "section": "",
    "text": "The base prompt:\n\nMany small-scale startup founders have strong technical skills but lack design expertise within their teams. They need cost-effective, high-quality product and graphic design support but have a limited budget. Hiring a full-time product or graphic designer is not cost-effective for them at this stage, as they are still testing their MVP and validating product-market fit.\nTheir primary goal is to get fundamental design work done‚Äîwhether for their product UI, branding, or marketing‚Äîso they can present a functional and visually coherent version to early users and investors. They are not looking for perfect, highly polished design but rather a solid starting point that can be refined later once they achieve growth.\nPlease generate profitable product ideas (tools, platforms, or services) that effectively address this gap. The solution should be: - Descent quality of design that is visually appealing and credible enough to engage early users and investors - Affordable compared to hiring a dedicated designer - Fast and efficient for founders who need quick, functional results - Scalable so it remains profitable while serving multiple startups\nThe target customers are small-scale startup founders who prioritize functionality over perfection but still want fair-quality design to establish credibility in the early stages.\n\n\n\nOn top of the base prompt, I told the LLM that &gt;‚ÄúI want you to take the dominant role in the idea generation process, meaning that you are the one who‚Äôs responsible for thinking of as many ideas as possible, while also trying to maintain the originality and feasibility of the idea. I‚Äôm going to be the judge, and will reward you if the ideas you are generating satisfy the standards I described above.‚Äù\nI started with GPT-4. For the first run, it came up with 8 ideas. All ideas are of fair quality, meaning that they possess the basic feasibility and attractiveness to early users and investors. The response follows a well-organized structure, consisting a description, key features, a revenue model, and an explanation of why it works.\nHowever, most ideas are very generic and homogeneous. Other than being framed differently, the core of the ideas remains the same. Boiling down to its core, the 8 ideas can be reorganized and combined into 3. All of them except 2 are heavily AI-focused, with very similar product offerings.\nHere‚Äôs a glimpse of GPT-4‚Äôs response.\n\nThen I manually selected two ideas and prompted the model to expand on them. This time, I provided a clear structure and asked the model to self-evaluate its own responses based on originality, feasibility, and attractiveness. Additionally, I requested a feasibility analysis, outlining the top technical or social challenges and potential solutions. In the end, the model generated an overall idea quality summary, synthesizing the ratings across different aspects.\n\n\n\n\nI tested the same prompt in Claude 3 and Deepseek-v2, each yielding very similar ideas and responses.\n\n\nBased on the test results, I found that LLMs are more effective at expanding concrete ideas‚Äîanswering questions and elaborating on details‚Äîrather than generating truly innovative or groundbreaking concepts. During the ideation process, the ideas generated by different Large Language Models tended to be quite homogeneous, with similar core offerings and technologies appearing across multiple suggestions.\nHowever, LLMs are exceptionally good at idea expansion, helping to identify blind spots that humans might initially overlook. When I asked the model to list key and stretch features, along with potential challenges, its insights were particularly valuable in providing a comprehensive and nuanced understanding of the market landscape.\nFrom this observation, my key takeaways are: * When crafting prompts, place strong emphasis on defining your target market and customers. The more niche and specific the problem space, the more original and unique the generated ideas will be. * Embrace LLMs as great helpers in the initial brainstorming stage. While they may not be the best at generating breakthrough ideas, they provide valuable insights on product development that humans might easily overlook.\nIn the next post, I‚Äôll be introducing LLMs‚Äô strengths and weaknesses as prompter."
  }
]