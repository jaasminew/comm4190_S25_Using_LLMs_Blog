[
  {
    "objectID": "posts/005_post5/index.html",
    "href": "posts/005_post5/index.html",
    "title": "Building A Better Ideation Partner 2",
    "section": "",
    "text": "Every creative process follows a pattern. The renowned design agency IDEO was instrumental in evangelizing this Design Thinking method, a method now widely used by creative agencies and tech companies during the design stage. This approach helps break down any seemingly intricate problems into solvable steps.\nTherefore, when trying to standardize the idea generation process, I brought back this classic technique. As an ideation tool, this product currently does not include the prototype and test phases. However, the steps of empathize, define, and ideate can be effectively replicated within an LLM. By incorporating Artificial Intelligence, I want to see it bring in its own strengths and empower humans to ideate more wisely. \n\n\n\nTo start with, I explored different possible templates for ideating. To test their effectiveness, I used the same prompt to initiate an ideating process and compared the quality of various ideas across different trial runs.\nHere’s my base prompt:\n\nI want to come up with a profitable product idea. You are my partner, an extremely creative entrepreneur who had successfully designed and built many commercial products, to help me to generate the ideas.\nHere’s the context: I want to design a new type of canvas bag (tote bag) that redefine this product. The product will target 18-35 years old female consumers. The “redefine” can be on a design and aesthetic level, functional level, semantic level (ie. carry some special meaning), branding level, or even combining some technology innovations. I want the ideas to be bold and innovative.\n\n\n\nAsk GPT to generate a set number of thought-provoking questions on a given topic. Users then provide responses, which serve as a secondary input for GPT to generate follow-up questions.\nAdded prompt:\n\nI want you to do the following: Ask me 5 thought-provoking questions that you think might be inspiring, formatted in “keywords: questions”. Based on each of my response, you will ask at least 2 follow-up questions. The questions can be about the context, my personal experience, creative process, interdisciplinary fields, or any aspects that you think might help with the idea generation in the context I provided above.\n\nOverall, our conversation followed a structured question-and-answer pattern. After five rounds of Q&A, we successfully developed three high-quality ideas that I’m very satisfied with. By round three, GPT started introducing key keywords that played a crucial role in shaping our final concepts. By round four, a rough idea had already taken form, and we focused on refining it—polishing details, selecting the strongest elements, and discarding others.\nHere are some insights about the question-inspired ideating:\n\nThe strucutre of follow-up questions + idea suggestions works well: The quality of ideas significantly improved by round three, when quick suggested ideas were provided alongside the questions. More than half of my ideas were sparked by keywords in GPT’s responses. This suggests that incorporating suggested directions as helper text within the Q&A structure could further enhance the ideation process.\nKeywords are more valuable than sentences: I realized that our Q&A process was unnecessarily long, making it inefficient in terms of both time and computing power. Many responses contained “filler” sentences that weren’t directly relevant to the core ideas. Instead, a few keywords or concise sentences would be sufficient for each point.\nDetails need human input to build up: GPT is good at solving challenges. Specific, detailed ideas came up the fastest when I described my vision and left GPT determine how to achieve it.\nRepetitive ideas still exist: Another factor contributing to the lengthy responses was the repetition of ideas. Real-time evaluation is crucial to prevent non-valuable ideas from reappearing in subsequent responses.\n\n\n\n\n\nAsk GPT to identify a set number of primary attributes or components of a given product. Then, prompt it to add, remove, or modify the connections between these attributes to generate new product ideas.\nAdded prompt:\n\nI want you to do the following: Identify a set number of (both external and internal) attributes or components of canvas bag. Later in our conversation, I’ll prompt you to identify the links between those attributes or components, and generate innovative ideas by modifying, adding, or removing those links.\nFor example, primary attributes or components of a pizza include ingredients, flavor, size, temperature, price, packaging, and brand.\n\nOnce I got the list of attributes, I sent out a more specific instructions on how to add, remove, and motify the product configuration chart.\n\nNow I want you to: 1. Combine seemingly unrelated attributes, such as shape × branding or material × cultural and social meaning. For each pair, write a short paragraph explaining how these attributes might interact in a unique way. 2. Remove attributes that seem essential or question underlying assumptions. For example, does the material have to be canvas? Can branding exist without any visible logos or signals? 3. Change the nature of certain connections and reinterpret their meanings. For each part, generate five distinct suggestions with clear explanations. Once we have these, we’ll work together to refine and explore some of them in greater depth.\n\nThe ideas generated using these prompts turned out to be bolder and more unconventional, but also less realistic and harder to implement with current materials and technology.\n\nSome insights about the attributes-driven ideating:\n\nBalancing the originality and feasibility: Attribute-driven ideation allows LLMs to fully leverage their creativity by combining seemingly incompatible aspects. However, this prompting strategy can be tricky to use, as many generated ideas may be impractical and ultimately deemed non-valuable by the prompter. To solve this problem, attributes-driven ideating needs to be used cautiously along with the question-inspired ideating.\nLess user input is needed: Compared to question-inspired prompting strategy, attributes-driven ideating requires less human input. This implies a lower cognitive load for users, while also introduces more uncertainty as there is less human oversight in guiding the ideas.\n\n\n\n\nAsk GPT to generate moderately relevant interdisciplinary subjects and topics. Then, prompt it to strategically merge concepts from different domains, providing inspiration for humans to develop new ideas.\nAdded prompt:\n\nI want you to do the following: Generate 10 moderately relevant interdisciplinary topics centered around culture, creativity, art, and Gen Z personality expression. Then, strategically merge each concept with canvas bags (tote bags) to create innovative product ideas.\nFor each idea, provide a short paragraph explaining how the concepts integrate and how they generate business value.\n\nIdeating with LLMs is a process of fine-tuning the model’s output to align ideas more closely with your desired direction. Encouraging LLMs to generate loosely relevant keywords has proven highly effective. I typically select 2–3 keywords from each idea and prompt the model to elaborate only on those details. This approach functions like a funnel, gradually narrowing broad concepts into specific, niche ideas.\nThe format of each response wasn’t fully settled. But the structure of “incorporating topics” + “concepts” + “business values” proved to be a helpful and comprehensive format. Currently, LLM still tends to wrap more-than-needed details and options in its response, which can easily lead to information overload. Controlling the amount of information released and ensuring that only the most useful insights are presented is one of the major challenges.\n \nSome insights about the interdisciplinary ideating:\n\nThis approach is effective only when interdisciplinary concepts align with the intended scope: Concept merge can be very arbitrary, and most arbitraty ideas are not of high value. Setting clear path for LLMs to follow is a necessary step early in the ideating session.\nResponses are easier to control than attributes-driven approach: Similar to the previous approach, interdisciplinary ideation requires relatively less human input, primarily involving the selection and integration of independent concepts. However, responses generated through this method are more likely to be relevant and implementable, as long as a proper direction has been established.\n\n\n\n\nAsk GPT to take on the perspective of customers and generate hypothetical feedback on the product. Use this feedback as inspiration to refine existing ideas or develop new ones.\nAdded prompt:\n\nI want you to do the following: put yourself in the shoes of 18-35 years old female consumers and generate 5 customer feedback on the product. Using this feedback, summarize the pain point and its root cause. Then, based on each point, briefly explain the reason and your suggested directions to solve them. At this stage, the suggested directions don’t have to be detailed and very specific.\n\nTalking to customers and gathering their feedback is always the first step in brainstorming. Here, I experimented with using LLMs to generate hypothetical customer feedback and then prompted it to respond to its own feedback with potential solutions.\nThe advantage of this approach is that ideation begins with a clear direction, and the ideas generated are more likely to resonate with customers since they directly address pain points. To explore root causes, I used the “5 Whys” technique. However, at this stage, the LLM’s ability to apply the “5 Whys” effectively is still quite limited. With further training on instructions and data, this technique could become a valuable tool in identifying pain points during the ideation process.\n\nAs seen in the conversation, this method allows us to dive into the topic quickly, improving efficiency compared to previous trials that required exploring multiple directions before finding a suitable one. However, the trade-off is that the ideas tend to become predictable, as they are solutions that an average person could easily come up with.\nSome insights about the imagenary-customer-prompted ideating:\n\nMore suitable for technical problem statements than creative ones: Thanks to its ability to pinpoint root causes, this prompting technique is most effective for solving technical problems but falls short when tackling creativity-driven challenges.\nWorks best when incorporating other prompting techniques: The ideas generated are precise and feasible but lack originality. However, this drawback can be effectively addressed by integrating it with interdisciplinary prompting."
  },
  {
    "objectID": "posts/005_post5/index.html#the-design-thinking-method",
    "href": "posts/005_post5/index.html#the-design-thinking-method",
    "title": "Building A Better Ideation Partner 2",
    "section": "",
    "text": "Every creative process follows a pattern. The renowned design agency IDEO was instrumental in evangelizing this Design Thinking method, a method now widely used by creative agencies and tech companies during the design stage. This approach helps break down any seemingly intricate problems into solvable steps.\nTherefore, when trying to standardize the idea generation process, I brought back this classic technique. As an ideation tool, this product currently does not include the prototype and test phases. However, the steps of empathize, define, and ideate can be effectively replicated within an LLM. By incorporating Artificial Intelligence, I want to see it bring in its own strengths and empower humans to ideate more wisely."
  },
  {
    "objectID": "posts/005_post5/index.html#experimentation-with-different-approaches",
    "href": "posts/005_post5/index.html#experimentation-with-different-approaches",
    "title": "Building A Better Ideation Partner 2",
    "section": "",
    "text": "To start with, I explored different possible templates for ideating. To test their effectiveness, I used the same prompt to initiate an ideating process and compared the quality of various ideas across different trial runs.\nHere’s my base prompt:\n\nI want to come up with a profitable product idea. You are my partner, an extremely creative entrepreneur who had successfully designed and built many commercial products, to help me to generate the ideas.\nHere’s the context: I want to design a new type of canvas bag (tote bag) that redefine this product. The product will target 18-35 years old female consumers. The “redefine” can be on a design and aesthetic level, functional level, semantic level (ie. carry some special meaning), branding level, or even combining some technology innovations. I want the ideas to be bold and innovative.\n\n\n\nAsk GPT to generate a set number of thought-provoking questions on a given topic. Users then provide responses, which serve as a secondary input for GPT to generate follow-up questions.\nAdded prompt:\n\nI want you to do the following: Ask me 5 thought-provoking questions that you think might be inspiring, formatted in “keywords: questions”. Based on each of my response, you will ask at least 2 follow-up questions. The questions can be about the context, my personal experience, creative process, interdisciplinary fields, or any aspects that you think might help with the idea generation in the context I provided above.\n\nOverall, our conversation followed a structured question-and-answer pattern. After five rounds of Q&A, we successfully developed three high-quality ideas that I’m very satisfied with. By round three, GPT started introducing key keywords that played a crucial role in shaping our final concepts. By round four, a rough idea had already taken form, and we focused on refining it—polishing details, selecting the strongest elements, and discarding others.\nHere are some insights about the question-inspired ideating:\n\nThe strucutre of follow-up questions + idea suggestions works well: The quality of ideas significantly improved by round three, when quick suggested ideas were provided alongside the questions. More than half of my ideas were sparked by keywords in GPT’s responses. This suggests that incorporating suggested directions as helper text within the Q&A structure could further enhance the ideation process.\nKeywords are more valuable than sentences: I realized that our Q&A process was unnecessarily long, making it inefficient in terms of both time and computing power. Many responses contained “filler” sentences that weren’t directly relevant to the core ideas. Instead, a few keywords or concise sentences would be sufficient for each point.\nDetails need human input to build up: GPT is good at solving challenges. Specific, detailed ideas came up the fastest when I described my vision and left GPT determine how to achieve it.\nRepetitive ideas still exist: Another factor contributing to the lengthy responses was the repetition of ideas. Real-time evaluation is crucial to prevent non-valuable ideas from reappearing in subsequent responses.\n\n\n\n\n\nAsk GPT to identify a set number of primary attributes or components of a given product. Then, prompt it to add, remove, or modify the connections between these attributes to generate new product ideas.\nAdded prompt:\n\nI want you to do the following: Identify a set number of (both external and internal) attributes or components of canvas bag. Later in our conversation, I’ll prompt you to identify the links between those attributes or components, and generate innovative ideas by modifying, adding, or removing those links.\nFor example, primary attributes or components of a pizza include ingredients, flavor, size, temperature, price, packaging, and brand.\n\nOnce I got the list of attributes, I sent out a more specific instructions on how to add, remove, and motify the product configuration chart.\n\nNow I want you to: 1. Combine seemingly unrelated attributes, such as shape × branding or material × cultural and social meaning. For each pair, write a short paragraph explaining how these attributes might interact in a unique way. 2. Remove attributes that seem essential or question underlying assumptions. For example, does the material have to be canvas? Can branding exist without any visible logos or signals? 3. Change the nature of certain connections and reinterpret their meanings. For each part, generate five distinct suggestions with clear explanations. Once we have these, we’ll work together to refine and explore some of them in greater depth.\n\nThe ideas generated using these prompts turned out to be bolder and more unconventional, but also less realistic and harder to implement with current materials and technology.\n\nSome insights about the attributes-driven ideating:\n\nBalancing the originality and feasibility: Attribute-driven ideation allows LLMs to fully leverage their creativity by combining seemingly incompatible aspects. However, this prompting strategy can be tricky to use, as many generated ideas may be impractical and ultimately deemed non-valuable by the prompter. To solve this problem, attributes-driven ideating needs to be used cautiously along with the question-inspired ideating.\nLess user input is needed: Compared to question-inspired prompting strategy, attributes-driven ideating requires less human input. This implies a lower cognitive load for users, while also introduces more uncertainty as there is less human oversight in guiding the ideas.\n\n\n\n\nAsk GPT to generate moderately relevant interdisciplinary subjects and topics. Then, prompt it to strategically merge concepts from different domains, providing inspiration for humans to develop new ideas.\nAdded prompt:\n\nI want you to do the following: Generate 10 moderately relevant interdisciplinary topics centered around culture, creativity, art, and Gen Z personality expression. Then, strategically merge each concept with canvas bags (tote bags) to create innovative product ideas.\nFor each idea, provide a short paragraph explaining how the concepts integrate and how they generate business value.\n\nIdeating with LLMs is a process of fine-tuning the model’s output to align ideas more closely with your desired direction. Encouraging LLMs to generate loosely relevant keywords has proven highly effective. I typically select 2–3 keywords from each idea and prompt the model to elaborate only on those details. This approach functions like a funnel, gradually narrowing broad concepts into specific, niche ideas.\nThe format of each response wasn’t fully settled. But the structure of “incorporating topics” + “concepts” + “business values” proved to be a helpful and comprehensive format. Currently, LLM still tends to wrap more-than-needed details and options in its response, which can easily lead to information overload. Controlling the amount of information released and ensuring that only the most useful insights are presented is one of the major challenges.\n \nSome insights about the interdisciplinary ideating:\n\nThis approach is effective only when interdisciplinary concepts align with the intended scope: Concept merge can be very arbitrary, and most arbitraty ideas are not of high value. Setting clear path for LLMs to follow is a necessary step early in the ideating session.\nResponses are easier to control than attributes-driven approach: Similar to the previous approach, interdisciplinary ideation requires relatively less human input, primarily involving the selection and integration of independent concepts. However, responses generated through this method are more likely to be relevant and implementable, as long as a proper direction has been established.\n\n\n\n\nAsk GPT to take on the perspective of customers and generate hypothetical feedback on the product. Use this feedback as inspiration to refine existing ideas or develop new ones.\nAdded prompt:\n\nI want you to do the following: put yourself in the shoes of 18-35 years old female consumers and generate 5 customer feedback on the product. Using this feedback, summarize the pain point and its root cause. Then, based on each point, briefly explain the reason and your suggested directions to solve them. At this stage, the suggested directions don’t have to be detailed and very specific.\n\nTalking to customers and gathering their feedback is always the first step in brainstorming. Here, I experimented with using LLMs to generate hypothetical customer feedback and then prompted it to respond to its own feedback with potential solutions.\nThe advantage of this approach is that ideation begins with a clear direction, and the ideas generated are more likely to resonate with customers since they directly address pain points. To explore root causes, I used the “5 Whys” technique. However, at this stage, the LLM’s ability to apply the “5 Whys” effectively is still quite limited. With further training on instructions and data, this technique could become a valuable tool in identifying pain points during the ideation process.\n\nAs seen in the conversation, this method allows us to dive into the topic quickly, improving efficiency compared to previous trials that required exploring multiple directions before finding a suitable one. However, the trade-off is that the ideas tend to become predictable, as they are solutions that an average person could easily come up with.\nSome insights about the imagenary-customer-prompted ideating:\n\nMore suitable for technical problem statements than creative ones: Thanks to its ability to pinpoint root causes, this prompting technique is most effective for solving technical problems but falls short when tackling creativity-driven challenges.\nWorks best when incorporating other prompting techniques: The ideas generated are precise and feasible but lack originality. However, this drawback can be effectively addressed by integrating it with interdisciplinary prompting."
  },
  {
    "objectID": "posts/004_post4/index.html",
    "href": "posts/004_post4/index.html",
    "title": "Building A Better Ideation Partner 1",
    "section": "",
    "text": "While working with LLMs in ideation sessions, I realized that they can generate original and feasible ideas—but only with the right prompts. Inspired by this thought, I decided to develop a tool that automates and streamlines the ideation process, enhancing results beyond the basic capabilities of current LLMs.\nIn short, I want to create an ideation partner that empowers human to generate more innovative product ideas.\n\n\n\nTo achieve this, I developed a project planning timeline. By applying prompt engineering and parameter tuning, the user’s original prompt is reworked in the background using some predefined templates that I will be creating using my research findings. This approach will leverage LLMs as both prompters and refiners, striving to open up more creative possibilities for product development.\n\n\n\n\nThere are numerous papers online exploring innovation-related topics, including both product innovation templates and the role of LLMs in supporting ideation. I’ve reviewed several relevant studies and distilled their insights into valuable references for the future development of my project.\n\n\nCo-authored by Jacob Goldenberg, David Mazursky, and Sorin Solomon, this paper presents scalable inventive templates and structured frameworks for technological innovation. These templates serve as guidelines for generating innovative product ideas, regardless of the product’s content or nature.\nStep 1: Analysis of product configuration\nEvery product consists of distinct attributes and components that interact to shape its overall appearance and function. Since innovation often involves modifications to a product’s configuration, dissecting its structure can provide valuable insights into its nature and serve as a strong starting point for transformation.\nBelow is the configuration chart of an ordinary chair.\n\nStep 2: Introducing operators to change the product configuration\nA typical innovation involves completely removing a well-established assumption about a product. However, this is only part of the picture. Various operations can be applied to modify a product’s configuration. For example, linking connects previously unrelated components or attributes, while splitting removes an internal component from the system while preserving its original function.\nInnovative ideas usually come from introducing one or more operators to the product configuration.\n\nStep 3: Forming templates\nAn inventive template consists of a sequence of operators. By strategically combining the six primary operators, we can create a scalable product innovation framework—transforming the core essence of one successful idea into thousands of variations without simply imitating it.\nFor example, the template control, involving the creation of a link in the form of control of one internal component over another internal or external component, uses the operator inclusion and linking sequentially.\n\n\n\n\nThe working paper by Lennart Meincke, Ethan Mollick, and Christian Terwiesch, examines LLMs’ ability to generate diverse ideas and explores strategies for optimizing prompts to enhance idea diversity.\nBy comparing 1,000 AI-generated ideas—produced using different prompts—with those generated by student groups, the authors identified several effective prompting techniques for brainstorming. These techniques help improve idea diversity while minimizing repetition.\nChain-of-Thought (CoT) works the best\nAmong the 35 prompting techniques, Chain-of-Thought remains the most effective. This approach breaks ideation into microtasks, prompting GPT to complete each step sequentially. Using this method, the Cosine similarity—an index measuring idea diversity—most closely aligns with that of student-generated ideas.\nHere’s an example prompt: &gt;Generate new product ideas with the following requirements: The product will target college students in the United States. It should be a physical good, not a service or software. I’d like a product that could be sold at a retail price of less than about USD 50. The ideas are just ideas. The product need not yet exist, nor may it necessarily be clearly feasible. &gt; &gt;Follow these steps. Do each step, even if you think you do not need to.\n&gt;First generate a list of 100 ideas (short title only) Second, go through the list and determine whether the ideas are different and bold, modify the ideas as needed to make them bolder and more different. No two ideas should be the same. This is important! Next, give the ideas a name and combine it with a product description. The name and idea are separated by a colon and followed by a description. The idea should be expressed as a paragraph of 40-80 words. Do this step by step!\nCreative Entrepreneur prmopting technique can be helpful\nSetting GPT’s role as an extremely creative entrepreneur also yields strong results. The tool can further specify well-known personas, such as Steve Jobs or Sam Altman, to guide idea generation. Additionally, incorporating modifiers like requesting ideas to be good and bold enhances the output.\nHere’s an example prompt: &gt;You are an extremely creative entrepreneur looking to generate new product ideas. The product will target college students in the United States. It should be a physical good, not a service or software. I’d like a product that could be sold at a retail price of less than about USD 50. The ideas are just ideas. The product need not yet exist, nor may it necessarily be clearly feasible. Number all ideas and give them a name. The name and idea are separated by a colon. Please generate 100 ideas as 100 separate paragraphs. The idea should be expressed as a paragraph of 40-80 words.\n\n\n\n\nBased on the insights from papers and experiments, I summarized the following techniques for ideating environment setup:\n\nChain-of-Thought prompting: Instead of generating ideas in a single run, the brainstorming process follows an incremental approach. Ideas are developed through multiple inquiries, recombinations, and evaluations. The original user prompt is broken down into microprompts, with LLMs responding to each step individually, gradually refining the output.\nHuman-machine collaboration: In idea generation, LLMs still lag behind humans, particularly in diversity. However, given their strength in productivity, they excel at rapid searching, comparing, matching, and exploring possibilities. When combined with human creativity and unique input, this partnership creates a synergistic approach to ideation.\nIdeation as schemes: As demonstrated by IDEO and many other design agencies, the creative process can be distilled into patterns. Just as design follows a structured process, so does brainstorming. By applying the inventive templates outlined by Jacob Goldenberg, ideation can be broken down into modular steps that LLMs can systematically follow.\nSmart prompt engineering: Different prompts yield vastly different results. A generic prompt often generates average, repetitive ideas, whereas a targeted, specific prompt significantly enhances idea quality. Techniques like setting the model’s role as a creative entrepreneur can make ideas more context-specific and engaging.\n\nIn the next post, I’ll be introducing more about ideation environment setup and prompt engineering.\n\n\n\n\nMeincke, Lennart, Ethan R. Mollick, and Christian Terwiesch. “Prompting Diverse Ideas: Increasing AI Idea Variance.” arXiv preprint arXiv:2402.01727 (2024).\nGoldenberg, Jacob, David Mazursky, and Sorin Solomon. “Toward identifying the inventive templates of new products: A channeled ideation approach.” Journal of Marketing Research 36.2 (1999): 200-210."
  },
  {
    "objectID": "posts/004_post4/index.html#project-concept",
    "href": "posts/004_post4/index.html#project-concept",
    "title": "Building A Better Ideation Partner 1",
    "section": "",
    "text": "While working with LLMs in ideation sessions, I realized that they can generate original and feasible ideas—but only with the right prompts. Inspired by this thought, I decided to develop a tool that automates and streamlines the ideation process, enhancing results beyond the basic capabilities of current LLMs.\nIn short, I want to create an ideation partner that empowers human to generate more innovative product ideas."
  },
  {
    "objectID": "posts/004_post4/index.html#project-planning",
    "href": "posts/004_post4/index.html#project-planning",
    "title": "Building A Better Ideation Partner 1",
    "section": "",
    "text": "To achieve this, I developed a project planning timeline. By applying prompt engineering and parameter tuning, the user’s original prompt is reworked in the background using some predefined templates that I will be creating using my research findings. This approach will leverage LLMs as both prompters and refiners, striving to open up more creative possibilities for product development."
  },
  {
    "objectID": "posts/004_post4/index.html#research-insights",
    "href": "posts/004_post4/index.html#research-insights",
    "title": "Building A Better Ideation Partner 1",
    "section": "",
    "text": "There are numerous papers online exploring innovation-related topics, including both product innovation templates and the role of LLMs in supporting ideation. I’ve reviewed several relevant studies and distilled their insights into valuable references for the future development of my project.\n\n\nCo-authored by Jacob Goldenberg, David Mazursky, and Sorin Solomon, this paper presents scalable inventive templates and structured frameworks for technological innovation. These templates serve as guidelines for generating innovative product ideas, regardless of the product’s content or nature.\nStep 1: Analysis of product configuration\nEvery product consists of distinct attributes and components that interact to shape its overall appearance and function. Since innovation often involves modifications to a product’s configuration, dissecting its structure can provide valuable insights into its nature and serve as a strong starting point for transformation.\nBelow is the configuration chart of an ordinary chair.\n\nStep 2: Introducing operators to change the product configuration\nA typical innovation involves completely removing a well-established assumption about a product. However, this is only part of the picture. Various operations can be applied to modify a product’s configuration. For example, linking connects previously unrelated components or attributes, while splitting removes an internal component from the system while preserving its original function.\nInnovative ideas usually come from introducing one or more operators to the product configuration.\n\nStep 3: Forming templates\nAn inventive template consists of a sequence of operators. By strategically combining the six primary operators, we can create a scalable product innovation framework—transforming the core essence of one successful idea into thousands of variations without simply imitating it.\nFor example, the template control, involving the creation of a link in the form of control of one internal component over another internal or external component, uses the operator inclusion and linking sequentially.\n\n\n\n\nThe working paper by Lennart Meincke, Ethan Mollick, and Christian Terwiesch, examines LLMs’ ability to generate diverse ideas and explores strategies for optimizing prompts to enhance idea diversity.\nBy comparing 1,000 AI-generated ideas—produced using different prompts—with those generated by student groups, the authors identified several effective prompting techniques for brainstorming. These techniques help improve idea diversity while minimizing repetition.\nChain-of-Thought (CoT) works the best\nAmong the 35 prompting techniques, Chain-of-Thought remains the most effective. This approach breaks ideation into microtasks, prompting GPT to complete each step sequentially. Using this method, the Cosine similarity—an index measuring idea diversity—most closely aligns with that of student-generated ideas.\nHere’s an example prompt: &gt;Generate new product ideas with the following requirements: The product will target college students in the United States. It should be a physical good, not a service or software. I’d like a product that could be sold at a retail price of less than about USD 50. The ideas are just ideas. The product need not yet exist, nor may it necessarily be clearly feasible. &gt; &gt;Follow these steps. Do each step, even if you think you do not need to.\n&gt;First generate a list of 100 ideas (short title only) Second, go through the list and determine whether the ideas are different and bold, modify the ideas as needed to make them bolder and more different. No two ideas should be the same. This is important! Next, give the ideas a name and combine it with a product description. The name and idea are separated by a colon and followed by a description. The idea should be expressed as a paragraph of 40-80 words. Do this step by step!\nCreative Entrepreneur prmopting technique can be helpful\nSetting GPT’s role as an extremely creative entrepreneur also yields strong results. The tool can further specify well-known personas, such as Steve Jobs or Sam Altman, to guide idea generation. Additionally, incorporating modifiers like requesting ideas to be good and bold enhances the output.\nHere’s an example prompt: &gt;You are an extremely creative entrepreneur looking to generate new product ideas. The product will target college students in the United States. It should be a physical good, not a service or software. I’d like a product that could be sold at a retail price of less than about USD 50. The ideas are just ideas. The product need not yet exist, nor may it necessarily be clearly feasible. Number all ideas and give them a name. The name and idea are separated by a colon. Please generate 100 ideas as 100 separate paragraphs. The idea should be expressed as a paragraph of 40-80 words."
  },
  {
    "objectID": "posts/004_post4/index.html#insights-summary",
    "href": "posts/004_post4/index.html#insights-summary",
    "title": "Building A Better Ideation Partner 1",
    "section": "",
    "text": "Based on the insights from papers and experiments, I summarized the following techniques for ideating environment setup:\n\nChain-of-Thought prompting: Instead of generating ideas in a single run, the brainstorming process follows an incremental approach. Ideas are developed through multiple inquiries, recombinations, and evaluations. The original user prompt is broken down into microprompts, with LLMs responding to each step individually, gradually refining the output.\nHuman-machine collaboration: In idea generation, LLMs still lag behind humans, particularly in diversity. However, given their strength in productivity, they excel at rapid searching, comparing, matching, and exploring possibilities. When combined with human creativity and unique input, this partnership creates a synergistic approach to ideation.\nIdeation as schemes: As demonstrated by IDEO and many other design agencies, the creative process can be distilled into patterns. Just as design follows a structured process, so does brainstorming. By applying the inventive templates outlined by Jacob Goldenberg, ideation can be broken down into modular steps that LLMs can systematically follow.\nSmart prompt engineering: Different prompts yield vastly different results. A generic prompt often generates average, repetitive ideas, whereas a targeted, specific prompt significantly enhances idea quality. Techniques like setting the model’s role as a creative entrepreneur can make ideas more context-specific and engaging.\n\nIn the next post, I’ll be introducing more about ideation environment setup and prompt engineering."
  },
  {
    "objectID": "posts/004_post4/index.html#reference",
    "href": "posts/004_post4/index.html#reference",
    "title": "Building A Better Ideation Partner 1",
    "section": "",
    "text": "Meincke, Lennart, Ethan R. Mollick, and Christian Terwiesch. “Prompting Diverse Ideas: Increasing AI Idea Variance.” arXiv preprint arXiv:2402.01727 (2024).\nGoldenberg, Jacob, David Mazursky, and Sorin Solomon. “Toward identifying the inventive templates of new products: A channeled ideation approach.” Journal of Marketing Research 36.2 (1999): 200-210."
  },
  {
    "objectID": "posts/001_post1/index.html",
    "href": "posts/001_post1/index.html",
    "title": "An Investigation Into the Creativity of LLMs Through Ideation",
    "section": "",
    "text": "Large Language Models (LLMs) have been intensively used in a lot of productivity settings, and they are notably good at data analysis, problem solving, and knowledge retrieval and synthesis. While they are often assumed to be useful for idea generation, their actual strengths and limitations in producing original and feasible ideas remain underexplored.\nDrawing inspiration from research on creativity in Artificial Intelligence tools, I set out to investigate LLMs’ capabilities in creative idea generation through a human-machine collaborative ideation session. Through multiple experiments, I aim to understand which collaborative role played by LLM is the most effective and inspring in the ideation setting.\n\n\n\nFor the experiments, I will be testing a mix of models to compare their capabilities, biases, and effectiveness in different ideation roles.\n\nGPT-4 (OpenAI)\nClaude 3 (Anthropic)\nDeepSeek-V2 (DeepSeek AI)\n\n\n\n\nTo break down LLMs’ contributions to idea generation, I categorize their roles into Generator, Prompter, and Refiner.\n\nThe Generator role puts the LLM in the driver’s seat, allowing it to take full control of idea creation with minimal human input.\nThe Prompter role flips the dynamic, positioning the LLM as a guide that stimulates human creativity by posing thought-provoking questions and suggestions.\nThe Refiner role focuses on polishing and enhancing existing ideas given by human, helping to improve clarity, coherence, and feasibility.\n\nBy testing these roles across different models, I aim to uncover which approach is the most effective in a collaborative ideation setting.\n\n\n\nThe base prompt:\n\nMany small-scale startup founders have strong technical skills but lack design expertise within their teams. They need cost-effective, high-quality product and graphic design support but have a limited budget. Hiring a full-time product or graphic designer is not cost-effective for them at this stage, as they are still testing their MVP and validating product-market fit.\nTheir primary goal is to get fundamental design work done—whether for their product UI, branding, or marketing—so they can present a functional and visually coherent version to early users and investors. They are not looking for perfect, highly polished design but rather a solid starting point that can be refined later once they achieve growth.\nPlease generate profitable product ideas (tools, platforms, or services) that effectively address this gap. The solution should be: - Descent quality of design that is visually appealing and credible enough to engage early users and investors - Affordable compared to hiring a dedicated designer - Fast and efficient for founders who need quick, functional results - Scalable so it remains profitable while serving multiple startups\nThe target customers are small-scale startup founders who prioritize functionality over perfection but still want fair-quality design to establish credibility in the early stages.\n\n\n\nOn top of the base prompt, I told the LLM that &gt;“I want you to take the dominant role in the idea generation process, meaning that you are the one who’s responsible for thinking of as many ideas as possible, while also trying to maintain the originality and feasibility of the idea. I’m going to be the judge, and will reward you if the ideas you are generating satisfy the standards I described above.”\nI started with GPT-4. For the first run, it came up with 8 ideas. All ideas are of fair quality, meaning that they possess the basic feasibility and attractiveness to early users and investors. The response follows a well-organized structure, consisting a description, key features, a revenue model, and an explanation of why it works.\nHowever, most ideas are very generic and homogeneous. Other than being framed differently, the core of the ideas remains the same. Boiling down to its core, the 8 ideas can be reorganized and combined into 3. All of them except 2 are heavily AI-focused, with very similar p/ roduct offerings.\nHere’s a glimpse of GPT-4’s response.\n\nThen I manually selected two ideas and prompted the model to expand on them. This time, I provided a clear structure and asked the model to self-evaluate its own responses based on originality, feasibility, and attractiveness. Additionally, I requested a feasibility analysis, outlining the top technical or social challenges and potential solutions. In the end, the model generated an overall idea quality summary, synthesizing the ratings across different aspects.\n\n\n\n\nI tested the same prompt in Claude 3 and Deepseek-v2, each yielding very similar ideas and responses.\n\n\nBased on the test results, I found that LLMs are more effective at expanding concrete ideas—answering questions and elaborating on details—rather than generating truly innovative or groundbreaking concepts. During the ideation process, the ideas generated by different Large Language Models tended to be quite homogeneous, with similar core offerings and technologies appearing across multiple suggestions.\nHowever, LLMs are exceptionally good at idea expansion, helping to identify blind spots that humans might initially overlook. When I asked the model to list key and stretch features, along with potential challenges, its insights were particularly valuable in providing a comprehensive and nuanced understanding of the market landscape.\nFrom this observation, my key takeaways are: * When crafting prompts, place strong emphasis on defining your target market and customers. The more niche and specific the problem space, the more original and unique the generated ideas will be. * Embrace LLMs as great helpers in the initial brainstorming stage. While they may not be the best at generating breakthrough ideas, they provide valuable insights on product development that humans might easily overlook.\nIn the next post, I’ll be introducing LLMs’ strengths and weaknesses as prompter."
  },
  {
    "objectID": "posts/001_post1/index.html#the-models-im-testing-with",
    "href": "posts/001_post1/index.html#the-models-im-testing-with",
    "title": "An Investigation Into the Creativity of LLMs Through Ideation",
    "section": "",
    "text": "For the experiments, I will be testing a mix of models to compare their capabilities, biases, and effectiveness in different ideation roles.\n\nGPT-4 (OpenAI)\nClaude 3 (Anthropic)\nDeepSeek-V2 (DeepSeek AI)"
  },
  {
    "objectID": "posts/001_post1/index.html#llm-collaborative-roles-in-human-machine-collaboration",
    "href": "posts/001_post1/index.html#llm-collaborative-roles-in-human-machine-collaboration",
    "title": "An Investigation Into the Creativity of LLMs Through Ideation",
    "section": "",
    "text": "To break down LLMs’ contributions to idea generation, I categorize their roles into Generator, Prompter, and Refiner.\n\nThe Generator role puts the LLM in the driver’s seat, allowing it to take full control of idea creation with minimal human input.\nThe Prompter role flips the dynamic, positioning the LLM as a guide that stimulates human creativity by posing thought-provoking questions and suggestions.\nThe Refiner role focuses on polishing and enhancing existing ideas given by human, helping to improve clarity, coherence, and feasibility.\n\nBy testing these roles across different models, I aim to uncover which approach is the most effective in a collaborative ideation setting."
  },
  {
    "objectID": "posts/001_post1/index.html#experiments-with-llms",
    "href": "posts/001_post1/index.html#experiments-with-llms",
    "title": "An Investigation Into the Creativity of LLMs Through Ideation",
    "section": "",
    "text": "The base prompt:\n\nMany small-scale startup founders have strong technical skills but lack design expertise within their teams. They need cost-effective, high-quality product and graphic design support but have a limited budget. Hiring a full-time product or graphic designer is not cost-effective for them at this stage, as they are still testing their MVP and validating product-market fit.\nTheir primary goal is to get fundamental design work done—whether for their product UI, branding, or marketing—so they can present a functional and visually coherent version to early users and investors. They are not looking for perfect, highly polished design but rather a solid starting point that can be refined later once they achieve growth.\nPlease generate profitable product ideas (tools, platforms, or services) that effectively address this gap. The solution should be: - Descent quality of design that is visually appealing and credible enough to engage early users and investors - Affordable compared to hiring a dedicated designer - Fast and efficient for founders who need quick, functional results - Scalable so it remains profitable while serving multiple startups\nThe target customers are small-scale startup founders who prioritize functionality over perfection but still want fair-quality design to establish credibility in the early stages.\n\n\n\nOn top of the base prompt, I told the LLM that &gt;“I want you to take the dominant role in the idea generation process, meaning that you are the one who’s responsible for thinking of as many ideas as possible, while also trying to maintain the originality and feasibility of the idea. I’m going to be the judge, and will reward you if the ideas you are generating satisfy the standards I described above.”\nI started with GPT-4. For the first run, it came up with 8 ideas. All ideas are of fair quality, meaning that they possess the basic feasibility and attractiveness to early users and investors. The response follows a well-organized structure, consisting a description, key features, a revenue model, and an explanation of why it works.\nHowever, most ideas are very generic and homogeneous. Other than being framed differently, the core of the ideas remains the same. Boiling down to its core, the 8 ideas can be reorganized and combined into 3. All of them except 2 are heavily AI-focused, with very similar p/ roduct offerings.\nHere’s a glimpse of GPT-4’s response.\n\nThen I manually selected two ideas and prompted the model to expand on them. This time, I provided a clear structure and asked the model to self-evaluate its own responses based on originality, feasibility, and attractiveness. Additionally, I requested a feasibility analysis, outlining the top technical or social challenges and potential solutions. In the end, the model generated an overall idea quality summary, synthesizing the ratings across different aspects.\n\n\n\n\nI tested the same prompt in Claude 3 and Deepseek-v2, each yielding very similar ideas and responses.\n\n\nBased on the test results, I found that LLMs are more effective at expanding concrete ideas—answering questions and elaborating on details—rather than generating truly innovative or groundbreaking concepts. During the ideation process, the ideas generated by different Large Language Models tended to be quite homogeneous, with similar core offerings and technologies appearing across multiple suggestions.\nHowever, LLMs are exceptionally good at idea expansion, helping to identify blind spots that humans might initially overlook. When I asked the model to list key and stretch features, along with potential challenges, its insights were particularly valuable in providing a comprehensive and nuanced understanding of the market landscape.\nFrom this observation, my key takeaways are: * When crafting prompts, place strong emphasis on defining your target market and customers. The more niche and specific the problem space, the more original and unique the generated ideas will be. * Embrace LLMs as great helpers in the initial brainstorming stage. While they may not be the best at generating breakthrough ideas, they provide valuable insights on product development that humans might easily overlook.\nIn the next post, I’ll be introducing LLMs’ strengths and weaknesses as prompter."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Push the boundary of LLMs in the realm of creativity."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jasmine’s explorations of LLMs",
    "section": "",
    "text": "Building A Better Ideation Partner 3\n\n\n\n\n\n\nLLMs\n\n\nproduct development\n\n\ncreativity\n\n\nideation\n\n\n\nInspired by my experiments with LLMs, I set out to build an ideation partner using Open AI API, that helps humans yield better ideation results than a basic idea generator.\n\n\n\n\n\nFeb 20, 2025\n\n\nJasmine Wang\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding A Better Ideation Partner 2\n\n\n\n\n\n\nLLMs\n\n\nproduct development\n\n\ncreativity\n\n\nideation\n\n\n\nInspired by my experiments with LLMs, I set out to build an ideation partner using Open AI API, that helps humans yield better ideation results than a basic idea generator.\n\n\n\n\n\nFeb 8, 2025\n\n\nJasmine Wang\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding A Better Ideation Partner 1\n\n\n\n\n\n\nLLMs\n\n\nproduct development\n\n\ncreativity\n\n\nideation\n\n\n\nInspired by my experiments with LLMs, I set out to build an ideation partner using Open AI API, that helps humans yield better ideation results than a basic idea generator.\n\n\n\n\n\nFeb 6, 2025\n\n\nJasmine Wang\n\n\n\n\n\n\n\n\n\n\n\n\nAn Investigation Into the Creativity of LLMs Through Ideation 3\n\n\n\n\n\n\nLLMs\n\n\nprompting\n\n\ncreativity\n\n\nideation\n\n\n\nTesting the strengths and constraints of LLMs ideating capability\n\n\n\n\n\nFeb 2, 2025\n\n\nJasmine Wang\n\n\n\n\n\n\n\n\n\n\n\n\nAn Investigation Into the Creativity of LLMs Through Ideation 2\n\n\n\n\n\n\nLLMs\n\n\nprompting\n\n\ncreativity\n\n\nideation\n\n\n\nTesting the strengths and constraints of LLMs ideating capability\n\n\n\n\n\nFeb 1, 2025\n\n\nJasmine Wang\n\n\n\n\n\n\n\n\n\n\n\n\nAn Investigation Into the Creativity of LLMs Through Ideation\n\n\n\n\n\n\nLLMs\n\n\nprompting\n\n\ncreativity\n\n\nideation\n\n\n\nTesting the strengths and constraints of LLMs ideating capability\n\n\n\n\n\nJan 30, 2025\n\n\nJasmine Wang\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/002_post2/index.html",
    "href": "posts/002_post2/index.html",
    "title": "An Investigation Into the Creativity of LLMs Through Ideation 2",
    "section": "",
    "text": "After experimenting with different collaborative roles for LLMs, my confidence in human creativity has only grown stronger. While Artificial Intelligence might be exceptionally good at synthesizing information and generating small, creative ideas, it still struggles to produce truly unique business concepts with real commercial potential.\nHowever, LLMs are excellent prompters–they are the perfect guide for brainstorming, pushing us to explore ideas more deeply and venture further down the rabbit hole.\n\n\n\nThis time, rather than directly asking LLM to generate ideas, I switched our roles and told it to become my prompter by asking thought-proviking questions. With base prompt the same, I added the following:\n\nI want you to do the following: ask me 5 questions that you think might be inspiring in this field and have the potential to expand my thoughts. Based on my answers, you can ask follow-up questions if needed. The questions can be about the context, my personal experience, creative process, interdisciplinary fields, or any aspects that you think might help with the idea generation in the context I provided above.\n\nInitially, GPT started with some generic questions, following a typical brainstorming procedure. From top to bottom, each question digs deeper and asks something more specific and tangible. One interesting point worth mentioning is the word templatable in the fifth question. It precisely captures the essence of the context I provided, even though I never used it in my original prompt. I was impressed by how well the LLM understood and summarized my content, and inferred something implicit from our conversation.\n\nAfter I answered the five questions, GPT provided additional feedback by highlighting key points from my responses and posing follow-up questions. Unlike the previous experiment, where GPT took the dominant role in ideation, this time I contributed more information infused with human perspectives and loosely structured ideas. Rather than generating a flood of generic “AI-powered platform” concepts, GPT followed my train of thought, engaging in a more nuanced dialogue and addressing specific questions I had raised in my previous response.\n\nI gave very positive feedback about the questions it prompted me. But this time, I asked it to not only ask me follow-up questions, but also respond to some of the questions I have left in my response and think together with me.\nThis time, each bullet point header was framed as a question. Below each question, GPT provided insightful responses, followed by several follow-up questions for me to consider. In the end, without being prompted, GPT offered a summary of final thoughts along with potential next steps.\n\n\n\n\n\nI was very satisfied with the result this time. Although we haven’t reached an ideal outcome yet, the human-machine collaboration took a promising direction. This made me reflect—what was the game changer?\nLarge Language Models integrate vast amounts of data from the internet, which means their outputs tend to be the most average—the statistically most probable responses. However, the most probable idea isn’t necessarily the most feasible, let alone the most creative. This is where human thought and contribution become essential. Unlike AI, our thinking process isn’t constrained by “word embeddings.” Some seemingly unrelated ideas—those that don’t follow predictable patterns—are often the ones that lead to true breakthroughs.\nSo, how exactly can AI support us in this space? As the saying goes, two minds are better than one. By leveraging AI’s ability to expand on concrete ideas and provide tangible solutions, it helps counteract biases and blind spots in human thinking. By flexibly combining the strengths of both human creativity and AI’s analytical capabilities, I found a more natural and effective approach to human-machine collaboration.\nIn the next post, I’ll be exploring LLMs’ strengths and weaknesses as idea refiner."
  },
  {
    "objectID": "posts/002_post2/index.html#what-is-the-game-changer",
    "href": "posts/002_post2/index.html#what-is-the-game-changer",
    "title": "An Investigation Into the Creativity of LLMs Through Ideation 2",
    "section": "",
    "text": "I was very satisfied with the result this time. Although we haven’t reached an ideal outcome yet, the human-machine collaboration took a promising direction. This made me reflect—what was the game changer?\nLarge Language Models integrate vast amounts of data from the internet, which means their outputs tend to be the most average—the statistically most probable responses. However, the most probable idea isn’t necessarily the most feasible, let alone the most creative. This is where human thought and contribution become essential. Unlike AI, our thinking process isn’t constrained by “word embeddings.” Some seemingly unrelated ideas—those that don’t follow predictable patterns—are often the ones that lead to true breakthroughs.\nSo, how exactly can AI support us in this space? As the saying goes, two minds are better than one. By leveraging AI’s ability to expand on concrete ideas and provide tangible solutions, it helps counteract biases and blind spots in human thinking. By flexibly combining the strengths of both human creativity and AI’s analytical capabilities, I found a more natural and effective approach to human-machine collaboration.\nIn the next post, I’ll be exploring LLMs’ strengths and weaknesses as idea refiner."
  },
  {
    "objectID": "posts/003_post3/index.html",
    "href": "posts/003_post3/index.html",
    "title": "An Investigation Into the Creativity of LLMs Through Ideation 3",
    "section": "",
    "text": "In my third attempt, I focused on using LLMs as an idea refiner. Initially, I was disappointed by the lack of variation in GPT’s output. However, as the conversation progressed, I was once again impressed by its ability to address my concerns and support its viewpoints with relevant market insights drawn from its data.\nBeyond generating a promising commercial idea, we collaborated to craft a compelling pitch statement—one that could be used for marketing and presenting to early investors.\n\n\n\nTo let LLM plays the refiner role, I reworked my prompt (the base prompt remains the same):\n\nI want to come up with a profitable product idea and want you to be my helper for this brainstorming session. I’m the one who will be providing the ideas, but you are responsible for improving how ideas are expressed, in order to maximize our product attraction to potential customers. You are also responsible for filling in the details, such as features or unique selling points, if needed. Your goal is to effectively communicate the idea and persuade the potential customers and investors to buy the product or make the investment.\n\nThis time, the dynamic shifted—the responsibility of generating ideas fell on me. Unlike before, the idea pool remained within the two concepts I provided. Instead of prioritizing breadth, we focused on depth, thoroughly examining and defending every possible argument that users or investors might raise.\n\n\nI wasn’t very satisfied at first, as the output didn’t differ much from what I had received in my initial attempt. Even after refinement, the ideas remained generic and unconvincing. Based on my previous interactions with LLMs, I decided not to ask for further elaboration or expansion. Instead, I structured my prompt to focus on the skepticism raised by the “imaginary” investors, and asked for its feedback:\n\nI presented these ideas to early investors, but they expressed skepticism about several aspects: the high level of market competition, my ability to sustain a large network of designers who are consistently available to take orders, and AI’s effectiveness in generating branding assets and UI in a truly valuable way. Please help me craft responses to address these concerns, using data and reasoning to demonstrate the potential of both ideas.\n\nThis time, it worked much better as we shifted from a broad landscape to a few highly specific concerns—an area where GPT seems to excel. By addressing each concern directly, GPT provided insightful feedback with clear reasoning, market validation, and key points to help persuade investors. For example, when countering concerns about sustaining a large network of designers, it argued that “we don’t need a huge designer pool to succeed–we just need smart workflow management to match demand and supply efficiently.”\n\nThen I asked a couple follow-up questions, pushing back once again by stating some potential skepticisms from investors.\n\nGPT maintained a structured and clear approach in its responses. As I reviewed them, I found most to be compelling and well-reasoned—strong enough to be included in a pitch deck with some refinements. In the end, GPT delivered a closing statement that effectively addressed the three key concerns I had raised.\n\n\n\n\n\nBased on my three rounds of experiments, I summarized several key takeaways when using LLMs as ideating partner:\n\nLLMs can be valuable ideation partners, especially when acting as prompters and refiners. By leveraging both roles at different stages—initial brainstorming and refinement—humans can generate more compelling and feasible ideas.\nThe base prompt has to be concrete and specific, providing the LLMs with sufficient context about target market and problem space.\nHumans should remain the driving force in ideation. The experiments have shown that AI tends to produce safe, average choices, while the ability to generate truly innovative, breakthrough ideas remains unique to human creativity due to our free-flowing thinking patterns.\nDuring the ideating process, think actively with LLM and ask specific, rather than open-ended, questions. The more information and personal insights you feed LLMs, the more ideal your output might be.\nWhen prompting LLMs, focus on concrete elements such as features, challenges, and actionable suggestions to get the most useful insights.\nLeverage LLMs to identify blind spots by constantly challenging their responses and prompting them with counterarguments and imaginary refutations."
  },
  {
    "objectID": "posts/003_post3/index.html#summary-how-should-we-use-llms-in-ideation",
    "href": "posts/003_post3/index.html#summary-how-should-we-use-llms-in-ideation",
    "title": "An Investigation Into the Creativity of LLMs Through Ideation 3",
    "section": "",
    "text": "Based on my three rounds of experiments, I summarized several key takeaways when using LLMs as ideating partner:\n\nLLMs can be valuable ideation partners, especially when acting as prompters and refiners. By leveraging both roles at different stages—initial brainstorming and refinement—humans can generate more compelling and feasible ideas.\nThe base prompt has to be concrete and specific, providing the LLMs with sufficient context about target market and problem space.\nHumans should remain the driving force in ideation. The experiments have shown that AI tends to produce safe, average choices, while the ability to generate truly innovative, breakthrough ideas remains unique to human creativity due to our free-flowing thinking patterns.\nDuring the ideating process, think actively with LLM and ask specific, rather than open-ended, questions. The more information and personal insights you feed LLMs, the more ideal your output might be.\nWhen prompting LLMs, focus on concrete elements such as features, challenges, and actionable suggestions to get the most useful insights.\nLeverage LLMs to identify blind spots by constantly challenging their responses and prompting them with counterarguments and imaginary refutations."
  },
  {
    "objectID": "posts/006_post6/index.html",
    "href": "posts/006_post6/index.html",
    "title": "Building A Better Ideation Partner 3",
    "section": "",
    "text": "I have been constantly reading papers and materials regarding engaging AI into creative tasks, including Co-intelligence by Ethan Mollick, The Innovation Tournament Handbook by Christian Terwiesch, and Using Large Language Models for Idea Generation in Innovation by Lennart Meincke, Karan Girotra, Gideon Nave, Christian Terwiesch, and Karl T. Ulrich. These resources have provided valuable insights, prompting me to develop my own guidelines for setting up an effective ideation environment.\nCurrently, when evaluating LLM-generated output, I rely on subjective judgment, selecting ideas that resonate most with me. However, I’ve realized that to help LLMs better understand the innovation process and the criteria for high-quality ideas, clearly defined and specialized rules and guidelines are essential.\nWorking with LLMs requires a balance of creativity and logic.\n\n\n\nTo teach GPT how to generate high-quality ideas, I first categorized AI-generated ideas into GOOD and BAD groups. Additionally, I created a third category containing GOOD human-generated ideas from renowned designers worldwide.\nI compiled the list of human-generated ideas by manually selecting product design concepts created by renowned designers, awarded prestigious design accolades, or achieving notable commercial success.\nFor AI-generated ideas, I employed various prompting strategies, including Chain-of-Thought, few-shot prompting, question-inspired prompting, attribute-driven prompting, and design methodology prompting to generate diverse innovative product concepts. Based primarily on subjective judgment, I selected 10 good ideas and 10 bad ideas from a pool of 100 AI-generated ideas.\n\nWith GPT’s assistance, I compared these groups to identify key differences. From this analysis, I distilled a list of characteristics unique to high-quality ideas and pinpointed common pitfalls that AI-generated ideas tend to fall into.\n\n\nAfter comparing ideas from different sources, I found out that those high-quality human-generated ideas still stand out among all ideas.\n\n\nOne differentiator between high-quality human-generated ideas and mediocre AI-generated ideas is that human draws meaningful connections by exploring deep linkages and metaphors between concepts. People might argue that: LLMs are connection machines! They must outperform humans in this aspect. I think this is only true from one perspective, that LLMs are good at producing connections on a surface level. Do these connections really help with idea generation? Only a small portion might be helpful. While most connections are just random mixtures of technology buzzwords and relevant terminologies. LLMs fail to see the deep connections between two seemingly irrelevant concepts.\nThis task can also be challenging even for a lot of humans, but it’s what sets great product designers and entrepreneurs apart. For example, a product concept designed to enhance focus and block distractions draws connections between focus sessions and immersive flights. While it would be difficult—though not entirely impossible—for LLMs to make this connection, humans grasp it intuitively. Flights naturally encourage focus since limited internet and signal access create an environment with fewer distractions.\n\n\n\nThe majority of high-quality human-generated ideas are deeply rooted in psychological needs—fear, anxiety, nostalgia, curiosity, and more. These needs serve as the core drivers behind product concepts. While LLMs can recognize keywords related to human emotions from their training data, generating ideas that genuinely resonate with these emotions remains a challenge. It is the sentimental depth of product ideas that celebrates humanity and creates concepts that truly connect with people.\nFor example, a human-generated idea involves a photo frame that fades over time. “Left untouched, the displayed photograph gradually blurs, mimicking the way memories fade over time. However, when the user touches the frame, the glass slowly clears, bringing the image back into focus. This design transforms nostalgia into a tactile experience, reinforcing the emotional act of remembering through interaction.\n\n\n\nWe imagine within reason, guided by clear constraints—we dance with shuckles on our feet. Humans intuitively understand the underlying meaning of words, linking them to real-world objects while considering their affordances and constraints. We can envision how objects function in different contexts, mentally simulate their combinations, and assess their feasibility before they even take shape. But LLMs sometimes generate ideas that are far from implementable. Technology buzzwords such as “AI-powered”, “AR-boosted”, “gesture-activated” can be seen almost everywhere.\nFor example, the haptic paper packaging designed by Kenya Hara, the art director of MUJI, perfectly leverages the affordances of paper–the diversity in its texture–to create packaging products. ” Each material—rough, smooth, embossed—corresponds to the product inside, allowing users to “read” the packaging through touch.”\n\nIn the next post, I’ll be sharing some insights on some common pitfalls that AI-generated ideas tend to fall into."
  },
  {
    "objectID": "posts/006_post6/index.html#what-can-be-considered-as-a-good-idea",
    "href": "posts/006_post6/index.html#what-can-be-considered-as-a-good-idea",
    "title": "Building A Better Ideation Partner 3",
    "section": "",
    "text": "To teach GPT how to generate high-quality ideas, I first categorized AI-generated ideas into GOOD and BAD groups. Additionally, I created a third category containing GOOD human-generated ideas from renowned designers worldwide.\nI compiled the list of human-generated ideas by manually selecting product design concepts created by renowned designers, awarded prestigious design accolades, or achieving notable commercial success.\nFor AI-generated ideas, I employed various prompting strategies, including Chain-of-Thought, few-shot prompting, question-inspired prompting, attribute-driven prompting, and design methodology prompting to generate diverse innovative product concepts. Based primarily on subjective judgment, I selected 10 good ideas and 10 bad ideas from a pool of 100 AI-generated ideas.\n\nWith GPT’s assistance, I compared these groups to identify key differences. From this analysis, I distilled a list of characteristics unique to high-quality ideas and pinpointed common pitfalls that AI-generated ideas tend to fall into.\n\n\nAfter comparing ideas from different sources, I found out that those high-quality human-generated ideas still stand out among all ideas.\n\n\nOne differentiator between high-quality human-generated ideas and mediocre AI-generated ideas is that human draws meaningful connections by exploring deep linkages and metaphors between concepts. People might argue that: LLMs are connection machines! They must outperform humans in this aspect. I think this is only true from one perspective, that LLMs are good at producing connections on a surface level. Do these connections really help with idea generation? Only a small portion might be helpful. While most connections are just random mixtures of technology buzzwords and relevant terminologies. LLMs fail to see the deep connections between two seemingly irrelevant concepts.\nThis task can also be challenging even for a lot of humans, but it’s what sets great product designers and entrepreneurs apart. For example, a product concept designed to enhance focus and block distractions draws connections between focus sessions and immersive flights. While it would be difficult—though not entirely impossible—for LLMs to make this connection, humans grasp it intuitively. Flights naturally encourage focus since limited internet and signal access create an environment with fewer distractions.\n\n\n\nThe majority of high-quality human-generated ideas are deeply rooted in psychological needs—fear, anxiety, nostalgia, curiosity, and more. These needs serve as the core drivers behind product concepts. While LLMs can recognize keywords related to human emotions from their training data, generating ideas that genuinely resonate with these emotions remains a challenge. It is the sentimental depth of product ideas that celebrates humanity and creates concepts that truly connect with people.\nFor example, a human-generated idea involves a photo frame that fades over time. “Left untouched, the displayed photograph gradually blurs, mimicking the way memories fade over time. However, when the user touches the frame, the glass slowly clears, bringing the image back into focus. This design transforms nostalgia into a tactile experience, reinforcing the emotional act of remembering through interaction.\n\n\n\nWe imagine within reason, guided by clear constraints—we dance with shuckles on our feet. Humans intuitively understand the underlying meaning of words, linking them to real-world objects while considering their affordances and constraints. We can envision how objects function in different contexts, mentally simulate their combinations, and assess their feasibility before they even take shape. But LLMs sometimes generate ideas that are far from implementable. Technology buzzwords such as “AI-powered”, “AR-boosted”, “gesture-activated” can be seen almost everywhere.\nFor example, the haptic paper packaging designed by Kenya Hara, the art director of MUJI, perfectly leverages the affordances of paper–the diversity in its texture–to create packaging products. ” Each material—rough, smooth, embossed—corresponds to the product inside, allowing users to “read” the packaging through touch.”\n\nIn the next post, I’ll be sharing some insights on some common pitfalls that AI-generated ideas tend to fall into."
  }
]