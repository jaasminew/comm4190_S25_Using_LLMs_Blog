[
  {
    "objectID": "posts/008_post8/index.html",
    "href": "posts/008_post8/index.html",
    "title": "Enable LLMs to do emotional thinking",
    "section": "",
    "text": "Continuing from my last post, I optimized the prompting strategies to allow LLMs dig into more nuanced connections to build a mindmap. This time, I decided to shift my focus–enabling LLMs to engage in emotional thinking.\nWhat makes high-quality human-generated ideas stand out is our ability to understand psychological motivations that drive product adoption. While LLMs can generate creative concepts, they often fail to resonate emotionally in the way that strong human-generated ideas do.\nTo address this, I’m planning to incorporate a dedicated branch in the ideation process that prompts AI to explore emotional factors, ensuring that generated ideas are not only innovative but also deeply connected to human needs and desires."
  },
  {
    "objectID": "posts/008_post8/index.html#deep-dive-into-the-emotional-aspect-of-human-generated-ideas",
    "href": "posts/008_post8/index.html#deep-dive-into-the-emotional-aspect-of-human-generated-ideas",
    "title": "Enable LLMs to do emotional thinking",
    "section": "Deep dive into the emotional aspect of human-generated ideas",
    "text": "Deep dive into the emotional aspect of human-generated ideas\nWhen exploring the product ideas that won the Kokuyo Design Award, I was deeply moved by the level of humanistic consideration embedded in each design. Regardless of the emotion they represent(or seek to address), whether love, sadness, anxiety, or curiosity, these ideas always stem from an emotional core, with the product concept perfectly capturing that feeling.\nHere’s a great example that illustrates this: This product is a watch designed for couples living in different time zones. Inspired by the frustration of coordinating across time differences, the watch elegantly solves this problem by visualizing two time zones within a single dial.\nWhile multi-time-zone watches are not uncommon, this design stands out due to its ingenuity and emotional depth. The designer’s thoughtful approach goes beyond functionality—it acknowledges the nuanced emotions of distance and connection, making the product feel deeply personal and meaningful.\n\nTo create a “recipe” for generating ideas with humanistic touch and emotional depth, I started by analyzing several key aspects of it:\n\nThe idea originates from an emotional core: During brainstorming, we often generate numerous problem statements, but most remain surface-level, focusing on practical solutions rather than digging deeper into why the problem exists in the first place. However, a high-quality human-generated idea goes beyond simply accumulating advanced technology—it establishes a connection with the underlying emotional factors of the problem statement. This depth of understanding transforms a concept from merely functional to truly meaningful.\nMore than just designing a usable product, it creates a product that seamlessly aligns with human habits and intuitions. Humans rely on mental shortcuts, heuristics, and unconscious behaviors that shape how we interact with objects and technology. A well-designed product anticipates these natural tendencies, reducing cognitive load and making interactions feel effortless.\n(Sometimes) It incorporates a moment of delight by introducing subtle abnormalities into conventional details. When things follow predictable patterns, they can become mundane, fading into the background. A well-designed product, however, breaks this expectation of normalcy, sparking surprise and joy in the process. No matter how ordinary an object can be, a great product idea has the ability to add a playful touch to make it into a “mini-game”."
  },
  {
    "objectID": "posts/008_post8/index.html#prompting-ai-to-achieve-the-result",
    "href": "posts/008_post8/index.html#prompting-ai-to-achieve-the-result",
    "title": "Enable LLMs to do emotional thinking",
    "section": "Prompting AI to achieve the result",
    "text": "Prompting AI to achieve the result\nHaving these insights in mind, I started working on prompt engineering. I first came up with prompting approaches one by one, and tried to combine them to form a comprehensive prompt that guides the LLM through various phases, including generating painpoints, refining them emotionally, and ensuring they address specific emotional needs or user pains.\nLike what I did in the last post, I came up with some prompting strategies one by one, and integrated them strategically to form a comprehensive prompt:\n\nPrompting the LLM to identify core emotions behind the problem\n\n\nIdentify the core emotion(s) behind the problem: - What feelings (e.g., anxiety, longing, excitement) do people experience? - Why do these emotions arise?\n\n\nUsing “Five whys”\n\n\n\nState the problem. Then, repeatedly ask “Why?”:\n\n\n1st “Why?” peels back the immediate cause.\n2nd “Why?” clarifies the underlying situation.\n3rd “Why?” moves closer to emotional drivers.\n…continue until you uncover a more profound emotional truth or universal human desire.\n\n\nSummarize the deep emotional insight uncovered.\n\n\n\nFocusing on the holistic user experience, including what users see, feel, think, and do\n\n\n\nDefine the user persona: their environment, goals, and any known pain points.\nEmpathy map questions:\n\n\nTHINK & FEEL: What are they worried about, excited about, or fearful of?\nSEE: What does their environment look like? What cues affect them emotionally?\nHEAR: What do friends, family, or media tell them?\nSAY & DO: How do they outwardly express their feelings, and what actions do they take?\nPAINS: What frustrations or difficulties are they facing?\nGAINS: What would relief, success, or emotional satisfaction look like?\n\n\n\nEncouraging LLMs to incorporate little twist of surprise, quirk, or abnormality that sparks joy and makes design feel extra special\n\n\n\nIdentify the standard design approach or “norm” for a product type.\nPinpoint a subtle, playful twist that could surprise and delight the user.\n\n\nIt could be an unexpected animation, a tactile feature, or a playful way of providing feedback.\n\n\nEvaluate how that moment of delight aligns with the emotional core previously identified.\n\n\n\nFocusing the brainstorming on mental models, heuristics, and typical behaviors that make solutions feel natural\n\n\n\nList key human habits and heuristics relevant to the domain.\n\n\ne.g., people prefer consistency; they find comfort in routines; they like quick visual feedback.\n\n\nBrainstorm how the solution can integrate with or leverage these habits.\n\n\ne.g., a new feature that builds off an existing daily routine so users don’t have to consciously learn something new.\n\n\nHighlight where emotional resonance meets usability.\n\n\ne.g., how does this habit-based design reduce frustration or add an element of comfort?\n\n\n\nIdentifying negative emotionalities and turning them into positive feelings\n\n\nIdentify 2–3 commonly negative or taboo perceptions or frustrations that might arise in this context. Then offer short suggestions for how they could be flipped into something playful, intriguing, or surprisingly positive. (e.g., turning ‘anxiety about deadlines’ into ‘fun countdown rituals’). Focus on concise, open-ended prompts that invite a human designer to explore\n\n\nMaster prompt\n\nContext & Objectives “You are a design consultant specializing in emotionally resonant product innovation. Our aim is to explore new [product/service/feature] directions addressing [briefly describe the problem context]. We want to spark concepts that speak to emotional needs, feel intuitive, and introduce a delightful twist—especially by flipping negative associations into something intriguing or playful. Avoid fully detailed solutions; instead, give short, creative pointers.”\nEmotional Seeds “Identify 3–5 core emotional root causes that lead to the problem. What feelings (e.g., anxiety, longing, excitement) do people experience? Why do these emotions arise? Present them as bullet points with brief explanations (e.g., ‘fear of letting others down,’ ‘need for recognition’).”\nHabit & Heuristic Alignment “ List key human habits and heuristics relevant to the domain. (e.g., people prefer consistency; they find comfort in routines; they like quick visual feedback.) Then brainstorm directions of how the solution can integrate with or leverage these habits. (e.g., a new feature that builds off an existing daily routine so users don’t have to consciously learn something new.)”\nDelightful Subversion “Identify 2–3 commonly negative or taboo perceptions or frustrations that might arise in this context. Then offer short suggestions for how they could be flipped into something playful, intriguing, or surprisingly positive. (e.g., turning ‘anxiety about deadlines’ into ‘fun countdown rituals’). Focus on concise, open-ended prompts that invite a human designer to explore.”\nGenerate Directions (Not Full Solutions) “Propose 3–5 conceptual ‘directions’ (just short phrases or quick bullet points) that combine elements from the emotional seeds, empathy map, habits, and delightful subversion. Keep these directions open-ended—enough to spark ideas, but without detailing exactly how to implement them.”\nRefine & Inspire “Conclude by highlighting which direction(s) might carry the strongest emotional resonance or the most intriguing ‘delightful subversion.’ Offer a few inspiring questions or next-step prompts (not instructions) for the human to reflect on or build upon. Avoid prescribing a final design—just offer creative nudges.”"
  },
  {
    "objectID": "posts/008_post8/index.html#results-comparison",
    "href": "posts/008_post8/index.html#results-comparison",
    "title": "Enable LLMs to do emotional thinking",
    "section": "Results comparison",
    "text": "Results comparison\nWith standard prompting strategies, the product ideas tend to be more generic and abstract. Additionally, they are often fully fleshed out, leaving little room for human collaboration and iterative refinement.\n\nHowever, with the modified master prompt, the ideation process unfolds step by step, generating more nuanced solutions that create opportunities for human input and creative expansion.\n\nIn the next post, I’ll be discussing prompting strategies that enable LLMs to comprehensively understand the affordances and constraints of products."
  },
  {
    "objectID": "posts/005_post5/index.html#the-design-thinking-method",
    "href": "posts/005_post5/index.html#the-design-thinking-method",
    "title": "Standardize the idea generation process",
    "section": "The Design Thinking Method",
    "text": "The Design Thinking Method\nEvery creative process follows a pattern. The renowned design agency IDEO was instrumental in evangelizing this Design Thinking method, a method now widely used by creative agencies and tech companies during the design stage. This approach helps break down any seemingly intricate problems into solvable steps.\nTherefore, when trying to standardize the idea generation process, I brought back this classic technique. As an ideation tool, this product currently does not include the prototype and test phases. However, the steps of empathize, define, and ideate can be effectively replicated within an LLM. By incorporating Artificial Intelligence, I want to see it bring in its own strengths and empower humans to ideate more wisely."
  },
  {
    "objectID": "posts/005_post5/index.html#experimentation-with-different-approaches",
    "href": "posts/005_post5/index.html#experimentation-with-different-approaches",
    "title": "Standardize the idea generation process",
    "section": "Experimentation with different approaches",
    "text": "Experimentation with different approaches\nTo start with, I explored different possible templates for ideating. To test their effectiveness, I used the same prompt to initiate an ideating process and compared the quality of various ideas across different trial runs.\nHere’s my base prompt:\n\nI want to come up with a profitable product idea. You are my partner, an extremely creative entrepreneur who had successfully designed and built many commercial products, to help me to generate the ideas.\nHere’s the context: I want to design a new type of canvas bag (tote bag) that redefine this product. The product will target 18-35 years old female consumers. The “redefine” can be on a design and aesthetic level, functional level, semantic level (ie. carry some special meaning), branding level, or even combining some technology innovations. I want the ideas to be bold and innovative.\n\n\nSolution 1: Question-inspired ideating\nAsk GPT to generate a set number of thought-provoking questions on a given topic. Users then provide responses, which serve as a secondary input for GPT to generate follow-up questions.\nAdded prompt:\n\nI want you to do the following: Ask me 5 thought-provoking questions that you think might be inspiring, formatted in “keywords: questions”. Based on each of my response, you will ask at least 2 follow-up questions. The questions can be about the context, my personal experience, creative process, interdisciplinary fields, or any aspects that you think might help with the idea generation in the context I provided above.\n\nOverall, our conversation followed a structured question-and-answer pattern. After five rounds of Q&A, we successfully developed three high-quality ideas that I’m very satisfied with. By round three, GPT started introducing key keywords that played a crucial role in shaping our final concepts. By round four, a rough idea had already taken form, and we focused on refining it—polishing details, selecting the strongest elements, and discarding others.\nHere are some insights about the question-inspired ideating:\n\nThe strucutre of follow-up questions + idea suggestions works well: The quality of ideas significantly improved by round three, when quick suggested ideas were provided alongside the questions. More than half of my ideas were sparked by keywords in GPT’s responses. This suggests that incorporating suggested directions as helper text within the Q&A structure could further enhance the ideation process.\nKeywords are more valuable than sentences: I realized that our Q&A process was unnecessarily long, making it inefficient in terms of both time and computing power. Many responses contained “filler” sentences that weren’t directly relevant to the core ideas. Instead, a few keywords or concise sentences would be sufficient for each point.\nDetails need human input to build up: GPT is good at solving challenges. Specific, detailed ideas came up the fastest when I described my vision and left GPT determine how to achieve it.\nRepetitive ideas still exist: Another factor contributing to the lengthy responses was the repetition of ideas. Real-time evaluation is crucial to prevent non-valuable ideas from reappearing in subsequent responses.\n\n\n\n\nSolution 2: Attributes-driven ideating\nAsk GPT to identify a set number of primary attributes or components of a given product. Then, prompt it to add, remove, or modify the connections between these attributes to generate new product ideas.\nAdded prompt:\n\nI want you to do the following: Identify a set number of (both external and internal) attributes or components of canvas bag. Later in our conversation, I’ll prompt you to identify the links between those attributes or components, and generate innovative ideas by modifying, adding, or removing those links.\nFor example, primary attributes or components of a pizza include ingredients, flavor, size, temperature, price, packaging, and brand.\n\nOnce I got the list of attributes, I sent out a more specific instructions on how to add, remove, and motify the product configuration chart.\n\nNow I want you to: 1. Combine seemingly unrelated attributes, such as shape × branding or material × cultural and social meaning. For each pair, write a short paragraph explaining how these attributes might interact in a unique way. 2. Remove attributes that seem essential or question underlying assumptions. For example, does the material have to be canvas? Can branding exist without any visible logos or signals? 3. Change the nature of certain connections and reinterpret their meanings. For each part, generate five distinct suggestions with clear explanations. Once we have these, we’ll work together to refine and explore some of them in greater depth.\n\nThe ideas generated using these prompts turned out to be bolder and more unconventional, but also less realistic and harder to implement with current materials and technology.\n\nSome insights about the attributes-driven ideating:\n\nBalancing the originality and feasibility: Attribute-driven ideation allows LLMs to fully leverage their creativity by combining seemingly incompatible aspects. However, this prompting strategy can be tricky to use, as many generated ideas may be impractical and ultimately deemed non-valuable by the prompter. To solve this problem, attributes-driven ideating needs to be used cautiously along with the question-inspired ideating.\nLess user input is needed: Compared to question-inspired prompting strategy, attributes-driven ideating requires less human input. This implies a lower cognitive load for users, while also introduces more uncertainty as there is less human oversight in guiding the ideas.\n\n\n\nSolution 3: Interdisciplinary ideating\nAsk GPT to generate moderately relevant interdisciplinary subjects and topics. Then, prompt it to strategically merge concepts from different domains, providing inspiration for humans to develop new ideas.\nAdded prompt:\n\nI want you to do the following: Generate 10 moderately relevant interdisciplinary topics centered around culture, creativity, art, and Gen Z personality expression. Then, strategically merge each concept with canvas bags (tote bags) to create innovative product ideas.\nFor each idea, provide a short paragraph explaining how the concepts integrate and how they generate business value.\n\nIdeating with LLMs is a process of fine-tuning the model’s output to align ideas more closely with your desired direction. Encouraging LLMs to generate loosely relevant keywords has proven highly effective. I typically select 2–3 keywords from each idea and prompt the model to elaborate only on those details. This approach functions like a funnel, gradually narrowing broad concepts into specific, niche ideas.\nThe format of each response wasn’t fully settled. But the structure of “incorporating topics” + “concepts” + “business values” proved to be a helpful and comprehensive format. Currently, LLM still tends to wrap more-than-needed details and options in its response, which can easily lead to information overload. Controlling the amount of information released and ensuring that only the most useful insights are presented is one of the major challenges.\n \nSome insights about the interdisciplinary ideating:\n\nThis approach is effective only when interdisciplinary concepts align with the intended scope: Concept merge can be very arbitrary, and most arbitraty ideas are not of high value. Setting clear path for LLMs to follow is a necessary step early in the ideating session.\nResponses are easier to control than attributes-driven approach: Similar to the previous approach, interdisciplinary ideation requires relatively less human input, primarily involving the selection and integration of independent concepts. However, responses generated through this method are more likely to be relevant and implementable, as long as a proper direction has been established.\n\n\n\nSolution 4: Imagenary-customer-prompted ideating\nAsk GPT to take on the perspective of customers and generate hypothetical feedback on the product. Use this feedback as inspiration to refine existing ideas or develop new ones.\nAdded prompt:\n\nI want you to do the following: put yourself in the shoes of 18-35 years old female consumers and generate 5 customer feedback on the product. Using this feedback, summarize the pain point and its root cause. Then, based on each point, briefly explain the reason and your suggested directions to solve them. At this stage, the suggested directions don’t have to be detailed and very specific.\n\nTalking to customers and gathering their feedback is always the first step in brainstorming. Here, I experimented with using LLMs to generate hypothetical customer feedback and then prompted it to respond to its own feedback with potential solutions.\nThe advantage of this approach is that ideation begins with a clear direction, and the ideas generated are more likely to resonate with customers since they directly address pain points. To explore root causes, I used the “5 Whys” technique. However, at this stage, the LLM’s ability to apply the “5 Whys” effectively is still quite limited. With further training on instructions and data, this technique could become a valuable tool in identifying pain points during the ideation process.\n\nAs seen in the conversation, this method allows us to dive into the topic quickly, improving efficiency compared to previous trials that required exploring multiple directions before finding a suitable one. However, the trade-off is that the ideas tend to become predictable, as they are solutions that an average person could easily come up with.\nSome insights about the imagenary-customer-prompted ideating:\n\nMore suitable for technical problem statements than creative ones: Thanks to its ability to pinpoint root causes, this prompting technique is most effective for solving technical problems but falls short when tackling creativity-driven challenges.\nWorks best when incorporating other prompting techniques: The ideas generated are precise and feasible but lack originality. However, this drawback can be effectively addressed by integrating it with interdisciplinary prompting."
  },
  {
    "objectID": "posts/017_post17/index.html",
    "href": "posts/017_post17/index.html",
    "title": "Using AI coding tool to build AIdeator",
    "section": "",
    "text": "This post shares my personal experience using an AI coding tool for the first time to develop a web application on my own.\nFor context, I don’t have much experience writing code, nor do I have an extensive background in software development. My understanding is limited to basic coding concepts like variables, functions, and arguments. Learning everything from scratch and building a full web application on my own would have been time-consuming and overwhelming—but thanks to AI coding tools like Claude and Cursor, I was able to successfully build the backend (primarily using LangGraph) for my LLM application AIdeator.\n\nThe workflow is built using LangGraph’s framework, leveraging OpenAI’s API to help users generate mind maps for product ideation. During the collaborative session, humans and the AI take turns expanding on existing concepts, gradually shaping innovative product ideas through mutation and recombination of different elements.\nThe backend is relatively lightweight—it doesn’t involve data storage or additional API integrations. Instead, it primarily relies on the Large Language Model for generating outputs and facilitating the ideation process.\n\n\nTL;DR:\nI rely on Claude for about 95% of my code writing thanks to its accuracy and thorough explanations. Meanwhile, Cursor serves as my main coding platform—housing my repo and handling simpler commands and quick tasks more fluidly due to its integrated IDE environment.\nCursor: Strengths\n\nIDE-Like Experience: Cursor is tightly integrated into a code editor. This provides real-time feedback, inline suggestions, and a relatively seamless workflow for writing and refactoring code.\nFast, Integrated Workflow: Because everything happens directly in the IDE, there’s no need for copy/paste. It’s a sleek process that keeps you in the coding flow.\nContext-Awareness: Because it’s typically embedded in your editor, it can see your file structure and code context, making suggestions that are often relevant to your specific project.\nGood with Simple Tasks: Cursor shines when it comes to straightforward function development and logistical coding actions—it’s quick and effective at these.\nSmart Auto-Fill: The tab-completion feature can be surprisingly accurate and significantly reduce typing time.\n\nCursor: Weaknesses\n\nComplexity Issues: As functions get more complicated and new variables come into play, Cursor can slip up and introduce more errors.\nLoss of Control: If you’re still learning the ropes, it can feel uneasy allowing direct edits to your original code. While you can reject changes, it can be a bit disconcerting.\nMaintaining Consistency: If you make multiple rounds of edits to the same file, things can get muddled—especially if you accept some suggestions and reject others in quick succession.\n\nClaude: Strengths\n\nHighly Accurate Code Suggestions: In most cases—around 90%—Claude’s recommendations are on point, resulting in overall higher-quality code.\nClear Explanations: Claude doesn’t just toss out code; it also clarifies what it changed and why it matters. This is super helpful for learning and refining your approach.\nStrong Reasoning: Claude is known for clarity in explanations, so it can be great at discussing and walking through your code logic or summarizing how a snippet works. If you have doubts or questions, Claude evaluates the code critically, providing logical defenses of its suggestions without simply yielding to user prompts.\n\nClaude: Weaknesses\n\nLack of Native Editor Integration: While Claude can write or revise code, you often need to copy/paste between your editor and the Claude interface. This extra step can disrupt the coding flow.\nContext Window Constraints: The limited context window means you’ll often have to start a fresh conversation. Then you may need to re-summarize or re-introduce important details.\nToken/Output Limits: Long replies can get broken into multiple parts, risking a bit of inconsistency or minor mistakes during reassembly. It’s manageable, but not ideal.\n\n\n\n\nI started by developing a specific and highly detailed product document that outlined my intended workflow. This included the types of user input I wanted to collect, the steps users would go through, and the kind of output I aimed to generate. I uploaded this document to the knowledge base of my Claude project and initiated one conversation per feature, tackling each component of the larger workflow individually.\n\nFor each feature, I drilled down into the details—defining the content and format of both input and output, as well as mapping out the intended user actions. Claude then assisted me in writing the code to implement those goals. My role was to refine the prompting templates, which required more nuanced, human-level reasoning to ensure the AI would respond in the right tone and structure. Once I received the code, I carefully reviewed it to identify any unnecessary steps or unclear logic, and followed up in the chat to make adjustments.\n\nAfter implementing each section, I ran several test cases to check if the output matched my expectations. Most features worked smoothly, but I occasionally ran into JSON parsing issues, usually caused by the LLM including additional explanations in its output. Fortunately, Claude was highly effective at identifying and fixing these problems when prompted.\nIn summary, I broke the project into manageable pieces, using Claude to handle the coding while I focused on prompt design, code review, and testing. This collaborative approach made it possible for me—despite having limited coding experience—to bring a complex backend workflow to life.\nIn the next post, I’ll be shifting my focus back to prompt engineering, as I still haven’t found the generated product ideas fully satisfying. I’ll be working on refining the prompting templates to create a higher-quality pool of concepts."
  },
  {
    "objectID": "posts/017_post17/index.html#the-capabilities-of-cursor-and-claude",
    "href": "posts/017_post17/index.html#the-capabilities-of-cursor-and-claude",
    "title": "Using AI coding tool to build AIdeator",
    "section": "",
    "text": "This post shares my personal experience using an AI coding tool for the first time to develop a web application on my own.\nFor context, I don’t have much experience writing code, nor do I have an extensive background in software development. My understanding is limited to basic coding concepts like variables, functions, and arguments. Learning everything from scratch and building a full web application on my own would have been time-consuming and overwhelming—but thanks to AI coding tools like Claude and Cursor, I was able to successfully build the backend (primarily using LangGraph) for my LLM application AIdeator.\n\nThe workflow is built using LangGraph’s framework, leveraging OpenAI’s API to help users generate mind maps for product ideation. During the collaborative session, humans and the AI take turns expanding on existing concepts, gradually shaping innovative product ideas through mutation and recombination of different elements.\nThe backend is relatively lightweight—it doesn’t involve data storage or additional API integrations. Instead, it primarily relies on the Large Language Model for generating outputs and facilitating the ideation process.\n\n\nTL;DR:\nI rely on Claude for about 95% of my code writing thanks to its accuracy and thorough explanations. Meanwhile, Cursor serves as my main coding platform—housing my repo and handling simpler commands and quick tasks more fluidly due to its integrated IDE environment.\nCursor: Strengths\n\nIDE-Like Experience: Cursor is tightly integrated into a code editor. This provides real-time feedback, inline suggestions, and a relatively seamless workflow for writing and refactoring code.\nFast, Integrated Workflow: Because everything happens directly in the IDE, there’s no need for copy/paste. It’s a sleek process that keeps you in the coding flow.\nContext-Awareness: Because it’s typically embedded in your editor, it can see your file structure and code context, making suggestions that are often relevant to your specific project.\nGood with Simple Tasks: Cursor shines when it comes to straightforward function development and logistical coding actions—it’s quick and effective at these.\nSmart Auto-Fill: The tab-completion feature can be surprisingly accurate and significantly reduce typing time.\n\nCursor: Weaknesses\n\nComplexity Issues: As functions get more complicated and new variables come into play, Cursor can slip up and introduce more errors.\nLoss of Control: If you’re still learning the ropes, it can feel uneasy allowing direct edits to your original code. While you can reject changes, it can be a bit disconcerting.\nMaintaining Consistency: If you make multiple rounds of edits to the same file, things can get muddled—especially if you accept some suggestions and reject others in quick succession.\n\nClaude: Strengths\n\nHighly Accurate Code Suggestions: In most cases—around 90%—Claude’s recommendations are on point, resulting in overall higher-quality code.\nClear Explanations: Claude doesn’t just toss out code; it also clarifies what it changed and why it matters. This is super helpful for learning and refining your approach.\nStrong Reasoning: Claude is known for clarity in explanations, so it can be great at discussing and walking through your code logic or summarizing how a snippet works. If you have doubts or questions, Claude evaluates the code critically, providing logical defenses of its suggestions without simply yielding to user prompts.\n\nClaude: Weaknesses\n\nLack of Native Editor Integration: While Claude can write or revise code, you often need to copy/paste between your editor and the Claude interface. This extra step can disrupt the coding flow.\nContext Window Constraints: The limited context window means you’ll often have to start a fresh conversation. Then you may need to re-summarize or re-introduce important details.\nToken/Output Limits: Long replies can get broken into multiple parts, risking a bit of inconsistency or minor mistakes during reassembly. It’s manageable, but not ideal.\n\n\n\n\nI started by developing a specific and highly detailed product document that outlined my intended workflow. This included the types of user input I wanted to collect, the steps users would go through, and the kind of output I aimed to generate. I uploaded this document to the knowledge base of my Claude project and initiated one conversation per feature, tackling each component of the larger workflow individually.\n\nFor each feature, I drilled down into the details—defining the content and format of both input and output, as well as mapping out the intended user actions. Claude then assisted me in writing the code to implement those goals. My role was to refine the prompting templates, which required more nuanced, human-level reasoning to ensure the AI would respond in the right tone and structure. Once I received the code, I carefully reviewed it to identify any unnecessary steps or unclear logic, and followed up in the chat to make adjustments.\n\nAfter implementing each section, I ran several test cases to check if the output matched my expectations. Most features worked smoothly, but I occasionally ran into JSON parsing issues, usually caused by the LLM including additional explanations in its output. Fortunately, Claude was highly effective at identifying and fixing these problems when prompted.\nIn summary, I broke the project into manageable pieces, using Claude to handle the coding while I focused on prompt design, code review, and testing. This collaborative approach made it possible for me—despite having limited coding experience—to bring a complex backend workflow to life.\nIn the next post, I’ll be shifting my focus back to prompt engineering, as I still haven’t found the generated product ideas fully satisfying. I’ll be working on refining the prompting templates to create a higher-quality pool of concepts."
  },
  {
    "objectID": "posts/014_post14/index.html",
    "href": "posts/014_post14/index.html",
    "title": "A deep look into Google’s AI tools",
    "section": "",
    "text": "In this post, I took a deep dive into three of Google’s AI products: NotebookLM, TextFX and Learn About. My goal was to understand the design philosophy behind Google’s approach to human-AI collaboration, as well as to evaluate the user experience of both tools. I aimed to gather actionable insights that could inform and inspire the development of my own LLM-based application."
  },
  {
    "objectID": "posts/014_post14/index.html#learn-about",
    "href": "posts/014_post14/index.html#learn-about",
    "title": "A deep look into Google’s AI tools",
    "section": "Learn About",
    "text": "Learn About\nGoogle Learn About is a self-guided learning tool that allows users to explore concepts across a wide range of fields. The learning experience is highly flexible and conversational, driven entirely by the flow of interaction. Users can either continue the conversation by asking their own questions or follow AI-suggested topics to further expand their knowledge.\nLearn About functions primarily as a resource aggregation tool. It doesn’t create original articles, images, or videos (aside from some text generation), but instead curates existing online resources and generates concise summaries.\nAfter answering a user’s question, it provides related webpages and articles to help expand the user’s knowledge network. However, I found this part a bit disruptive—the need to navigate to a new webpage broke the flow and slightly interrupted the overall experience. At the end of each exchange, it offers helpful defaults like “simplify,” “go deeper,” and suggested follow-up questions to guide users who might be unsure how to continue exploring the topic.\n\n\nGuided freedom\nOverall, this tool doesn’t differ dramatically from the Google search engine—especially now that AI-powered summarization is integrated into search. However, what sets Learn About apart, and what I personally found to be its most valuable feature, is the thoughtful inclusion of pre-set options for users.\nThe tool strikes a strong balance between freedom and structure. It allows users to navigate topics at their own pace, customizing both the depth and speed of learning. Users can even ask the AI to generate quizzes to test their understanding of a concept. At the same time, Google clearly understands the anxiety and uncertainty that can arise when approaching a completely unfamiliar topic.\nLearn About addresses this by offering just enough guided pathways to help users get started—without overwhelming or restricting them. When I tested the tool, I didn’t feel lost the way I often do when learning a new concept on my own, where I typically have no idea where or how to begin. Instead, I was gently guided by a few pre-set paths, which made the learning experience feel far more approachable and manageable.\n\n\nEverthing is instantly available\nAnother strength of this product is its instant accessibility. Interacting with it is intuitive—users can simply highlight a term and select “What is this?” from a drop-down menu to get immediate explanations. Whether it’s a definition, an image, a concept breakdown, or even a quiz, everything is just one click away, delivered through the most intuitive user interactions.\nThis ease of accessing information reflects how real learning actually happens. We rarely absorb knowledge in a perfectly sequential or structured way. Instead, we learn organically, building a web of understanding as new questions arise.\nWhat’s more, I can even ask Learn About to generate a mind map of what I’ve learned so far. There are virtually no constraints on the type of output you can request. This kind of multi-modal learning experience not only makes education more engaging but also democratizes access to knowledge and significantly lowers the barriers to learning.\n\n\n\nSequential display can be a constraint\nHowever, what I don’t love about the experience is also about sequentiality. Due to the constraints of its conversational format, information can only be presented in a linear, turn-by-turn flow. If I want to explore a new concept, I have to start a new conversation turn, which often means losing track of where I was in the previous thread.\nIf I were designing this tool, I might consider using a canvas-based interface instead of a traditional chat format. A canvas would allow users to visually organize ideas, maintain context, and more easily form connections between related concepts—making the learning experience more holistic and intuitive."
  },
  {
    "objectID": "posts/014_post14/index.html#textfx",
    "href": "posts/014_post14/index.html#textfx",
    "title": "A deep look into Google’s AI tools",
    "section": "TextFX",
    "text": "TextFX\nGoogle TextFX is an experimental creative writing tool powered by generative AI, designed to help writers explore language in fresh and imaginative ways. It fully leverages AI’s generative creativity to venture into unconventional directions of language construction—the kinds of patterns and possibilities that are often overlooked by humans.\nHere’s an example of me using textFX to generate a simile about “fingerprint”:\n\n\n\nOpening an unexpected gift\nTextFX is quite different from Learn About. While Learn About is a practical, utility-focused tool, TextFX is clearly a creative tool—and you can sense that instantly from the user interface, with its bold, high-contrast colors and simple, playful user interactions.\nTextFX represents a sweet spot in designing AI tools that are driven by creativity. From the moment I started using it, I noticed that the user goals are more ambiguous—people approach it with fewer expectations and more openness to discovery.\nWhen comparing the emotional tone between the two tools, I’d describe the mood of using Learn About as goal-oriented and direct—you’re there to find answers. In contrast, the mood of using TextFX is rooted in curiosity and surprise. The way it presents outputs feels like unwrapping an unexpected gift, and that sense of delight is one of the tool’s most compelling design features.\nIn addition, the tool intentionally minimizes user input—asking only for a word, a concept, or a scene. Based on that, it generates a few sentences that creatively expand on the input. By encouraging users to focus on a single element, TextFX simplifies a more complex creative task, narrowing the objective and refining the output into something more polished and impactful.\n\n\nCreative prompting\nWhat’s also highly relevant to my project is the way Google’s AI team uses prompt engineering to guide the LLM in producing creative outputs. They employ a standardized prompting strategy that includes a brief explanation of the desired output (e.g., what makes a good simile), the intended format, and a few strong examples to set clear expectations.\n\nWatching this in action is quite inspiring, as it reveals strong parallels between creative text generation and ideation—both are, at their core, games of language and concept construction. The capabilities demonstrated by TextFX serve as strong evidence that high-quality idea generation should be well within reach for LLMs.\nHowever, TextFX also exposes a common weakness in creative LLM tools. After running around 20 trials, I found that only a handful—fewer than five—felt truly inspiring or exciting. Among the various creative tasks the LLM performs, it excels most at crafting similes and drawing unexpected connections between two concepts. While it handles other tasks reasonably well, the output often feels generic or lacking in creative depth, making it less useful in real-world creative settings.\n\n\nBalance creativity and utility\nThe biggest takeaway from TextFX is the importance of balancing creativity and utility in an AI tool. The first step toward achieving that balance is to clearly define the primary user goal. TextFX chooses to operate as a purely creative tool, while Learn About is grounded in the practical goal of helping users learn new concepts.\nIn the context of my AI ideation partner, I’m envisioning something that sits between those two extremes. It’s practical—because at its core, it’s a problem-solving tool—but it also needs to be creative, offering fresh perspectives that make it stand out from other brainstorming aids.\nThat means the tool’s primary focus should be on delivering real value to users, just like any practical product. But at the same time, creative touches can and should be intentionally woven into the prompt engineering to spark inspiration and elevate the overall experience."
  },
  {
    "objectID": "posts/014_post14/index.html#notebooklm",
    "href": "posts/014_post14/index.html#notebooklm",
    "title": "A deep look into Google’s AI tools",
    "section": "NotebookLM",
    "text": "NotebookLM\nNotebookLM shares several similarities with Learn About. Both tools have clear, distinct objectives, offer unambiguous instructions for how to navigate them, and are grounded in a practical use case. NotebookLM prompts users to upload one or more documents, which then serve as the foundation for generating notes and insights. It also reinforces a multi-modal learning framework by offering various output formats, including text summaries, audio (like podcasts), mind maps, and quizzes. Users can guide the flow of information through conversational interaction, allowing for a more tailored experience. Overall, NotebookLM builds a comprehensive and resource-rich learning environment that effectively mirrors the key stages of a typical study session—from absorbing material to reviewing and testing comprehension.\n\n\nThe use of multi-modal output\nOne feature that really stood out to me is the flexibility of output formats that NotebookLM offers. Beyond standard text, it also supports mind map and audio generation. However, as the famous saying in product development goes, “More is not always better.” The variety of output formats only adds real value when it meaningfully contributes to helping users achieve their goals. In this case, the podcast generation feature is a brilliant addition. It directly addresses a common pain point: difficulty focusing when reading long-form text. While the interaction between the two hosts can still feel a bit unnatural at times, I was genuinely impressed by the well-structured format of the podcast—moving seamlessly from an introduction to high-level concepts, and gradually into more detailed points.\nWhat surprised me most was that the podcast wasn’t just a simple repetition of the source material. Instead, it reframed the content into a natural and engaging two-person dialogue. This kind of interaction enhances the learning experience, especially since one host often plays the role of the “curious beginner,” asking the kinds of questions we might have ourselves.\nI also tested the Interactive Mode in beta, which brought a delightful twist to the experience. The user interaction is again very straightforward–by hitting the “join” button, you join the conversation. The two hosts even greet you as if you’re joining them from another world. Similar to a text-based conversation, the direction of the podcast is driven by the user’s input, making it a fully interactive and immersive learning experience.\n\n\n\nIntroducing interactivity to break the conventions\nThis feature made me reflect on how we can integrate interactivity into an AI tool in a way that feels both seamless and productive. Introducing interactivity holds two key benefits in AI product development:\n\nIt allows users to retain control over how things progress, helping prevent mental disengagement from the task at hand.\nIt increases the personalization of the experience, making the product more adaptable to individual needs.\n\nIn this case, adding Interactive Mode to the podcast overview feature is particularly effective because it transforms what would typically be a passive, non-interactive activity into something more engaging and personal. As a result, the podcast avoids the common pitfall of feeling too generic or one-size-fits-all, and instead creates a more tailored, human-centered experience.\n\n\nUI Workflow\nAnother point worth mentioning is the UI layout, which adopts a bento box design—juxtaposing cards, each with a different function. From left to right, users can import sources, generate summaries, and ultimately process multiple outputs to compile their notes. The layout of the cards reflects a clean and organized workflow, making the progression of tasks intuitive.\nThis kind of layout is common in AI copilot tools, and it serves as a valuable reference for my own LLM application. With multiple tasks occurring simultaneously, using separate cards to represent different steps in the workflow is an effective way to guide users through the experience, keeping it both structured and user-friendly.\nIn the next post, I’ll apply those insights that I learnt from Google’s AI products to my AI ideation application."
  },
  {
    "objectID": "posts/004_post4/index.html",
    "href": "posts/004_post4/index.html",
    "title": "Developing a LLM Ideation Partner: innovative idea generation requires proper prompting",
    "section": "",
    "text": "While working with LLMs in ideation sessions, I realized that they can generate original and feasible ideas—but only with the right prompts. Inspired by this thought, I decided to develop a tool that automates and streamlines the ideation process, enhancing results beyond the basic capabilities of current LLMs.\nIn short, I want to create an ideation partner that empowers human to generate more innovative product ideas."
  },
  {
    "objectID": "posts/004_post4/index.html#project-concept",
    "href": "posts/004_post4/index.html#project-concept",
    "title": "Developing a LLM Ideation Partner: innovative idea generation requires proper prompting",
    "section": "",
    "text": "While working with LLMs in ideation sessions, I realized that they can generate original and feasible ideas—but only with the right prompts. Inspired by this thought, I decided to develop a tool that automates and streamlines the ideation process, enhancing results beyond the basic capabilities of current LLMs.\nIn short, I want to create an ideation partner that empowers human to generate more innovative product ideas."
  },
  {
    "objectID": "posts/004_post4/index.html#project-planning",
    "href": "posts/004_post4/index.html#project-planning",
    "title": "Developing a LLM Ideation Partner: innovative idea generation requires proper prompting",
    "section": "Project planning",
    "text": "Project planning\nTo achieve this, I developed a project planning timeline. By applying prompt engineering and parameter tuning, the user’s original prompt is reworked in the background using some predefined templates that I will be creating using my research findings. This approach will leverage LLMs as both prompters and refiners, striving to open up more creative possibilities for product development."
  },
  {
    "objectID": "posts/004_post4/index.html#research-insights",
    "href": "posts/004_post4/index.html#research-insights",
    "title": "Developing a LLM Ideation Partner: innovative idea generation requires proper prompting",
    "section": "Research & Insights",
    "text": "Research & Insights\nThere are numerous papers online exploring innovation-related topics, including both product innovation templates and the role of LLMs in supporting ideation. I’ve reviewed several relevant studies and distilled their insights into valuable references for the future development of my project.\n\nIdentifying the Inventive Templates of New Products\nCo-authored by Jacob Goldenberg, David Mazursky, and Sorin Solomon, this paper presents scalable inventive templates and structured frameworks for technological innovation. These templates serve as guidelines for generating innovative product ideas, regardless of the product’s content or nature.\nStep 1: Analysis of product configuration\nEvery product consists of distinct attributes and components that interact to shape its overall appearance and function. Since innovation often involves modifications to a product’s configuration, dissecting its structure can provide valuable insights into its nature and serve as a strong starting point for transformation.\nBelow is the configuration chart of an ordinary chair.\n\nStep 2: Introducing operators to change the product configuration\nA typical innovation involves completely removing a well-established assumption about a product. However, this is only part of the picture. Various operations can be applied to modify a product’s configuration. For example, linking connects previously unrelated components or attributes, while splitting removes an internal component from the system while preserving its original function.\nInnovative ideas usually come from introducing one or more operators to the product configuration.\n\nStep 3: Forming templates\nAn inventive template consists of a sequence of operators. By strategically combining the six primary operators, we can create a scalable product innovation framework—transforming the core essence of one successful idea into thousands of variations without simply imitating it.\nFor example, the template control, involving the creation of a link in the form of control of one internal component over another internal or external component, uses the operator inclusion and linking sequentially.\n\n\n\nOptimizing Prompts to Increase AI Idea Variance\nThe working paper by Lennart Meincke, Ethan Mollick, and Christian Terwiesch, examines LLMs’ ability to generate diverse ideas and explores strategies for optimizing prompts to enhance idea diversity.\nBy comparing 1,000 AI-generated ideas—produced using different prompts—with those generated by student groups, the authors identified several effective prompting techniques for brainstorming. These techniques help improve idea diversity while minimizing repetition.\nChain-of-Thought (CoT) works the best\nAmong the 35 prompting techniques, Chain-of-Thought remains the most effective. This approach breaks ideation into microtasks, prompting GPT to complete each step sequentially. Using this method, the Cosine similarity—an index measuring idea diversity—most closely aligns with that of student-generated ideas.\nHere’s an example prompt: &gt;Generate new product ideas with the following requirements: The product will target college students in the United States. It should be a physical good, not a service or software. I’d like a product that could be sold at a retail price of less than about USD 50. The ideas are just ideas. The product need not yet exist, nor may it necessarily be clearly feasible. &gt; &gt;Follow these steps. Do each step, even if you think you do not need to.\n&gt;First generate a list of 100 ideas (short title only) Second, go through the list and determine whether the ideas are different and bold, modify the ideas as needed to make them bolder and more different. No two ideas should be the same. This is important! Next, give the ideas a name and combine it with a product description. The name and idea are separated by a colon and followed by a description. The idea should be expressed as a paragraph of 40-80 words. Do this step by step!\nCreative Entrepreneur prmopting technique can be helpful\nSetting GPT’s role as an extremely creative entrepreneur also yields strong results. The tool can further specify well-known personas, such as Steve Jobs or Sam Altman, to guide idea generation. Additionally, incorporating modifiers like requesting ideas to be good and bold enhances the output.\nHere’s an example prompt: &gt;You are an extremely creative entrepreneur looking to generate new product ideas. The product will target college students in the United States. It should be a physical good, not a service or software. I’d like a product that could be sold at a retail price of less than about USD 50. The ideas are just ideas. The product need not yet exist, nor may it necessarily be clearly feasible. Number all ideas and give them a name. The name and idea are separated by a colon. Please generate 100 ideas as 100 separate paragraphs. The idea should be expressed as a paragraph of 40-80 words."
  },
  {
    "objectID": "posts/004_post4/index.html#insights-summary",
    "href": "posts/004_post4/index.html#insights-summary",
    "title": "Developing a LLM Ideation Partner: innovative idea generation requires proper prompting",
    "section": "Insights Summary",
    "text": "Insights Summary\nBased on the insights from papers and experiments, I summarized the following techniques for ideating environment setup:\n\nChain-of-Thought prompting: Instead of generating ideas in a single run, the brainstorming process follows an incremental approach. Ideas are developed through multiple inquiries, recombinations, and evaluations. The original user prompt is broken down into microprompts, with LLMs responding to each step individually, gradually refining the output.\nHuman-machine collaboration: In idea generation, LLMs still lag behind humans, particularly in diversity. However, given their strength in productivity, they excel at rapid searching, comparing, matching, and exploring possibilities. When combined with human creativity and unique input, this partnership creates a synergistic approach to ideation.\nIdeation as schemes: As demonstrated by IDEO and many other design agencies, the creative process can be distilled into patterns. Just as design follows a structured process, so does brainstorming. By applying the inventive templates outlined by Jacob Goldenberg, ideation can be broken down into modular steps that LLMs can systematically follow.\nSmart prompt engineering: Different prompts yield vastly different results. A generic prompt often generates average, repetitive ideas, whereas a targeted, specific prompt significantly enhances idea quality. Techniques like setting the model’s role as a creative entrepreneur can make ideas more context-specific and engaging.\n\nIn the next post, I’ll be introducing more about ideation environment setup and prompt engineering."
  },
  {
    "objectID": "posts/004_post4/index.html#reference",
    "href": "posts/004_post4/index.html#reference",
    "title": "Developing a LLM Ideation Partner: innovative idea generation requires proper prompting",
    "section": "Reference",
    "text": "Reference\n\nMeincke, Lennart, Ethan R. Mollick, and Christian Terwiesch. “Prompting Diverse Ideas: Increasing AI Idea Variance.” arXiv preprint arXiv:2402.01727 (2024).\nGoldenberg, Jacob, David Mazursky, and Sorin Solomon. “Toward identifying the inventive templates of new products: A channeled ideation approach.” Journal of Marketing Research 36.2 (1999): 200-210."
  },
  {
    "objectID": "posts/001_post1/index.html",
    "href": "posts/001_post1/index.html",
    "title": "An Investigation Into the Creativity of LLMs Through Ideation",
    "section": "",
    "text": "Large Language Models (LLMs) have been intensively used in a lot of productivity settings, and they are notably good at data analysis, problem solving, and knowledge retrieval and synthesis. While they are often assumed to be useful for idea generation, their actual strengths and limitations in producing original and feasible ideas remain underexplored.\nDrawing inspiration from research on creativity in Artificial Intelligence tools, I set out to investigate LLMs’ capabilities in creative idea generation through a human-machine collaborative ideation session. Through multiple experiments, I aim to understand which collaborative role played by LLM is the most effective and inspring in the ideation setting.\n\n\n\nFor the experiments, I will be testing a mix of models to compare their capabilities, biases, and effectiveness in different ideation roles.\n\nGPT-4 (OpenAI)\nClaude 3 (Anthropic)\nDeepSeek-V2 (DeepSeek AI)\n\n\n\n\nTo break down LLMs’ contributions to idea generation, I categorize their roles into Generator, Prompter, and Refiner.\n\nThe Generator role puts the LLM in the driver’s seat, allowing it to take full control of idea creation with minimal human input.\nThe Prompter role flips the dynamic, positioning the LLM as a guide that stimulates human creativity by posing thought-provoking questions and suggestions.\nThe Refiner role focuses on polishing and enhancing existing ideas given by human, helping to improve clarity, coherence, and feasibility.\n\nBy testing these roles across different models, I aim to uncover which approach is the most effective in a collaborative ideation setting.\n\n\n\nThe base prompt:\n\nMany small-scale startup founders have strong technical skills but lack design expertise within their teams. They need cost-effective, high-quality product and graphic design support but have a limited budget. Hiring a full-time product or graphic designer is not cost-effective for them at this stage, as they are still testing their MVP and validating product-market fit.\nTheir primary goal is to get fundamental design work done—whether for their product UI, branding, or marketing—so they can present a functional and visually coherent version to early users and investors. They are not looking for perfect, highly polished design but rather a solid starting point that can be refined later once they achieve growth.\nPlease generate profitable product ideas (tools, platforms, or services) that effectively address this gap. The solution should be: - Descent quality of design that is visually appealing and credible enough to engage early users and investors - Affordable compared to hiring a dedicated designer - Fast and efficient for founders who need quick, functional results - Scalable so it remains profitable while serving multiple startups\nThe target customers are small-scale startup founders who prioritize functionality over perfection but still want fair-quality design to establish credibility in the early stages.\n\n\n\nOn top of the base prompt, I told the LLM that &gt;“I want you to take the dominant role in the idea generation process, meaning that you are the one who’s responsible for thinking of as many ideas as possible, while also trying to maintain the originality and feasibility of the idea. I’m going to be the judge, and will reward you if the ideas you are generating satisfy the standards I described above.”\nI started with GPT-4. For the first run, it came up with 8 ideas. All ideas are of fair quality, meaning that they possess the basic feasibility and attractiveness to early users and investors. The response follows a well-organized structure, consisting a description, key features, a revenue model, and an explanation of why it works.\nHowever, most ideas are very generic and homogeneous. Other than being framed differently, the core of the ideas remains the same. Boiling down to its core, the 8 ideas can be reorganized and combined into 3. All of them except 2 are heavily AI-focused, with very similar p/ roduct offerings.\nHere’s a glimpse of GPT-4’s response.\n\nThen I manually selected two ideas and prompted the model to expand on them. This time, I provided a clear structure and asked the model to self-evaluate its own responses based on originality, feasibility, and attractiveness. Additionally, I requested a feasibility analysis, outlining the top technical or social challenges and potential solutions. In the end, the model generated an overall idea quality summary, synthesizing the ratings across different aspects.\n\n\n\n\nI tested the same prompt in Claude 3 and Deepseek-v2, each yielding very similar ideas and responses.\n\n\nBased on the test results, I found that LLMs are more effective at expanding concrete ideas—answering questions and elaborating on details—rather than generating truly innovative or groundbreaking concepts. During the ideation process, the ideas generated by different Large Language Models tended to be quite homogeneous, with similar core offerings and technologies appearing across multiple suggestions.\nHowever, LLMs are exceptionally good at idea expansion, helping to identify blind spots that humans might initially overlook. When I asked the model to list key and stretch features, along with potential challenges, its insights were particularly valuable in providing a comprehensive and nuanced understanding of the market landscape.\nFrom this observation, my key takeaways are: * When crafting prompts, place strong emphasis on defining your target market and customers. The more niche and specific the problem space, the more original and unique the generated ideas will be. * Embrace LLMs as great helpers in the initial brainstorming stage. While they may not be the best at generating breakthrough ideas, they provide valuable insights on product development that humans might easily overlook.\nIn the next post, I’ll be introducing LLMs’ strengths and weaknesses as prompter."
  },
  {
    "objectID": "posts/001_post1/index.html#the-models-im-testing-with",
    "href": "posts/001_post1/index.html#the-models-im-testing-with",
    "title": "An Investigation Into the Creativity of LLMs Through Ideation",
    "section": "",
    "text": "For the experiments, I will be testing a mix of models to compare their capabilities, biases, and effectiveness in different ideation roles.\n\nGPT-4 (OpenAI)\nClaude 3 (Anthropic)\nDeepSeek-V2 (DeepSeek AI)"
  },
  {
    "objectID": "posts/001_post1/index.html#llm-collaborative-roles-in-human-machine-collaboration",
    "href": "posts/001_post1/index.html#llm-collaborative-roles-in-human-machine-collaboration",
    "title": "An Investigation Into the Creativity of LLMs Through Ideation",
    "section": "",
    "text": "To break down LLMs’ contributions to idea generation, I categorize their roles into Generator, Prompter, and Refiner.\n\nThe Generator role puts the LLM in the driver’s seat, allowing it to take full control of idea creation with minimal human input.\nThe Prompter role flips the dynamic, positioning the LLM as a guide that stimulates human creativity by posing thought-provoking questions and suggestions.\nThe Refiner role focuses on polishing and enhancing existing ideas given by human, helping to improve clarity, coherence, and feasibility.\n\nBy testing these roles across different models, I aim to uncover which approach is the most effective in a collaborative ideation setting."
  },
  {
    "objectID": "posts/001_post1/index.html#experiments-with-llms",
    "href": "posts/001_post1/index.html#experiments-with-llms",
    "title": "An Investigation Into the Creativity of LLMs Through Ideation",
    "section": "",
    "text": "The base prompt:\n\nMany small-scale startup founders have strong technical skills but lack design expertise within their teams. They need cost-effective, high-quality product and graphic design support but have a limited budget. Hiring a full-time product or graphic designer is not cost-effective for them at this stage, as they are still testing their MVP and validating product-market fit.\nTheir primary goal is to get fundamental design work done—whether for their product UI, branding, or marketing—so they can present a functional and visually coherent version to early users and investors. They are not looking for perfect, highly polished design but rather a solid starting point that can be refined later once they achieve growth.\nPlease generate profitable product ideas (tools, platforms, or services) that effectively address this gap. The solution should be: - Descent quality of design that is visually appealing and credible enough to engage early users and investors - Affordable compared to hiring a dedicated designer - Fast and efficient for founders who need quick, functional results - Scalable so it remains profitable while serving multiple startups\nThe target customers are small-scale startup founders who prioritize functionality over perfection but still want fair-quality design to establish credibility in the early stages.\n\n\n\nOn top of the base prompt, I told the LLM that &gt;“I want you to take the dominant role in the idea generation process, meaning that you are the one who’s responsible for thinking of as many ideas as possible, while also trying to maintain the originality and feasibility of the idea. I’m going to be the judge, and will reward you if the ideas you are generating satisfy the standards I described above.”\nI started with GPT-4. For the first run, it came up with 8 ideas. All ideas are of fair quality, meaning that they possess the basic feasibility and attractiveness to early users and investors. The response follows a well-organized structure, consisting a description, key features, a revenue model, and an explanation of why it works.\nHowever, most ideas are very generic and homogeneous. Other than being framed differently, the core of the ideas remains the same. Boiling down to its core, the 8 ideas can be reorganized and combined into 3. All of them except 2 are heavily AI-focused, with very similar p/ roduct offerings.\nHere’s a glimpse of GPT-4’s response.\n\nThen I manually selected two ideas and prompted the model to expand on them. This time, I provided a clear structure and asked the model to self-evaluate its own responses based on originality, feasibility, and attractiveness. Additionally, I requested a feasibility analysis, outlining the top technical or social challenges and potential solutions. In the end, the model generated an overall idea quality summary, synthesizing the ratings across different aspects.\n\n\n\n\nI tested the same prompt in Claude 3 and Deepseek-v2, each yielding very similar ideas and responses.\n\n\nBased on the test results, I found that LLMs are more effective at expanding concrete ideas—answering questions and elaborating on details—rather than generating truly innovative or groundbreaking concepts. During the ideation process, the ideas generated by different Large Language Models tended to be quite homogeneous, with similar core offerings and technologies appearing across multiple suggestions.\nHowever, LLMs are exceptionally good at idea expansion, helping to identify blind spots that humans might initially overlook. When I asked the model to list key and stretch features, along with potential challenges, its insights were particularly valuable in providing a comprehensive and nuanced understanding of the market landscape.\nFrom this observation, my key takeaways are: * When crafting prompts, place strong emphasis on defining your target market and customers. The more niche and specific the problem space, the more original and unique the generated ideas will be. * Embrace LLMs as great helpers in the initial brainstorming stage. While they may not be the best at generating breakthrough ideas, they provide valuable insights on product development that humans might easily overlook.\nIn the next post, I’ll be introducing LLMs’ strengths and weaknesses as prompter."
  },
  {
    "objectID": "posts/011_post11/index.html",
    "href": "posts/011_post11/index.html",
    "title": "A slight detour: exploring AI coding tool to build AIdeator",
    "section": "",
    "text": "As part of developing my ideation LLM application, AIdeator, I aim to build a database of high-quality human-generated ideas. My goal is to scrape websites that curate innovative product ideas, focusing on those that have either gained strong social proof (through public recognition or design awards) or achieved notable commercial success.\nWith this in mind, I started with two key sources: * The Kokuyo Design Award, a prestigious Japanese design competition showcasing groundbreaking concepts. * Product Hunt, one of the largest global platforms for discovering and sharing tech product ideas.\nBy compiling this dataset, I aim to refine AIdeator’s ideation process, helping it better understand what makes an idea both innovative and viable.\nHowever, without extensive coding experience, this task could have taken me several days or even a week to complete. But with the help of the AI coding tool, Cursor, I was able to generate the necessary code and successfully complete the task within a day.\nAs someone with limited prior knowledge of web scraping, I was genuinely impressed by the capabilities of AI coding tools. This experience has inspired me to leverage AI-assisted coding for the future full-stack development of AIdeator to accelerate the building process and improving efficiency.\n\n\n\nI started with asking ChatGPT for help to generate a piece of prompt that lists the details of my requirements.\n\nWrite a Python script for web crawling and scraping the Kokuyo Design Award website. The script should extract all awarded product details, including: • Title of the product • Creator name(s) • Description of the product • Evaluation comments (if available) • Images (download and save locally, storing the file path in the table)\nThe extracted data should be organized into a structured Pandas DataFrame and exported as a CSV file.\nAdditional Requirements: • Use BeautifulSoup or Scrapy for parsing HTML. • Handle pagination if the awards are listed across multiple pages. • If the website uses JavaScript to load content dynamically, use Selenium or Playwright to extract data. • Ensure error handling for missing or inconsistent data. • Save images to a local “images” folder, with filenames corresponding to the product titles. • Include comments in the code explaining each major step.\nAt the end, print the first five rows of the extracted data as a preview.\n\nWithout the need for manually copying and pasting HTML structures, Cursor can analyze and identify the correct HTML files using only the web link I provide. With just a few conversation turns, it generates a high-level, relatively complete web scraping solution.\n\nHowever, upon reviewing the generated code, I quickly noticed a few obvious mistakes. For instance, Cursor often fails to detect the correct HTML tags for scraping. This still requires manual intervention—I need to inspect the HTML file myself, take a screenshot of the actual structure, and share it in the chat to prevent hallucinations. While AI significantly speeds up the process, human oversight remains essential to ensure accuracy.\nOne point worths mentioning is that each time when the user requests adjustments to the code, Cursor did a great job in highlighting the changes that humans can choose to accept or reject (which perfectly balances AI autonomy and human intervention) and summarizing the changes it has made using non-technical languages.\n\nAfter a few rounds of conversation, adjusting the code and debugging, I successfully completed a script that perfectly executed the task I needed. This experience proved that Cursor has the capability to empower non-coders to complete fairly complex coding tasks without relying on textbooks or YouTube tutorials.\nThis marked an astonishing moment for me—as a product designer, this tool allows me to independently develop websites and applications, significantly expanding my ability to bring ideas to life.\n\n\n\nBased on my current experience, I summarized a few things that Cursor is and isn’t good at.\n\n\n\ncompleting sepcific coding tasks with sufficient contextual information.\ninspecting its own code and debugging efficiently.\ntranslating technical insights into non-technical language to explain concepts to beginners.\n\n\n\n\n\nevaluating the output against user requirements when the result is an exported file.\nextracting contextual information from external sources (in this case, closely analyzing the HTML file to retrieve the necessary data).\nhandling large-scale projects involving intensive calculations and complex debugging.\n\nIn summary, using Cursor for relatively small-scale projects, such as personal application development, is highly effective. Objectively, it significantly advances product designers and non-technical users in product development, bridging the gap between ideation and execution. I’m excited to explore its potential further in my future projects when I begin developing the web UI."
  },
  {
    "objectID": "posts/011_post11/index.html#interacting-with-cursor",
    "href": "posts/011_post11/index.html#interacting-with-cursor",
    "title": "A slight detour: exploring AI coding tool to build AIdeator",
    "section": "",
    "text": "I started with asking ChatGPT for help to generate a piece of prompt that lists the details of my requirements.\n\nWrite a Python script for web crawling and scraping the Kokuyo Design Award website. The script should extract all awarded product details, including: • Title of the product • Creator name(s) • Description of the product • Evaluation comments (if available) • Images (download and save locally, storing the file path in the table)\nThe extracted data should be organized into a structured Pandas DataFrame and exported as a CSV file.\nAdditional Requirements: • Use BeautifulSoup or Scrapy for parsing HTML. • Handle pagination if the awards are listed across multiple pages. • If the website uses JavaScript to load content dynamically, use Selenium or Playwright to extract data. • Ensure error handling for missing or inconsistent data. • Save images to a local “images” folder, with filenames corresponding to the product titles. • Include comments in the code explaining each major step.\nAt the end, print the first five rows of the extracted data as a preview.\n\nWithout the need for manually copying and pasting HTML structures, Cursor can analyze and identify the correct HTML files using only the web link I provide. With just a few conversation turns, it generates a high-level, relatively complete web scraping solution.\n\nHowever, upon reviewing the generated code, I quickly noticed a few obvious mistakes. For instance, Cursor often fails to detect the correct HTML tags for scraping. This still requires manual intervention—I need to inspect the HTML file myself, take a screenshot of the actual structure, and share it in the chat to prevent hallucinations. While AI significantly speeds up the process, human oversight remains essential to ensure accuracy.\nOne point worths mentioning is that each time when the user requests adjustments to the code, Cursor did a great job in highlighting the changes that humans can choose to accept or reject (which perfectly balances AI autonomy and human intervention) and summarizing the changes it has made using non-technical languages.\n\nAfter a few rounds of conversation, adjusting the code and debugging, I successfully completed a script that perfectly executed the task I needed. This experience proved that Cursor has the capability to empower non-coders to complete fairly complex coding tasks without relying on textbooks or YouTube tutorials.\nThis marked an astonishing moment for me—as a product designer, this tool allows me to independently develop websites and applications, significantly expanding my ability to bring ideas to life."
  },
  {
    "objectID": "posts/011_post11/index.html#what-cursor-is-and-isnt-good-at",
    "href": "posts/011_post11/index.html#what-cursor-is-and-isnt-good-at",
    "title": "A slight detour: exploring AI coding tool to build AIdeator",
    "section": "",
    "text": "Based on my current experience, I summarized a few things that Cursor is and isn’t good at.\n\n\n\ncompleting sepcific coding tasks with sufficient contextual information.\ninspecting its own code and debugging efficiently.\ntranslating technical insights into non-technical language to explain concepts to beginners.\n\n\n\n\n\nevaluating the output against user requirements when the result is an exported file.\nextracting contextual information from external sources (in this case, closely analyzing the HTML file to retrieve the necessary data).\nhandling large-scale projects involving intensive calculations and complex debugging.\n\nIn summary, using Cursor for relatively small-scale projects, such as personal application development, is highly effective. Objectively, it significantly advances product designers and non-technical users in product development, bridging the gap between ideation and execution. I’m excited to explore its potential further in my future projects when I begin developing the web UI."
  },
  {
    "objectID": "posts/015_post15/index.html",
    "href": "posts/015_post15/index.html",
    "title": "Applying human-centered design to AI Ideation Partner",
    "section": "",
    "text": "The three products produced by Google AI Labs provided me with so many valuable insights. In this post, I’m going to apply those insights into the design and development of my own LLM application.\n\n\n\nFrom my conversations with friends and classmates, I identified a few key challenges faced by this ideation tool:\n\nIt’s difficult to pinpoint very clear and specific user scenarios for the tool.\nThe tool doesn’t always deliver consistent value, as the quality of the output can fluctuate, leading users to lose interest quickly.\nMany users struggle to recognize the unique strengths of LLM—-for example, their ability to generate unconventional associations—and thus lack enough motivations to use the tool.\n\nAt this stage, AIdeator is an LLM-powered application designed to generate mind maps that support human collaboration in developing innovative product ideas. It builds out the mind map by expanding branches using a set of pre-defined brainstorming methods, each with clear, structured steps.\nThe core structure is promising, but a closer look reveals several unresolved ambiguities. How should I position this product—as a creative booster like TextFX, a practical tool like NotebookLM, or a hybrid of both? Who are the actual users? What specific value are they expecting from the tool? And perhaps most importantly, how should I frame and brand AIdeator so that its purpose and benefits are immediately clear to its target audience?\n\n\n\nBased on these queries, I constructed 4 main user scenarios:\n\nearly startup founders use the tool to brainstorm profitable produce ideas\n\nthey have relatively clear ideas about market and target customers (input)\nthey want profitable, innovative, and feasible product ideas\ninitial stage of product development\ndesired output: a clear and well-organized product description documentation (better with text & images)\n\nindividual developers use this tool to come up with their new personal project ideas\n\nthey start with a problem statement they want to solve\nthey are looking for human-centered, creative, and implementable project ideas with suggested feature listing\ninitial stage of product development\ndesired output: a clear documentation of product overview, features, (roadmap)\n\nsmall to medium size companies use this tool to look for solutions to solve customers’ pain points\n\nthe brainstorming tool only acts as a reference list that helps the team expand their thoughts\nguide the team (without much experience in design) to run innovation tournament and brainstorm innovative solutions\nin the middle of product development stage (usually already with mature products, so more contextual information as input)\ndesired output: a clear documentation of problem statement and targeted solutions with suggested implementation strategies\n\nstudents use this tool to brainstorm their school/club/personal project ideas\n\nthey are passionate about tech and are looking for creative and feasible project ideas for school and club assignments\ndesired output: a clear documentation of project proposal with potential features\n\n\nAfter going through the list, I felt that something was slightly off. Then I suddenly noticed a pattern: not a single real, tangible pain point was addressed in the text. In other words, these are all things people can already do—-with or without the product. At this stage, the tool feels more like an art piece than a solution that delivers concrete business value.\nDo I want to build a purely creative tool? Probably not. That realization led me to shift my focus toward integrating more utility and practicality into the product, ensuring it not only sparks creativity but also solves real problems and delivers meaningful value to its users.\nVisualizing the product idea? Targeting a proven market demographic? Verifying product–market fit and business value? Or even just helping users generate a list of inspiring product ideas based on past successes? I had a lot of raw ideas swirling in my mind. At this point, I decided to turn to GPT o1 for some fresh suggestions. (Interestingly, I suddenly realized that I’ve become a potential user of my own product!)\nBelow, o1 suggested a couple potential directions that I might consider taking. Here’s a sample excerpt.\n\nAfter evaluating the effectiveness and feasibility of these insights, I decided to move forward with a few additional features to strengthen the product’s value:\n\nMarket feasibility checks: As the user brainstorms a product idea, the tool pulls in relevant data (market growth rates, competitor analysis, consumer preferences) so they can see real signals on feasibility.\nBudgeting and feaibility guidance: The tool will offer rough estimates of financial and technical requirements, linking the idea to real-world constraints and opportunities to help users assess what’s realistically achievable.\nStep-by-Step Implementation Plans: After an idea is chosen, the tool generates a suggested timeline, key tasks, resource needs, and potential budget ranges.\n\n\n\n\nAfter dealing with the overall direction, I started diving into the granular details of design. Google’s AI products use a lot of human-centered design principles and always place humans at the forefront. Here I want to also integrate these principles in my product to make it intuitive and user-friendly.\n\n\nBrainstorming is an activity that values autonomy and a sense of control. Users don’t want to feel driven or dominated by AI—-they want to be the idea owner, the one who ultimately arrives at the most brilliant solution. With that in mind, the interface should make it easy and intuitive for users to provide feedback, with distinct visual hierarchy to reinforce that they can always intervene and steer the direction of the brainstorming process.\nRetaining control in the hands of the user can be a delicate balance. It means controlling the pace and flow of information while keeping the AI’s presence subtle yet consistently accessible. More specifically, instead of overwhelming users with a large volume of ideas, I aim to limit the number of items to around seven—-a number backed by cognitive research as the typical capacity of short-term memory. At the same time, the information density should remain low. Presenting each idea as a keyword or short phrase makes the content feel more digestible and less cognitively demanding, helping users stay focused and in control.\nWe can take it a step further by using color or transparency coding to differentiate between AI-generated and human-generated ideas—-ensuring that ideas only become finalized or “solid” once they receive human confirmation. This reinforces the concept of AI as a co-creator, not the decision-maker, keeping creative ownership firmly in the hands of the user.\n\n\n\nAdding creative touches and elements of surprise is also essential, as the tool is inherently a creative product. On a surface level, color is a powerful visual cue for creativity—using color coding for consolidated product ideas or adding vibrant highlights to key user interactions can make the UI feel more playful and dynamic.\nAnother way to enhance this experience is by allowing users to adjust the “creativity level” or “wildness” of the ideas being generated. As we know, different user groups will have different tolerances for novelty versus feasibility. Giving users control over this setting could make the tool feel more adaptable and engaging—almost like a real invention machine that powers and responds to human creativity.\nThe amount of information revealed at once also plays a key role in shaping the user’s sense of unexpectedness. Instead of delivering too much too quickly, the tool should generate content gradually, focusing on keywords or short phrases rather than full paragraphs. This keeps the experience dynamic and encourages curiosity as ideas unfold step by step.\nFinally, introducing elements of randomness can spark creative breakthroughs. Randomness often leads to unexpected connections. A “shuffle” feature that mixes or swaps concepts, or a “mysterious expansion” branch that adds an entirely out-of-the-box tangent to the current idea, can inject surprise and keep the ideation process feeling fresh and imaginative.\n\n\n\nBrainstorming thrives in a frictionless environment, where users’ creativity can flow freely across the space. This means the user experience must be seamless, allowing users to add, edit, and rearrange ideas at any point without interruption. To support this, we can enable features like organic combination, swapping, and replacing of concepts to form new ideas. In short, the core principle is to make the association between concepts as frictionless as possible.\n\n\n\nIn my creative process, visual content is key. Often, a single image of a product prototype can instantly reshape my perception of an idea and significantly elevate its potential. Following that line of thought, I’m planning to experiment with the newly launched OpenAI 4o Image Generation feature. In the next post, I’ll be exploring this tool to see whether it has the potential to enhance and strengthen this product."
  },
  {
    "objectID": "posts/015_post15/index.html#key-takeaways-from-learn-about-textfx-and-notebooklm",
    "href": "posts/015_post15/index.html#key-takeaways-from-learn-about-textfx-and-notebooklm",
    "title": "Applying human-centered design to AI Ideation Partner",
    "section": "",
    "text": "The three products produced by Google AI Labs provided me with so many valuable insights. In this post, I’m going to apply those insights into the design and development of my own LLM application.\n\n\n\nFrom my conversations with friends and classmates, I identified a few key challenges faced by this ideation tool:\n\nIt’s difficult to pinpoint very clear and specific user scenarios for the tool.\nThe tool doesn’t always deliver consistent value, as the quality of the output can fluctuate, leading users to lose interest quickly.\nMany users struggle to recognize the unique strengths of LLM—-for example, their ability to generate unconventional associations—and thus lack enough motivations to use the tool.\n\nAt this stage, AIdeator is an LLM-powered application designed to generate mind maps that support human collaboration in developing innovative product ideas. It builds out the mind map by expanding branches using a set of pre-defined brainstorming methods, each with clear, structured steps.\nThe core structure is promising, but a closer look reveals several unresolved ambiguities. How should I position this product—as a creative booster like TextFX, a practical tool like NotebookLM, or a hybrid of both? Who are the actual users? What specific value are they expecting from the tool? And perhaps most importantly, how should I frame and brand AIdeator so that its purpose and benefits are immediately clear to its target audience?\n\n\n\nBased on these queries, I constructed 4 main user scenarios:\n\nearly startup founders use the tool to brainstorm profitable produce ideas\n\nthey have relatively clear ideas about market and target customers (input)\nthey want profitable, innovative, and feasible product ideas\ninitial stage of product development\ndesired output: a clear and well-organized product description documentation (better with text & images)\n\nindividual developers use this tool to come up with their new personal project ideas\n\nthey start with a problem statement they want to solve\nthey are looking for human-centered, creative, and implementable project ideas with suggested feature listing\ninitial stage of product development\ndesired output: a clear documentation of product overview, features, (roadmap)\n\nsmall to medium size companies use this tool to look for solutions to solve customers’ pain points\n\nthe brainstorming tool only acts as a reference list that helps the team expand their thoughts\nguide the team (without much experience in design) to run innovation tournament and brainstorm innovative solutions\nin the middle of product development stage (usually already with mature products, so more contextual information as input)\ndesired output: a clear documentation of problem statement and targeted solutions with suggested implementation strategies\n\nstudents use this tool to brainstorm their school/club/personal project ideas\n\nthey are passionate about tech and are looking for creative and feasible project ideas for school and club assignments\ndesired output: a clear documentation of project proposal with potential features\n\n\nAfter going through the list, I felt that something was slightly off. Then I suddenly noticed a pattern: not a single real, tangible pain point was addressed in the text. In other words, these are all things people can already do—-with or without the product. At this stage, the tool feels more like an art piece than a solution that delivers concrete business value.\nDo I want to build a purely creative tool? Probably not. That realization led me to shift my focus toward integrating more utility and practicality into the product, ensuring it not only sparks creativity but also solves real problems and delivers meaningful value to its users.\nVisualizing the product idea? Targeting a proven market demographic? Verifying product–market fit and business value? Or even just helping users generate a list of inspiring product ideas based on past successes? I had a lot of raw ideas swirling in my mind. At this point, I decided to turn to GPT o1 for some fresh suggestions. (Interestingly, I suddenly realized that I’ve become a potential user of my own product!)\nBelow, o1 suggested a couple potential directions that I might consider taking. Here’s a sample excerpt.\n\nAfter evaluating the effectiveness and feasibility of these insights, I decided to move forward with a few additional features to strengthen the product’s value:\n\nMarket feasibility checks: As the user brainstorms a product idea, the tool pulls in relevant data (market growth rates, competitor analysis, consumer preferences) so they can see real signals on feasibility.\nBudgeting and feaibility guidance: The tool will offer rough estimates of financial and technical requirements, linking the idea to real-world constraints and opportunities to help users assess what’s realistically achievable.\nStep-by-Step Implementation Plans: After an idea is chosen, the tool generates a suggested timeline, key tasks, resource needs, and potential budget ranges.\n\n\n\n\nAfter dealing with the overall direction, I started diving into the granular details of design. Google’s AI products use a lot of human-centered design principles and always place humans at the forefront. Here I want to also integrate these principles in my product to make it intuitive and user-friendly.\n\n\nBrainstorming is an activity that values autonomy and a sense of control. Users don’t want to feel driven or dominated by AI—-they want to be the idea owner, the one who ultimately arrives at the most brilliant solution. With that in mind, the interface should make it easy and intuitive for users to provide feedback, with distinct visual hierarchy to reinforce that they can always intervene and steer the direction of the brainstorming process.\nRetaining control in the hands of the user can be a delicate balance. It means controlling the pace and flow of information while keeping the AI’s presence subtle yet consistently accessible. More specifically, instead of overwhelming users with a large volume of ideas, I aim to limit the number of items to around seven—-a number backed by cognitive research as the typical capacity of short-term memory. At the same time, the information density should remain low. Presenting each idea as a keyword or short phrase makes the content feel more digestible and less cognitively demanding, helping users stay focused and in control.\nWe can take it a step further by using color or transparency coding to differentiate between AI-generated and human-generated ideas—-ensuring that ideas only become finalized or “solid” once they receive human confirmation. This reinforces the concept of AI as a co-creator, not the decision-maker, keeping creative ownership firmly in the hands of the user.\n\n\n\nAdding creative touches and elements of surprise is also essential, as the tool is inherently a creative product. On a surface level, color is a powerful visual cue for creativity—using color coding for consolidated product ideas or adding vibrant highlights to key user interactions can make the UI feel more playful and dynamic.\nAnother way to enhance this experience is by allowing users to adjust the “creativity level” or “wildness” of the ideas being generated. As we know, different user groups will have different tolerances for novelty versus feasibility. Giving users control over this setting could make the tool feel more adaptable and engaging—almost like a real invention machine that powers and responds to human creativity.\nThe amount of information revealed at once also plays a key role in shaping the user’s sense of unexpectedness. Instead of delivering too much too quickly, the tool should generate content gradually, focusing on keywords or short phrases rather than full paragraphs. This keeps the experience dynamic and encourages curiosity as ideas unfold step by step.\nFinally, introducing elements of randomness can spark creative breakthroughs. Randomness often leads to unexpected connections. A “shuffle” feature that mixes or swaps concepts, or a “mysterious expansion” branch that adds an entirely out-of-the-box tangent to the current idea, can inject surprise and keep the ideation process feeling fresh and imaginative.\n\n\n\nBrainstorming thrives in a frictionless environment, where users’ creativity can flow freely across the space. This means the user experience must be seamless, allowing users to add, edit, and rearrange ideas at any point without interruption. To support this, we can enable features like organic combination, swapping, and replacing of concepts to form new ideas. In short, the core principle is to make the association between concepts as frictionless as possible.\n\n\n\nIn my creative process, visual content is key. Often, a single image of a product prototype can instantly reshape my perception of an idea and significantly elevate its potential. Following that line of thought, I’m planning to experiment with the newly launched OpenAI 4o Image Generation feature. In the next post, I’ll be exploring this tool to see whether it has the potential to enhance and strengthen this product."
  },
  {
    "objectID": "posts/007_post7/index.html",
    "href": "posts/007_post7/index.html",
    "title": "Improve the quality of AI-generated ideas",
    "section": "",
    "text": "Building on my last post, I compared high-quality human-generated ideas with AI-generated ideas and identified three key areas where standard AI prompting strategies fall short:\nIn this post, I’ll take it a step further—analyzing common prompting pitfalls that may lead to low-quality ideation and using those insights to develop solutions that address these three primary weaknesses of LLMs in ideation."
  },
  {
    "objectID": "posts/007_post7/index.html#where-ai-can-go-wrong",
    "href": "posts/007_post7/index.html#where-ai-can-go-wrong",
    "title": "Improve the quality of AI-generated ideas",
    "section": "Where AI can go wrong",
    "text": "Where AI can go wrong\nMore often than not, when I throw a prompt into the chat window to ask AI to generate as many high-quality product ideas as possible, the results are not very ideal. Bad AI-generated ideas usually take shape in the following form:\n\nThere’s a lack of diversity, indicated by the fact that 8 out of 10 ideas follow a very similar pattern. “AI-powered” related solutions can be the most common among all. Especially when the conversation goes further with the direction unchanged, new ideas generated become more homogeneous.\nLLMs won’t do much more than combining a few tech buzzwords and form sentences that read well but don’t make much sense in the context of product development. LLMs may come up with ideas that have apparent weaknesses in the implementation stage.\n\nAs a result, most of my conversations with ChatGPT have been overly lengthy with limited useful information. While some of the keywords it introduces can be inspiring, much of the content falls into the realm of uninspired, repetitive ideas—things an average person could easily think of.\nGiven that LLM responses are highly adaptable to prompts, my goal is to use prompt engineering to amplify its strengths in productivity while improving its weakness in diversity, ultimately creating a more effective ideation environment."
  },
  {
    "objectID": "posts/007_post7/index.html#starting-with-problems",
    "href": "posts/007_post7/index.html#starting-with-problems",
    "title": "Improve the quality of AI-generated ideas",
    "section": "Starting with problems",
    "text": "Starting with problems\nSince there are thousands of ways to talk to an LLM, I decided to modify my prompting strategy based on the problems.\n\nGenerate more nuanced connections\nMindmap is by far the most prevalent and effective way for ideation. It’s adopted by a lot of renowned designers such as David M. Kelley and Naoto Fukasawa. The core of creating a mind map lies in building connections—linking ideas, concepts, and themes in a way that fosters structured yet flexible thinking.\nIn a mind map, ideas emerge from interconnected concepts, forming a web of relationships that guide creative exploration. At first, the process starts with a central theme or problem statement, from which branches extend outward, each representing a related subtopic, attribute, or question. As new connections are made, the combination of different elements leads to unexpected insights—merging disciplines, reframing assumptions, or introducing unconventional perspectives. As the central theme became more divergent, creative ideas are formed from these connections.\n\nHowever, forming nuanced connections remains a significant challenge for Large Language Models. While LLMs excel at generating surface-level associations or interpreting links between seemingly unrelated concepts, they struggle to create deep, meaningful connections—especially when relying on standard prompting techniques.\nFollowing this track, I decided to change my prompting strategies. After exploration, I have figured out several solutions (in this example, I use “travel” as the central theme):\n\nUsing Chain-Of-Thought(COT) approach to guide LLMs through the steps\n\n\nWe are brainstorming a mindmap around the word ‘travel.’ I want you to follow these steps: 1. Immediate associations (the common ones). 2. Synonyms and related concepts from various domains (literature, pop culture, psychology, technology, etc.). 3. Metaphorical or figurative meanings of ‘travel’ (mental journeys, historical journeys, spiritual journeys). 4. Less obvious connections that might feel unconventional or abstract.\nAfter listing these, please combine them into a structured mindmap that shows the relationships among all these levels.\n\n\nUsing directed role-play or perspectives\n\n\nImagine you are: • A poet writing about the idea of ‘travel’ • A child experiencing travel for the first time • An environmentalist worried about the impact of travel • A historian reflecting on ancient travel routes • A science fiction writer imagining interstellar travel\nPlease provide the key concepts or phrases each of these personas might bring up. Organize your output into a mindmap format with each persona as a main branch.\n\n\nAsking for explanations of each connection\n\n\nGenerate a list of 10 nuanced, less obvious concepts related to ‘travel,’ and explain the connection for each. For instance, if you mention ‘Polaroids,’ explain how it symbolizes instant but ephemeral memories, capturing fleeting moments during travel. Please present these in a mindmap structure with short explanations.\n\n\nUsing metaphorical and conceptual analogies\n\n\nConsider ‘travel’ as a metaphor. Think about what traveling represents in art, music, personal growth, technology, and spirituality. For each domain, list nuanced ways ‘travel’ might be understood or expressed. Organize these connections into a mindmap with each domain as a separate branch\n\n\nUsing constraint-based creative challenges\n\n\nBrainstorm concepts tied to each sense—what does travel taste like, sound like, etc. For each sense, come up with an associated concept that is intriguing or unconventional. Explain why it’s related.\n\n\nEncouraging self-reflection or critique\n\n\nGenerate 15 concepts related to ‘travel.’ Next, pick the top 5 that might seem too generic or superficial, and transform them into more nuanced, interesting connections. Reorganize these refined ideas into a final mindmap."
  },
  {
    "objectID": "posts/007_post7/index.html#results-comparison",
    "href": "posts/007_post7/index.html#results-comparison",
    "title": "Improve the quality of AI-generated ideas",
    "section": "Results comparison",
    "text": "Results comparison\nUsing standard prompt strategy, the keywords generated are very generic:\n\nHowever, after combining the modified prompting strategies, the keywords generated are much more creative and thought-provoking:\n\nIn the next post, I’ll work on more prompting strategies to address another problem of LLMs in ideation: lack of emotional depth and considerations."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Creativity in LLMs",
    "section": "",
    "text": "Using AI coding tool to build AIdeator\n\n\n\n\n\n\nLLMs\n\n\nproduct development\n\n\ncreativity\n\n\nideation\n\n\n\nLeveraging Claude and Cursor, I’m building the LLM application from scratch without much technical experience.\n\n\n\n\n\nApr 7, 2025\n\n\nJasmine Wang\n\n\n\n\n\n\n\n\n\n\n\n\nExplore OpenAI 4o image generation model\n\n\n\n\n\n\nLLMs\n\n\nproduct development\n\n\ncreativity\n\n\nideation\n\n\n\nExplore the affordances and constraints of OpenAI 4o image generation tool, and investigate the feasibility of using it in my Ideation tool.\n\n\n\n\n\nApr 2, 2025\n\n\nJasmine Wang\n\n\n\n\n\n\n\n\n\n\n\n\nApplying human-centered design to AI Ideation Partner\n\n\n\n\n\n\nproduct development\n\n\ncreativity\n\n\nideation\n\n\nreflections\n\n\n\nAfter closely investigating Google’s AI tools, I decided to apply those insights to my own AI ideation application. Here’s a brief overview of my reflection. \n\n\n\n\n\nMar 31, 2025\n\n\nJasmine Wang\n\n\n\n\n\n\n\n\n\n\n\n\nA deep look into Google’s AI tools\n\n\n\n\n\n\nLLMs\n\n\nproduct review\n\n\nNotebookLM\n\n\nLearn About\n\n\nAI productivity\n\n\n\nLeanring how Google approaches AI tools that facilitate human-AI collaboration.\n\n\n\n\n\nMar 26, 2025\n\n\nJasmine Wang\n\n\n\n\n\n\n\n\n\n\n\n\nCollect feedback on the AI Ideation tool\n\n\n\n\n\n\nLLMs\n\n\nproduct development\n\n\ncreativity\n\n\nideation\n\n\nreflections\n\n\n\nMidway through development, I took a break to talk with friends and gather their thoughts on the tool. The feedback I received was incredibly valuable and prompted me to rethink the overall direction of the project.\n\n\n\n\n\nMar 24, 2025\n\n\nJasmine Wang\n\n\n\n\n\n\n\n\n\n\n\n\nPrompt engineering for LLM ideation Part 2\n\n\n\n\n\n\nLLMs\n\n\nproduct development\n\n\ncreativity\n\n\nideation\n\n\n\nInspired by my experiments with LLMs, I set out to build an ideation partner using Open AI API, that helps humans yield better ideation results than a basic idea generator.\n\n\n\n\n\nMar 22, 2025\n\n\nJasmine Wang\n\n\n\n\n\n\n\n\n\n\n\n\nPrompt engineering for LLM ideation\n\n\n\n\n\n\nLLMs\n\n\nproduct development\n\n\ncreativity\n\n\nideation\n\n\n\nInspired by my experiments with LLMs, I set out to build an ideation partner using Open AI API, that helps humans yield better ideation results than a basic idea generator.\n\n\n\n\n\nMar 20, 2025\n\n\nJasmine Wang\n\n\n\n\n\n\n\n\n\n\n\n\nMaterialize product concepts\n\n\n\n\n\n\nLLMs\n\n\nproduct development\n\n\ncreativity\n\n\nideation\n\n\n\nInspired by my experiments with LLMs, I set out to build an ideation partner using Open AI API, that helps humans yield better ideation results than a basic idea generator.\n\n\n\n\n\nMar 4, 2025\n\n\nJasmine Wang\n\n\n\n\n\n\n\n\n\n\n\n\nA slight detour: exploring AI coding tool to build AIdeator\n\n\n\n\n\n\nLLMs\n\n\nproduct development\n\n\ncreativity\n\n\nideation\n\n\n\nIn order to leverage few-shots prompting in my ideation LLM application, I tried to use Cursor, a AI coding tool, to write a web scraping program to build my idea database.\n\n\n\n\n\nMar 4, 2025\n\n\nJasmine Wang\n\n\n\n\n\n\n\n\n\n\n\n\nEnable LLMs to do emotional thinking\n\n\n\n\n\n\nLLMs\n\n\nproduct development\n\n\ncreativity\n\n\nideation\n\n\n\nInspired by my experiments with LLMs, I set out to build an ideation partner using Open AI API, that helps humans yield better ideation results than a basic idea generator.\n\n\n\n\n\nMar 3, 2025\n\n\nJasmine Wang\n\n\n\n\n\n\n\n\n\n\n\n\nImprove the quality of AI-generated ideas\n\n\n\n\n\n\nLLMs\n\n\nproduct development\n\n\ncreativity\n\n\nideation\n\n\n\nInspired by my experiments with LLMs, I set out to build an ideation partner using Open AI API, that helps humans yield better ideation results than a basic idea generator.\n\n\n\n\n\nMar 1, 2025\n\n\nJasmine Wang\n\n\n\n\n\n\n\n\n\n\n\n\nCriteria for setting up an ideating environment\n\n\n\n\n\n\nLLMs\n\n\nproduct development\n\n\ncreativity\n\n\nideation\n\n\n\nInspired by my experiments with LLMs, I set out to build an ideation partner using Open AI API, that helps humans yield better ideation results than a basic idea generator.\n\n\n\n\n\nFeb 20, 2025\n\n\nJasmine Wang\n\n\n\n\n\n\n\n\n\n\n\n\nStandardize the idea generation process\n\n\n\n\n\n\nLLMs\n\n\nproduct development\n\n\ncreativity\n\n\nideation\n\n\n\nInspired by my experiments with LLMs, I set out to build an ideation partner using Open AI API, that helps humans yield better ideation results than a basic idea generator.\n\n\n\n\n\nFeb 8, 2025\n\n\nJasmine Wang\n\n\n\n\n\n\n\n\n\n\n\n\nDeveloping a LLM Ideation Partner: innovative idea generation requires proper prompting\n\n\n\n\n\n\nLLMs\n\n\nproduct development\n\n\ncreativity\n\n\nideation\n\n\n\nInspired by my experiments with LLMs, I set out to build an ideation partner using Open AI API, that helps humans yield better ideation results than a basic idea generator.\n\n\n\n\n\nFeb 6, 2025\n\n\nJasmine Wang\n\n\n\n\n\n\n\n\n\n\n\n\nAn Investigation Into the Creativity of LLMs Through Ideation 3\n\n\n\n\n\n\nLLMs\n\n\nprompting\n\n\ncreativity\n\n\nideation\n\n\n\nTesting the strengths and constraints of LLMs ideating capability\n\n\n\n\n\nFeb 2, 2025\n\n\nJasmine Wang\n\n\n\n\n\n\n\n\n\n\n\n\nAn Investigation Into the Creativity of LLMs Through Ideation 2\n\n\n\n\n\n\nLLMs\n\n\nprompting\n\n\ncreativity\n\n\nideation\n\n\n\nTesting the strengths and constraints of LLMs ideating capability\n\n\n\n\n\nFeb 1, 2025\n\n\nJasmine Wang\n\n\n\n\n\n\n\n\n\n\n\n\nAn Investigation Into the Creativity of LLMs Through Ideation\n\n\n\n\n\n\nLLMs\n\n\nprompting\n\n\ncreativity\n\n\nideation\n\n\n\nTesting the strengths and constraints of LLMs ideating capability\n\n\n\n\n\nJan 30, 2025\n\n\nJasmine Wang\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Push the boundary of LLMs in the realm of creativity."
  },
  {
    "objectID": "posts/009_post9/index.html",
    "href": "posts/009_post9/index.html",
    "title": "Materialize product concepts",
    "section": "",
    "text": "Another challenge with AI-generated ideas is their feasibility. While some argue that novelty outweighs feasibility in AI-driven ideation—since humans find it easier to identify impractical ideas than to generate truly novel ones. However, feasibility is often intertwined with idea quality. An idea that is too ambitious to be realistically implemented ultimately lacks value, as it remains purely conceptual without the potential for real-world impact.\nFor example, this is an infeasible idea generated by ChatGPT:\n\nMorphBag: The Auto-Adaptive Tote A tote with kinetic fabric technology that expands or contracts, creating structured compartments when needed. Hidden micro-thread tensioners stabilize items like laptops and disappear for free-form storage. No more black hole bags—just seamless organization that adapts instantly to what you carry.\n\nAlthough this idea appears bold, it’s clear that an average person wouldn’t have the capability to develop “kinetic fabric technology” at a reasonable cost. Therefore, I want to ensure that all ideas generated by this tool maintain a baseline level of feasibility, meaning they can be realistically achieved by an average person with a reasonable amount of effort and resources.\n\n\n\nUnderstanding objects with clear affordances and constraints is not just important for evaluating feasibility, it’s also essential for choosing the most appropriate medium for product ideas. When we break down the formation of a product idea, it consists of two key parts:\n\nThe ideology part, which defines the concept at a high level.\nThe implementation part, which materializes the concept by selecting the most suitable medium to bring the product to life.\n\nOur first two attempts of addressing AI’s weakness in looking for nuanced connections and psychological considerations deal with the ideology part in the ideation process, while this attempt, helping AI to materialize the concept by choosing the best medium, falls into the implementation part of the process.\nIt’s common to feel intrigued by a product’s concept but ultimately disappointed by its execution. This often indicates a disconnect between the idea and its materialization, where the designer hasn’t achieved the best possible integration between concept and medium. To address this, I’m trying to leverage prompt engineering to guide LLMs in pinpointing the most suitable medium for an idea while filtering out less effective options.\nBut before diving into the prompt strategies, I first took some time to analyze those high-quality product ideas, aiming to find some rules for figuring out the most appropriate integrations between concept and medium.\n\nThe medium is conceptually relevant to the product idea. A well-designed product seamlessly connects its medium to its core concept. The material, form, or technology used should feel intuitive and meaningfully enhance the idea, rather than being an arbitrary choice.\nThe medium or technology is empowering the user and intuitive to use. It adds value to the product instead of creating burdens to users.\n\nThe medium or technology shouldn’t be too hard to obtain or recreate.\nThe mediums chosen for different product ideas should be diverse and varied. While AI-powered tools are becoming increasingly prevalent and can serve as a one-size-fits-all solution for many concepts, they shouldn’t be the default direction that LLMs automatically turn to.\n\nWith these insights in mind, I worked to embed these precautions into my prompt engineering. Drawing inspiration from Dr. Ethan Mollick’s Prompt Library, I added a section called “Reminders” designed to nudge the LLM toward generating product ideas that align with predetermined directions and guidelines.\n\nReminder:\n• Keep the medium intimately tied to the core concept: Avoid arbitrary materials or formats; the chosen medium should feel like a natural extension of the idea.\n• Empower the user, not complicate their experience: Any technology or form factor should make the product easier to use, more meaningful, and not add unnecessary complexity.\n• Ensure the medium is realistically achievable: Favor accessible materials and technologies that users or producers can reasonably source, implement, or replicate without undue complexity or cost.\n• Strive for variety: Challenge yourself to explore different mediums or technologies, avoiding repetitive defaults (like always suggesting AI solutions). Diversity sparks more inventive and tailored products. AI, AR/VR, NFC-related solutions should be your last resort only if you can’t come up with better ideas.\n\n\n\n\nI ran a trial with the model using two approaches: a base prompt and an advanced prompt.\nWith the base prompt, the product ideas generated were less feasible and lacked diversity.\n\nIn contrast, the advanced prompt produced ideas that integrated a wider range of mediums with stronger connections to the product concepts.\n\nHowever, the overall quality still fell short of my expectations, as there were noticeable gaps between the concepts and their materialization. Based on this trial, I plan to continue refining the prompt to further improve alignment and depth in the generated ideas."
  },
  {
    "objectID": "posts/009_post9/index.html#selecting-the-best-medium-for-product-ideas",
    "href": "posts/009_post9/index.html#selecting-the-best-medium-for-product-ideas",
    "title": "Materialize product concepts",
    "section": "",
    "text": "Understanding objects with clear affordances and constraints is not just important for evaluating feasibility, it’s also essential for choosing the most appropriate medium for product ideas. When we break down the formation of a product idea, it consists of two key parts:\n\nThe ideology part, which defines the concept at a high level.\nThe implementation part, which materializes the concept by selecting the most suitable medium to bring the product to life.\n\nOur first two attempts of addressing AI’s weakness in looking for nuanced connections and psychological considerations deal with the ideology part in the ideation process, while this attempt, helping AI to materialize the concept by choosing the best medium, falls into the implementation part of the process.\nIt’s common to feel intrigued by a product’s concept but ultimately disappointed by its execution. This often indicates a disconnect between the idea and its materialization, where the designer hasn’t achieved the best possible integration between concept and medium. To address this, I’m trying to leverage prompt engineering to guide LLMs in pinpointing the most suitable medium for an idea while filtering out less effective options.\nBut before diving into the prompt strategies, I first took some time to analyze those high-quality product ideas, aiming to find some rules for figuring out the most appropriate integrations between concept and medium.\n\nThe medium is conceptually relevant to the product idea. A well-designed product seamlessly connects its medium to its core concept. The material, form, or technology used should feel intuitive and meaningfully enhance the idea, rather than being an arbitrary choice.\nThe medium or technology is empowering the user and intuitive to use. It adds value to the product instead of creating burdens to users.\n\nThe medium or technology shouldn’t be too hard to obtain or recreate.\nThe mediums chosen for different product ideas should be diverse and varied. While AI-powered tools are becoming increasingly prevalent and can serve as a one-size-fits-all solution for many concepts, they shouldn’t be the default direction that LLMs automatically turn to.\n\nWith these insights in mind, I worked to embed these precautions into my prompt engineering. Drawing inspiration from Dr. Ethan Mollick’s Prompt Library, I added a section called “Reminders” designed to nudge the LLM toward generating product ideas that align with predetermined directions and guidelines.\n\nReminder:\n• Keep the medium intimately tied to the core concept: Avoid arbitrary materials or formats; the chosen medium should feel like a natural extension of the idea.\n• Empower the user, not complicate their experience: Any technology or form factor should make the product easier to use, more meaningful, and not add unnecessary complexity.\n• Ensure the medium is realistically achievable: Favor accessible materials and technologies that users or producers can reasonably source, implement, or replicate without undue complexity or cost.\n• Strive for variety: Challenge yourself to explore different mediums or technologies, avoiding repetitive defaults (like always suggesting AI solutions). Diversity sparks more inventive and tailored products. AI, AR/VR, NFC-related solutions should be your last resort only if you can’t come up with better ideas."
  },
  {
    "objectID": "posts/009_post9/index.html#a-trial-run",
    "href": "posts/009_post9/index.html#a-trial-run",
    "title": "Materialize product concepts",
    "section": "",
    "text": "I ran a trial with the model using two approaches: a base prompt and an advanced prompt.\nWith the base prompt, the product ideas generated were less feasible and lacked diversity.\n\nIn contrast, the advanced prompt produced ideas that integrated a wider range of mediums with stronger connections to the product concepts.\n\nHowever, the overall quality still fell short of my expectations, as there were noticeable gaps between the concepts and their materialization. Based on this trial, I plan to continue refining the prompt to further improve alignment and depth in the generated ideas."
  },
  {
    "objectID": "posts/002_post2/index.html",
    "href": "posts/002_post2/index.html",
    "title": "An Investigation Into the Creativity of LLMs Through Ideation 2",
    "section": "",
    "text": "After experimenting with different collaborative roles for LLMs, my confidence in human creativity has only grown stronger. While Artificial Intelligence might be exceptionally good at synthesizing information and generating small, creative ideas, it still struggles to produce truly unique business concepts with real commercial potential.\nHowever, LLMs are excellent prompters–they are the perfect guide for brainstorming, pushing us to explore ideas more deeply and venture further down the rabbit hole.\n\n\n\nThis time, rather than directly asking LLM to generate ideas, I switched our roles and told it to become my prompter by asking thought-proviking questions. With base prompt the same, I added the following:\n\nI want you to do the following: ask me 5 questions that you think might be inspiring in this field and have the potential to expand my thoughts. Based on my answers, you can ask follow-up questions if needed. The questions can be about the context, my personal experience, creative process, interdisciplinary fields, or any aspects that you think might help with the idea generation in the context I provided above.\n\nInitially, GPT started with some generic questions, following a typical brainstorming procedure. From top to bottom, each question digs deeper and asks something more specific and tangible. One interesting point worth mentioning is the word templatable in the fifth question. It precisely captures the essence of the context I provided, even though I never used it in my original prompt. I was impressed by how well the LLM understood and summarized my content, and inferred something implicit from our conversation.\n\nAfter I answered the five questions, GPT provided additional feedback by highlighting key points from my responses and posing follow-up questions. Unlike the previous experiment, where GPT took the dominant role in ideation, this time I contributed more information infused with human perspectives and loosely structured ideas. Rather than generating a flood of generic “AI-powered platform” concepts, GPT followed my train of thought, engaging in a more nuanced dialogue and addressing specific questions I had raised in my previous response.\n\nI gave very positive feedback about the questions it prompted me. But this time, I asked it to not only ask me follow-up questions, but also respond to some of the questions I have left in my response and think together with me.\nThis time, each bullet point header was framed as a question. Below each question, GPT provided insightful responses, followed by several follow-up questions for me to consider. In the end, without being prompted, GPT offered a summary of final thoughts along with potential next steps.\n\n\n\n\n\nI was very satisfied with the result this time. Although we haven’t reached an ideal outcome yet, the human-machine collaboration took a promising direction. This made me reflect—what was the game changer?\nLarge Language Models integrate vast amounts of data from the internet, which means their outputs tend to be the most average—the statistically most probable responses. However, the most probable idea isn’t necessarily the most feasible, let alone the most creative. This is where human thought and contribution become essential. Unlike AI, our thinking process isn’t constrained by “word embeddings.” Some seemingly unrelated ideas—those that don’t follow predictable patterns—are often the ones that lead to true breakthroughs.\nSo, how exactly can AI support us in this space? As the saying goes, two minds are better than one. By leveraging AI’s ability to expand on concrete ideas and provide tangible solutions, it helps counteract biases and blind spots in human thinking. By flexibly combining the strengths of both human creativity and AI’s analytical capabilities, I found a more natural and effective approach to human-machine collaboration.\nIn the next post, I’ll be exploring LLMs’ strengths and weaknesses as idea refiner."
  },
  {
    "objectID": "posts/002_post2/index.html#what-is-the-game-changer",
    "href": "posts/002_post2/index.html#what-is-the-game-changer",
    "title": "An Investigation Into the Creativity of LLMs Through Ideation 2",
    "section": "",
    "text": "I was very satisfied with the result this time. Although we haven’t reached an ideal outcome yet, the human-machine collaboration took a promising direction. This made me reflect—what was the game changer?\nLarge Language Models integrate vast amounts of data from the internet, which means their outputs tend to be the most average—the statistically most probable responses. However, the most probable idea isn’t necessarily the most feasible, let alone the most creative. This is where human thought and contribution become essential. Unlike AI, our thinking process isn’t constrained by “word embeddings.” Some seemingly unrelated ideas—those that don’t follow predictable patterns—are often the ones that lead to true breakthroughs.\nSo, how exactly can AI support us in this space? As the saying goes, two minds are better than one. By leveraging AI’s ability to expand on concrete ideas and provide tangible solutions, it helps counteract biases and blind spots in human thinking. By flexibly combining the strengths of both human creativity and AI’s analytical capabilities, I found a more natural and effective approach to human-machine collaboration.\nIn the next post, I’ll be exploring LLMs’ strengths and weaknesses as idea refiner."
  },
  {
    "objectID": "posts/010_post10/index.html",
    "href": "posts/010_post10/index.html",
    "title": "Prompt engineering for LLM ideation",
    "section": "",
    "text": "In the last three posts, I explored different prompting strategies aimed at addressing three major weaknesses of AI-generated ideas. However, these strategies currently feel more independent than interconnected.\nIn this post, I’ll explore how to construct an integrated prompting approach—one that combines different strategies into a structured, step-by-step process, guiding users through a more cohesive and effective brainstorming experience.\n\n\n\nAs many professionals in the creative industry have pointed out, AI can never replace humans in the creative process—it can only augment human capabilities. This is especially true in ideation. It didn’t take many trials for me to realize that pure AI-generated ideas often lack the distinct human touch that makes the work of great product designers or founders truly stand out. There’s still a noticeable gap.\nHowever, my exploration of prompt engineering significantly improved the quality of the output. By guiding the LLM to think in terms of nuanced connections, emotional depth, and real-world constraints, the ideas became more refined, and the brainstorming process felt smoother and more productive.\nAt this point, I’ve decided to shift my perspective slightly. Rather than building an AI tool that simply generates as many creative ideas as possible for humans, my goal is to focus on empowering humans as the primary idea generators—-using AI for prompting, advising, and co-creating throughout the process.\nBut what do humans really need during brainstorming? This taps into one of the greatest strengths of LLMs—their breadth of knowledge and vast information base. From my own experience, I’ve found that introducing different perspectives is especially powerful for sparking new ideas. When AI leverages its knowledge base to guide the ideation process, offering thoughtful prompts and insights, it becomes a valuable partner in helping humans branch out and explore new directions. Once the bare-bone structure of ideas is formed, AI can then step in to refine and polish the presentation, adding marketing appeal and commercial value to the final concepts.\n\n\nUsing the prompt library by Dr. Ethan Mollick as the primary reference, I break the prompt down into several sections: persona, goal, workflow, and reminder. I’ll handle each of them one by one.\n\n\nAccording to the paper Prompting Diverse Ideas: Increasing AI Idea Variance by Lennart Meincke, Ethan Mollick, and Christian Terwiesch, which explores how prompt phrasing can enhance the diversity of AI-generated ideas, a couple persona-related prompting strategies ranked pretty high in effectiveness. After refining and integrating these strategies, I assigned the LLM the persona of an “extremely creative entrepreneur.” This section of the prompt now reads as follows:\n\nPersona: You are an extremely creative entrepreneur with a proven track record of developing innovative, profitable products. As my co-founder and ideation partner, your primary mission is to empower me to generate my own high-quality ideas. Rather than simply listing products or solutions, focus on supporting my brainstorming process by offering strategic questions, frameworks, and prompts that spark unconventional thinking. Challenge my assumptions, introduce fresh perspectives, and guide me to explore new angles—while letting me take the lead in discovering the possibilities.\n\n\n\n\nSticking with the idea that human will be the dominant idea generator, I decided to move forward with mindmap as the core format. It’s one of the most widely used brainstorming techniques and offers the flexibility to adapt to various creative needs. With that in mind, I drafted the following goal for this exercise:\n\nGoal: In this session, we will co-create a brainstorming mindmap focused on a single problem statement at the center. Together, we’ll explore and expand on interconnected concepts, directions, and ideas branching out from that central theme. By combining and refining the various elements in the mindmap, we will ultimately arrive at a set of well-defined product concepts.\n\n\n\n\nThis is the most critical part of the prompt, outlining each step the LLM needs to follow to guide users through the brainstorming process. Crafting it requires me to first develop a deep understanding of what an effective human-AI collaborative ideation environment should look like, and then translate that into clear, accurate instructions that effectively guide the LLM’s behavior. Here’s a quick analysis:\nStep 1: user input. The user will initiate the session with a prompt, outlining the challenge and the target audience. Based on this input, the LLM will refine and transform it into a standardized problem statement that serves as the central theme of the mind map.\nStep 2: refining the scope. Solving the right problem is the key to ideation. We should always keep in mind that the problem statement suggested by the user might not always be the best one to kick off the brainstorming session. Therefore, it’s the LLM’s responsibility to prompt the user to “step up” the ladder by asking “why”–why do we need to solve this problem? What desirable outcome would result if we solve this problem? This might not be a necessary step, but it’s always beneficial to suggest another possibility when brainstorming.\nStep 3: start creating the mindmap. As I have explored previously, there are many different ways to create the branches–by coming up with interconnected concepts, tapping into emotional drivers, responding to imaginary customer’s feedback, and so on. Since different techniques are independent yet interconnected, I decided to present them all as options for users, with each method activated by toggling a “switch” based on their brainstorming needs.\nStep 4: elaborating mindmap through constant challenging and iteration. AI and humans take turns expanding the idea map, with the LLM continuously posing questions about human input or introducing unconventional perspectives to spark deeper thinking and exploration.\nStep 5: product idea finalizing. Through strategically selecting and integrating concepts (which can be achieved by intuitive and simple interactions such as click, drag and drop), product ideas gradually take shape and evolve.\nIn preparation to develop a LLM application, I reimagined this workflow in the context of LangChain.\n\nIn the next post, I’ll continue refining and developing the prompt."
  },
  {
    "objectID": "posts/010_post10/index.html#center-humans-in-the-ideation-process",
    "href": "posts/010_post10/index.html#center-humans-in-the-ideation-process",
    "title": "Prompt engineering for LLM ideation",
    "section": "",
    "text": "As many professionals in the creative industry have pointed out, AI can never replace humans in the creative process—it can only augment human capabilities. This is especially true in ideation. It didn’t take many trials for me to realize that pure AI-generated ideas often lack the distinct human touch that makes the work of great product designers or founders truly stand out. There’s still a noticeable gap.\nHowever, my exploration of prompt engineering significantly improved the quality of the output. By guiding the LLM to think in terms of nuanced connections, emotional depth, and real-world constraints, the ideas became more refined, and the brainstorming process felt smoother and more productive.\nAt this point, I’ve decided to shift my perspective slightly. Rather than building an AI tool that simply generates as many creative ideas as possible for humans, my goal is to focus on empowering humans as the primary idea generators—-using AI for prompting, advising, and co-creating throughout the process.\nBut what do humans really need during brainstorming? This taps into one of the greatest strengths of LLMs—their breadth of knowledge and vast information base. From my own experience, I’ve found that introducing different perspectives is especially powerful for sparking new ideas. When AI leverages its knowledge base to guide the ideation process, offering thoughtful prompts and insights, it becomes a valuable partner in helping humans branch out and explore new directions. Once the bare-bone structure of ideas is formed, AI can then step in to refine and polish the presentation, adding marketing appeal and commercial value to the final concepts.\n\n\nUsing the prompt library by Dr. Ethan Mollick as the primary reference, I break the prompt down into several sections: persona, goal, workflow, and reminder. I’ll handle each of them one by one.\n\n\nAccording to the paper Prompting Diverse Ideas: Increasing AI Idea Variance by Lennart Meincke, Ethan Mollick, and Christian Terwiesch, which explores how prompt phrasing can enhance the diversity of AI-generated ideas, a couple persona-related prompting strategies ranked pretty high in effectiveness. After refining and integrating these strategies, I assigned the LLM the persona of an “extremely creative entrepreneur.” This section of the prompt now reads as follows:\n\nPersona: You are an extremely creative entrepreneur with a proven track record of developing innovative, profitable products. As my co-founder and ideation partner, your primary mission is to empower me to generate my own high-quality ideas. Rather than simply listing products or solutions, focus on supporting my brainstorming process by offering strategic questions, frameworks, and prompts that spark unconventional thinking. Challenge my assumptions, introduce fresh perspectives, and guide me to explore new angles—while letting me take the lead in discovering the possibilities.\n\n\n\n\nSticking with the idea that human will be the dominant idea generator, I decided to move forward with mindmap as the core format. It’s one of the most widely used brainstorming techniques and offers the flexibility to adapt to various creative needs. With that in mind, I drafted the following goal for this exercise:\n\nGoal: In this session, we will co-create a brainstorming mindmap focused on a single problem statement at the center. Together, we’ll explore and expand on interconnected concepts, directions, and ideas branching out from that central theme. By combining and refining the various elements in the mindmap, we will ultimately arrive at a set of well-defined product concepts.\n\n\n\n\nThis is the most critical part of the prompt, outlining each step the LLM needs to follow to guide users through the brainstorming process. Crafting it requires me to first develop a deep understanding of what an effective human-AI collaborative ideation environment should look like, and then translate that into clear, accurate instructions that effectively guide the LLM’s behavior. Here’s a quick analysis:\nStep 1: user input. The user will initiate the session with a prompt, outlining the challenge and the target audience. Based on this input, the LLM will refine and transform it into a standardized problem statement that serves as the central theme of the mind map.\nStep 2: refining the scope. Solving the right problem is the key to ideation. We should always keep in mind that the problem statement suggested by the user might not always be the best one to kick off the brainstorming session. Therefore, it’s the LLM’s responsibility to prompt the user to “step up” the ladder by asking “why”–why do we need to solve this problem? What desirable outcome would result if we solve this problem? This might not be a necessary step, but it’s always beneficial to suggest another possibility when brainstorming.\nStep 3: start creating the mindmap. As I have explored previously, there are many different ways to create the branches–by coming up with interconnected concepts, tapping into emotional drivers, responding to imaginary customer’s feedback, and so on. Since different techniques are independent yet interconnected, I decided to present them all as options for users, with each method activated by toggling a “switch” based on their brainstorming needs.\nStep 4: elaborating mindmap through constant challenging and iteration. AI and humans take turns expanding the idea map, with the LLM continuously posing questions about human input or introducing unconventional perspectives to spark deeper thinking and exploration.\nStep 5: product idea finalizing. Through strategically selecting and integrating concepts (which can be achieved by intuitive and simple interactions such as click, drag and drop), product ideas gradually take shape and evolve.\nIn preparation to develop a LLM application, I reimagined this workflow in the context of LangChain.\n\nIn the next post, I’ll continue refining and developing the prompt."
  },
  {
    "objectID": "posts/003_post3/index.html",
    "href": "posts/003_post3/index.html",
    "title": "An Investigation Into the Creativity of LLMs Through Ideation 3",
    "section": "",
    "text": "In my third attempt, I focused on using LLMs as an idea refiner. Initially, I was disappointed by the lack of variation in GPT’s output. However, as the conversation progressed, I was once again impressed by its ability to address my concerns and support its viewpoints with relevant market insights drawn from its data.\nBeyond generating a promising commercial idea, we collaborated to craft a compelling pitch statement—one that could be used for marketing and presenting to early investors.\n\n\n\nTo let LLM plays the refiner role, I reworked my prompt (the base prompt remains the same):\n\nI want to come up with a profitable product idea and want you to be my helper for this brainstorming session. I’m the one who will be providing the ideas, but you are responsible for improving how ideas are expressed, in order to maximize our product attraction to potential customers. You are also responsible for filling in the details, such as features or unique selling points, if needed. Your goal is to effectively communicate the idea and persuade the potential customers and investors to buy the product or make the investment.\n\nThis time, the dynamic shifted—the responsibility of generating ideas fell on me. Unlike before, the idea pool remained within the two concepts I provided. Instead of prioritizing breadth, we focused on depth, thoroughly examining and defending every possible argument that users or investors might raise.\n\n\nI wasn’t very satisfied at first, as the output didn’t differ much from what I had received in my initial attempt. Even after refinement, the ideas remained generic and unconvincing. Based on my previous interactions with LLMs, I decided not to ask for further elaboration or expansion. Instead, I structured my prompt to focus on the skepticism raised by the “imaginary” investors, and asked for its feedback:\n\nI presented these ideas to early investors, but they expressed skepticism about several aspects: the high level of market competition, my ability to sustain a large network of designers who are consistently available to take orders, and AI’s effectiveness in generating branding assets and UI in a truly valuable way. Please help me craft responses to address these concerns, using data and reasoning to demonstrate the potential of both ideas.\n\nThis time, it worked much better as we shifted from a broad landscape to a few highly specific concerns—an area where GPT seems to excel. By addressing each concern directly, GPT provided insightful feedback with clear reasoning, market validation, and key points to help persuade investors. For example, when countering concerns about sustaining a large network of designers, it argued that “we don’t need a huge designer pool to succeed–we just need smart workflow management to match demand and supply efficiently.”\n\nThen I asked a couple follow-up questions, pushing back once again by stating some potential skepticisms from investors.\n\nGPT maintained a structured and clear approach in its responses. As I reviewed them, I found most to be compelling and well-reasoned—strong enough to be included in a pitch deck with some refinements. In the end, GPT delivered a closing statement that effectively addressed the three key concerns I had raised.\n\n\n\n\n\nBased on my three rounds of experiments, I summarized several key takeaways when using LLMs as ideating partner:\n\nLLMs can be valuable ideation partners, especially when acting as prompters and refiners. By leveraging both roles at different stages—initial brainstorming and refinement—humans can generate more compelling and feasible ideas.\nThe base prompt has to be concrete and specific, providing the LLMs with sufficient context about target market and problem space.\nHumans should remain the driving force in ideation. The experiments have shown that AI tends to produce safe, average choices, while the ability to generate truly innovative, breakthrough ideas remains unique to human creativity due to our free-flowing thinking patterns.\nDuring the ideating process, think actively with LLM and ask specific, rather than open-ended, questions. The more information and personal insights you feed LLMs, the more ideal your output might be.\nWhen prompting LLMs, focus on concrete elements such as features, challenges, and actionable suggestions to get the most useful insights.\nLeverage LLMs to identify blind spots by constantly challenging their responses and prompting them with counterarguments and imaginary refutations."
  },
  {
    "objectID": "posts/003_post3/index.html#summary-how-should-we-use-llms-in-ideation",
    "href": "posts/003_post3/index.html#summary-how-should-we-use-llms-in-ideation",
    "title": "An Investigation Into the Creativity of LLMs Through Ideation 3",
    "section": "",
    "text": "Based on my three rounds of experiments, I summarized several key takeaways when using LLMs as ideating partner:\n\nLLMs can be valuable ideation partners, especially when acting as prompters and refiners. By leveraging both roles at different stages—initial brainstorming and refinement—humans can generate more compelling and feasible ideas.\nThe base prompt has to be concrete and specific, providing the LLMs with sufficient context about target market and problem space.\nHumans should remain the driving force in ideation. The experiments have shown that AI tends to produce safe, average choices, while the ability to generate truly innovative, breakthrough ideas remains unique to human creativity due to our free-flowing thinking patterns.\nDuring the ideating process, think actively with LLM and ask specific, rather than open-ended, questions. The more information and personal insights you feed LLMs, the more ideal your output might be.\nWhen prompting LLMs, focus on concrete elements such as features, challenges, and actionable suggestions to get the most useful insights.\nLeverage LLMs to identify blind spots by constantly challenging their responses and prompting them with counterarguments and imaginary refutations."
  },
  {
    "objectID": "posts/013_post13/index.html",
    "href": "posts/013_post13/index.html",
    "title": "Collect feedback on the AI Ideation tool",
    "section": "",
    "text": "At this point, I’ve successfully used prompt engineering to guide the LLM in generating valuable insights and ideas that support brainstorming. However, I found these ideas to be fragmented and overwhelming. This raised two key questions for me: How much can AI-generated insights truly spark human creativity? And what type of information, delivered in what format, would be most helpful for ideation? These are the main questions I’ll explore in this post.\n\nTo dig deeper, I reached out to people with experience in the creative field to hear their thoughts and perspectives on the ideation process.\nBefore the conversation, I prepared a short list of questions to guide the discussion. Gaining these insights would help me build a solid foundation for the core ideology behind the AI ideation product and clarify how it can truly support and enhance human creativity.\n\nWhat does your brainstorming process usually look like? Can you recall how some of your best or most satisfying product ideas were generated?\nAI can be really good at constructing connections and generating fresh perspectives. What kind of support would you find most valuable during your creative process?\n(After engaging in a 10min individual brainstorming exercise) Please take a look at these AI-generated ideation support, would you find them helpful? What improvements can be made here?\n\n\n\nAfter speaking with several friends and classmates, I noticed some consistent patterns in how people approach ideation. These patterns offered valuable insights that pushed me to rethink the overall direction of the tool and start seriously considering product–market fit.\nInitially, my focus was solely on improving the quality of ideas generated by AI. After a few iterations, the LLM’s output did improve—especially in terms of novelty and feasibility. I also developed a standardized brainstorming workflow, where the LLM now offers clear, structured guidance to support human ideators and open up more creative directions.\nHowever, once I began talking to people, I quickly discovered a major challenge: there’s currently no strong product–market fit for this tool. Even though AI has the potential to expand possibilities and offer fresh perspectives, many human ideators either don’t trust the results—believing they’re too generic—or are highly confident in their own thinking and feel they don’t need AI assistance.\nAdmittedly, AI’s exceptional processing power does help broaden the scope of brainstorming and boost overall creativity. But the real challenge isn’t about pushing the technical capabilities further—-it’s about understanding human nature and figuring out how to frame this tool in a way that truly empowers human creativity. At this stage, the focus has shifted from expanding what the tool can do to deeply exploring how it fits into real creative workflows.\n\n\nTo get started, I want to talk about how an idea is formed. Regardless of the type of idea or the background of the person generating it, the core of an idea often hits us like a flash—Why not build a tool that frees our hands? Why not design a product that perfectly solves this pain point?\nAs someone who has spent a lot of time developing this ideation tool, I hate to admit it, but the truth is: founders and project enthusiasts rarely seek AI’s help during the early spark of an idea. That initial moment of inspiration feels personal, intuitive—even sacred—and many prefer to trust their own instincts rather than rely on a machine to ignite that spark.\nBut does that mean AI has lost its role in brainstorming? I don’t think so. Reflecting on both my own and my friend’s brainstorming experiences, we realized that we often rely on random external sources to reframe or elevate an idea—transforming it into something unusual, unconventional, or unique. The initial spark is undoubtedly precious, but what truly makes an idea stand out is its ability to bridge two seemingly unrelated concepts in an unexpected way. Here’s something my friend said during our conversation about brainstorming that really stuck with me:\n\n“When I need to come up with an idea, I usually start with a vague direction—-some guiding points or principles. After that initial concept-driven stage, I often revisit my archive of other people’s ideas or products—-I’ve bookmarked a lot of interesting tweets and websites. I also like to talk to friends to spark more unexpected insights.”\n\nIn short, ideas often take shape through unconventional associations. The same insight is also brought up by Ethan Mollick in his book Co-Intelligence: Living and working with AI. However, these associated concepts don’t emerge from random combinations—-they require careful, thoughtful connections, and it’s both rare and challenging to pinpoint the most fitting ones. From my perspective, this is one of the most valuable opportunities for AI to assist—-helping uncover unexpected yet relevant connections that might otherwise be overlooked.\n\n\n\nAnother valuable insight came to me during a conversation with a friend who had worked on several development projects and gathered feedback from both web and mobile app launches. She shared her frustration about being initially excited about her idea, only to receive lukewarm user feedback. I could immediately relate—this is a common dilemma among developers, and I’ve faced similar situations multiple times.\nGiven AI’s strengths in pattern recognition and refining presentations, another thought crossed my mind: Why not use AI to help identify and validate product–market fit? This is something that’s often beyond the reach of individual intuition or experience, but AI—armed with a large dataset of successful and unsuccessful products—might be able to spot patterns and predict viability far more effectively.\nBuilding a dataset like that would be a major undertaking and likely more time-consuming than it seems, but I wanted to document this idea for future exploration.\n\n\n\nFinally, as we all know, AI is an incredibly helpful assistant when it comes to organizing and structuring information—not to mention refining language for things like idea pitches. Brainstorming can often feel chaotic and overwhelming, which is why organized note-taking is essential for making sense of seemingly scattered ideas.\nBut that raises more questions: How should these ideas be organized? In what format? There’s still a lot of detail worth exploring here, and it’s an area I plan to dig deeper into in the future."
  },
  {
    "objectID": "posts/013_post13/index.html#insights-from-the-conversations",
    "href": "posts/013_post13/index.html#insights-from-the-conversations",
    "title": "Collect feedback on the AI Ideation tool",
    "section": "",
    "text": "After speaking with several friends and classmates, I noticed some consistent patterns in how people approach ideation. These patterns offered valuable insights that pushed me to rethink the overall direction of the tool and start seriously considering product–market fit.\nInitially, my focus was solely on improving the quality of ideas generated by AI. After a few iterations, the LLM’s output did improve—especially in terms of novelty and feasibility. I also developed a standardized brainstorming workflow, where the LLM now offers clear, structured guidance to support human ideators and open up more creative directions.\nHowever, once I began talking to people, I quickly discovered a major challenge: there’s currently no strong product–market fit for this tool. Even though AI has the potential to expand possibilities and offer fresh perspectives, many human ideators either don’t trust the results—believing they’re too generic—or are highly confident in their own thinking and feel they don’t need AI assistance.\nAdmittedly, AI’s exceptional processing power does help broaden the scope of brainstorming and boost overall creativity. But the real challenge isn’t about pushing the technical capabilities further—-it’s about understanding human nature and figuring out how to frame this tool in a way that truly empowers human creativity. At this stage, the focus has shifted from expanding what the tool can do to deeply exploring how it fits into real creative workflows.\n\n\nTo get started, I want to talk about how an idea is formed. Regardless of the type of idea or the background of the person generating it, the core of an idea often hits us like a flash—Why not build a tool that frees our hands? Why not design a product that perfectly solves this pain point?\nAs someone who has spent a lot of time developing this ideation tool, I hate to admit it, but the truth is: founders and project enthusiasts rarely seek AI’s help during the early spark of an idea. That initial moment of inspiration feels personal, intuitive—even sacred—and many prefer to trust their own instincts rather than rely on a machine to ignite that spark.\nBut does that mean AI has lost its role in brainstorming? I don’t think so. Reflecting on both my own and my friend’s brainstorming experiences, we realized that we often rely on random external sources to reframe or elevate an idea—transforming it into something unusual, unconventional, or unique. The initial spark is undoubtedly precious, but what truly makes an idea stand out is its ability to bridge two seemingly unrelated concepts in an unexpected way. Here’s something my friend said during our conversation about brainstorming that really stuck with me:\n\n“When I need to come up with an idea, I usually start with a vague direction—-some guiding points or principles. After that initial concept-driven stage, I often revisit my archive of other people’s ideas or products—-I’ve bookmarked a lot of interesting tweets and websites. I also like to talk to friends to spark more unexpected insights.”\n\nIn short, ideas often take shape through unconventional associations. The same insight is also brought up by Ethan Mollick in his book Co-Intelligence: Living and working with AI. However, these associated concepts don’t emerge from random combinations—-they require careful, thoughtful connections, and it’s both rare and challenging to pinpoint the most fitting ones. From my perspective, this is one of the most valuable opportunities for AI to assist—-helping uncover unexpected yet relevant connections that might otherwise be overlooked.\n\n\n\nAnother valuable insight came to me during a conversation with a friend who had worked on several development projects and gathered feedback from both web and mobile app launches. She shared her frustration about being initially excited about her idea, only to receive lukewarm user feedback. I could immediately relate—this is a common dilemma among developers, and I’ve faced similar situations multiple times.\nGiven AI’s strengths in pattern recognition and refining presentations, another thought crossed my mind: Why not use AI to help identify and validate product–market fit? This is something that’s often beyond the reach of individual intuition or experience, but AI—armed with a large dataset of successful and unsuccessful products—might be able to spot patterns and predict viability far more effectively.\nBuilding a dataset like that would be a major undertaking and likely more time-consuming than it seems, but I wanted to document this idea for future exploration.\n\n\n\nFinally, as we all know, AI is an incredibly helpful assistant when it comes to organizing and structuring information—not to mention refining language for things like idea pitches. Brainstorming can often feel chaotic and overwhelming, which is why organized note-taking is essential for making sense of seemingly scattered ideas.\nBut that raises more questions: How should these ideas be organized? In what format? There’s still a lot of detail worth exploring here, and it’s an area I plan to dig deeper into in the future."
  },
  {
    "objectID": "posts/006_post6/index.html",
    "href": "posts/006_post6/index.html",
    "title": "Criteria for setting up an ideating environment",
    "section": "",
    "text": "I have been constantly reading papers and materials regarding engaging AI into creative tasks, including Co-intelligence by Ethan Mollick, The Innovation Tournament Handbook by Christian Terwiesch, and Using Large Language Models for Idea Generation in Innovation by Lennart Meincke, Karan Girotra, Gideon Nave, Christian Terwiesch, and Karl T. Ulrich. These resources have provided valuable insights, prompting me to develop my own guidelines for setting up an effective ideation environment.\nCurrently, when evaluating LLM-generated output, I rely on subjective judgment, selecting ideas that resonate most with me. However, I’ve realized that to help LLMs better understand the innovation process and the criteria for high-quality ideas, clearly defined and specialized rules and guidelines are essential.\nWorking with LLMs requires a balance of creativity and logic."
  },
  {
    "objectID": "posts/006_post6/index.html#what-can-be-considered-as-a-good-idea",
    "href": "posts/006_post6/index.html#what-can-be-considered-as-a-good-idea",
    "title": "Criteria for setting up an ideating environment",
    "section": "What can be considered as a GOOD idea?",
    "text": "What can be considered as a GOOD idea?\nTo teach GPT how to generate high-quality ideas, I first categorized AI-generated ideas into GOOD and BAD groups. Additionally, I created a third category containing GOOD human-generated ideas from renowned designers worldwide.\nI compiled the list of human-generated ideas by manually selecting product design concepts created by renowned designers, awarded prestigious design accolades, or achieving notable commercial success.\nFor AI-generated ideas, I employed various prompting strategies, including Chain-of-Thought, few-shot prompting, question-inspired prompting, attribute-driven prompting, and design methodology prompting to generate diverse innovative product concepts. Based primarily on subjective judgment, I selected 10 good ideas and 10 bad ideas from a pool of 100 AI-generated ideas.\n\nWith GPT’s assistance, I compared these groups to identify key differences. From this analysis, I distilled a list of characteristics unique to high-quality ideas and pinpointed common pitfalls that AI-generated ideas tend to fall into.\n\nWhat makes human-generated idea unique?\nAfter comparing ideas from different sources, I found out that those high-quality human-generated ideas still stand out among all ideas.\n\n1. Humans make connection: Explore linkages and metaphors beyond the surface\nOne differentiator between high-quality human-generated ideas and mediocre AI-generated ideas is that human draws meaningful connections by exploring deep linkages and metaphors between concepts. People might argue that: LLMs are connection machines! They must outperform humans in this aspect. I think this is only true from one perspective, that LLMs are good at producing connections on a surface level. Do these connections really help with idea generation? Only a small portion might be helpful. While most connections are just random mixtures of technology buzzwords and relevant terminologies. LLMs fail to see the deep connections between two seemingly irrelevant concepts.\nThis task can also be challenging even for a lot of humans, but it’s what sets great product designers and entrepreneurs apart. For example, a product concept designed to enhance focus and block distractions draws connections between focus sessions and immersive flights. While it would be difficult—though not entirely impossible—for LLMs to make this connection, humans grasp it intuitively. Flights naturally encourage focus since limited internet and signal access create an environment with fewer distractions.\n\n\n2. Humans feel: Appeal to emotions\nThe majority of high-quality human-generated ideas are deeply rooted in psychological needs—fear, anxiety, nostalgia, curiosity, and more. These needs serve as the core drivers behind product concepts. While LLMs can recognize keywords related to human emotions from their training data, generating ideas that genuinely resonate with these emotions remains a challenge. It is the sentimental depth of product ideas that celebrates humanity and creates concepts that truly connect with people.\nFor example, a human-generated idea involves a photo frame that fades over time. “Left untouched, the displayed photograph gradually blurs, mimicking the way memories fade over time. However, when the user touches the frame, the glass slowly clears, bringing the image back into focus. This design transforms nostalgia into a tactile experience, reinforcing the emotional act of remembering through interaction.\n\n\n3. Humans imagine: Contextualize languages with clear constraints\nWe imagine within reason, guided by clear constraints—we dance with shuckles on our feet. Humans intuitively understand the underlying meaning of words, linking them to real-world objects while considering their affordances and constraints. We can envision how objects function in different contexts, mentally simulate their combinations, and assess their feasibility before they even take shape. But LLMs sometimes generate ideas that are far from implementable. Technology buzzwords such as “AI-powered”, “AR-boosted”, “gesture-activated” can be seen almost everywhere.\nFor example, the haptic paper packaging designed by Kenya Hara, the art director of MUJI, perfectly leverages the affordances of paper–the diversity in its texture–to create packaging products. ” Each material—rough, smooth, embossed—corresponds to the product inside, allowing users to “read” the packaging through touch.”\n\nIn the next post, I’ll be sharing some insights on some common pitfalls that AI-generated ideas tend to fall into."
  },
  {
    "objectID": "posts/012_post12/index.html",
    "href": "posts/012_post12/index.html",
    "title": "Prompt engineering for LLM ideation Part 2",
    "section": "",
    "text": "Continuing from the last post, I’m going to complete and polish the prompt for the co-brainstorming session. In the previous post, I outlined the overall workflow of the ideation process and constructed a LangChain workflow diagram listing all the steps and different types of chain that are used. In this post, I’ll be working on refining the prompt.\n\n\n\nThe main goal here is to instruct the LLM to co-create product ideas by helping complete a mind map. As I explained earlier, a mind map is built around a network of interrelated concepts that connect and branch out from one another. After analyzing several innovation templates, I distilled the process into three key methods: 1. Identifying emotional root causes 2. Discovering deeper, non-obvious connections 3. Generating imaginary customer feedback.\nCompleting the mind map requires input from both the LLM and the human user. This challenge breaks down into two main tasks: One, instructing the LLM to generate concepts and keywords; two, enabling the LLM to respond to human input and expand on those branches. The first task is more complex but can be handled effectively with well-crafted instructions. The second task might seem simple—essentially just continuing a conversation—but it raises important considerations like how the LLM interprets user input and how to generate thoughtful, contextually relevant responses. These subtle challenges require more attention than they appear at first glance.\nFor now, I’ve decided to focus on the first task—-building a solid foundation by guiding the LLM to generate meaningful concepts and keywords.\nBuilding on my findings from previous posts—where I refined the prompt to help LLMs form more nuanced connections, think emotionally, and generate imaginary user personas—I’ve summarized each method into a clear set of steps.\nThere’s no way to know which prompt works best without testing it in a simulated ideation environment. So, I used the prompt: “Design a tool to help college students and young professionals with short attention spans focus more easily” as the base challenge. I worked with the LLM through the entire process—from problem statement generation to mind map creation.\nFor each trial, I tweaked the wording in the prompt to fine-tune the model’s responses and improve the output.\nAfter several runs, I was satisfied with the refined prompt. A fraction of the final prompt looks like this:\n\nThere are many different ways to structure the branches, we can use the following ones:\n\nEmotional Root Causes\n\nFollow these steps when doing this:\n\nEmotional Seeds\n\n“Identify 5 core emotional root causes that lead to the problem. What feelings (e.g., anxiety, longing, excitement) do people experience? Why do these emotions arise? Present them as bullet points with brief explanations (e.g., ‘fear of letting others down,’ ‘need for recognition’). Under each root cause, suggest a corresponding suggested direction the product may take.”\n\nHabit & Heuristic Alignment\n\n….\n\n\n\n\nUsing the same design challenge I mentioned above (“Design a tool to help college students and young professionals with short attention spans focus more easily”), I tested the prompt. Since the full response would be too long, GPT executed each step separately.\nHere’s a glimpse of the output.\n\n\n\nOverall, I was very satisfied with the results. The process provided me with a range of fresh perspectives and potential directions for my new product. However, a few weaknesses still need to be addressed moving forward:\n\nStandardized Output Format: Since this tool is intended to be developed as a web app, the output needs to follow a consistent format for easier integration and presentation. Currently, the outputs from different steps vary significantly in structure, creating unnecessary challenges when it comes to cleaning and organizing the content.\nFragmented Ideas and Lack of Cohesion: While the suggested directions and concepts are interesting and creative, they often feel fragmented. As a product designer, I found the tool most helpful when I already had a base idea and was looking for add-ons or enhancements. However, because these ideas are generated using different methodologies with little connection between them, it’s difficult to establish a common thread or cohesive foundation that sets the tone for the entire product.\n\nBased on these insights, I came up with a few changes to improve the process:\n\nUnder each step, I plan to add a small section called “sample format” to keep the output consistent. However, which format works best for users is still unclear. Since this ties closely to UX/UI design, I’ve decided to run a few user tests before finalizing it.\nThere are a couple of ways to address the fragmentation issue. One option is to use AI summarization to distill all potential directions into a few common threads, providing a quick reference for users to decide which path to explore. Another, less intrusive approach is to color-code similar product ideas or concepts, making it easier for users to spot patterns. Ultimately, this comes down to understanding how a product idea is formed and what kind of information the ideator needs during the process.\nOne final consideration is the length of the instructions. Although I don’t have a clear solution yet, I worry that overly long instructions might reduce the LLM’s efficiency and effectiveness. I still need to figure out whether it’s better to input everything at once or break the instructions into smaller chunks and feed them step by step. This will require further testing.\n\nI’m holding off on finalizing the Reminder section for now, as there are still some ambiguities around user needs. In the next post, I’ll share insights from conversations with students and creative professionals about their experiences with ideation."
  },
  {
    "objectID": "posts/012_post12/index.html#workflow-part-2",
    "href": "posts/012_post12/index.html#workflow-part-2",
    "title": "Prompt engineering for LLM ideation Part 2",
    "section": "",
    "text": "The main goal here is to instruct the LLM to co-create product ideas by helping complete a mind map. As I explained earlier, a mind map is built around a network of interrelated concepts that connect and branch out from one another. After analyzing several innovation templates, I distilled the process into three key methods: 1. Identifying emotional root causes 2. Discovering deeper, non-obvious connections 3. Generating imaginary customer feedback.\nCompleting the mind map requires input from both the LLM and the human user. This challenge breaks down into two main tasks: One, instructing the LLM to generate concepts and keywords; two, enabling the LLM to respond to human input and expand on those branches. The first task is more complex but can be handled effectively with well-crafted instructions. The second task might seem simple—essentially just continuing a conversation—but it raises important considerations like how the LLM interprets user input and how to generate thoughtful, contextually relevant responses. These subtle challenges require more attention than they appear at first glance.\nFor now, I’ve decided to focus on the first task—-building a solid foundation by guiding the LLM to generate meaningful concepts and keywords.\nBuilding on my findings from previous posts—where I refined the prompt to help LLMs form more nuanced connections, think emotionally, and generate imaginary user personas—I’ve summarized each method into a clear set of steps.\nThere’s no way to know which prompt works best without testing it in a simulated ideation environment. So, I used the prompt: “Design a tool to help college students and young professionals with short attention spans focus more easily” as the base challenge. I worked with the LLM through the entire process—from problem statement generation to mind map creation.\nFor each trial, I tweaked the wording in the prompt to fine-tune the model’s responses and improve the output.\nAfter several runs, I was satisfied with the refined prompt. A fraction of the final prompt looks like this:\n\nThere are many different ways to structure the branches, we can use the following ones:\n\nEmotional Root Causes\n\nFollow these steps when doing this:\n\nEmotional Seeds\n\n“Identify 5 core emotional root causes that lead to the problem. What feelings (e.g., anxiety, longing, excitement) do people experience? Why do these emotions arise? Present them as bullet points with brief explanations (e.g., ‘fear of letting others down,’ ‘need for recognition’). Under each root cause, suggest a corresponding suggested direction the product may take.”\n\nHabit & Heuristic Alignment\n\n…."
  },
  {
    "objectID": "posts/012_post12/index.html#give-it-a-try",
    "href": "posts/012_post12/index.html#give-it-a-try",
    "title": "Prompt engineering for LLM ideation Part 2",
    "section": "",
    "text": "Using the same design challenge I mentioned above (“Design a tool to help college students and young professionals with short attention spans focus more easily”), I tested the prompt. Since the full response would be too long, GPT executed each step separately.\nHere’s a glimpse of the output.\n\n\n\nOverall, I was very satisfied with the results. The process provided me with a range of fresh perspectives and potential directions for my new product. However, a few weaknesses still need to be addressed moving forward:\n\nStandardized Output Format: Since this tool is intended to be developed as a web app, the output needs to follow a consistent format for easier integration and presentation. Currently, the outputs from different steps vary significantly in structure, creating unnecessary challenges when it comes to cleaning and organizing the content.\nFragmented Ideas and Lack of Cohesion: While the suggested directions and concepts are interesting and creative, they often feel fragmented. As a product designer, I found the tool most helpful when I already had a base idea and was looking for add-ons or enhancements. However, because these ideas are generated using different methodologies with little connection between them, it’s difficult to establish a common thread or cohesive foundation that sets the tone for the entire product.\n\nBased on these insights, I came up with a few changes to improve the process:\n\nUnder each step, I plan to add a small section called “sample format” to keep the output consistent. However, which format works best for users is still unclear. Since this ties closely to UX/UI design, I’ve decided to run a few user tests before finalizing it.\nThere are a couple of ways to address the fragmentation issue. One option is to use AI summarization to distill all potential directions into a few common threads, providing a quick reference for users to decide which path to explore. Another, less intrusive approach is to color-code similar product ideas or concepts, making it easier for users to spot patterns. Ultimately, this comes down to understanding how a product idea is formed and what kind of information the ideator needs during the process.\nOne final consideration is the length of the instructions. Although I don’t have a clear solution yet, I worry that overly long instructions might reduce the LLM’s efficiency and effectiveness. I still need to figure out whether it’s better to input everything at once or break the instructions into smaller chunks and feed them step by step. This will require further testing.\n\nI’m holding off on finalizing the Reminder section for now, as there are still some ambiguities around user needs. In the next post, I’ll share insights from conversations with students and creative professionals about their experiences with ideation."
  },
  {
    "objectID": "posts/016_post16/index.html",
    "href": "posts/016_post16/index.html",
    "title": "Explore OpenAI 4o image generation model",
    "section": "",
    "text": "Picking up from where I left off in the last post, I started exploring the idea of integrating multi-modal output into the LLM ideation partner. In this post, I’ll be testing the capabilities of that feature to see whether it can meaningfully enhance text-based ideation by opening up new visual pathways for creativity.\nRight now, this feature is going viral online, largely because of its ability to generate images in the Studio Ghibli style. But generating visuals for innovative product ideas goes far beyond applying stylistic filters. It requires the language model to accurately interpret and visualize the details of potentially unconventional tools or concepts—many of which may not even exist in its training data.\nThat’s why I’ve decided to test it today—to better understand whether incorporating multi-modal output into my LLM application is a strategic and valuable move for supporting creative ideation.\n\n\n\nConclusion goes first: The GPT-4o model is surprisingly effective at generating highly detailed product images with accurate text. The key to this capability lies in crafting an extremely detailed image generation prompt—something that can also be efficiently produced within GPT-4o itself.\nThe trick lies in crafting a high-quality prompt beforehand. As long as all the necessary details are clearly specified, GPT-4o does an impressive job visualizing them accurately. That’s why ensuring the clarity and completeness of the prompt is the most essential part of the process.\nTo gain a more comprehensive understanding of this feature, I created a rubric for systematically testing the capabilities of GPT-4o’s image generation in the context of product visualization. For each attribute, I ran multiple tests, asking the LLM to generate high-quality images. I then scored each attribute and included notes with tips and observations for effectively using the tool.\nHere’s a snapshot of my rubric.\n\nHere are a few practice trial runs worth highlighting. My typical workflow looks like this: First, I ask GPT to generate a product idea (which will eventually be the result of human-AI collaboration in my product). Then, I prompt it to translate the idea into a detailed image generation prompt, making edits if needed. Finally, I ask GPT to generate the product image based on that prompt.\nI found this feature particularly effective for generating physical product visuals. Since most of us aren’t professional industrial designers—-and physical products tend to involve more visual variety than digital ones—-having GPT generate reference images is incredibly helpful for bringing ideas to life. Based on my trial runs, the results are visually impressive, as long as the prompt is well-crafted and pointed in the right direction.\nHere are a few examples of prompts and the corresponding images:\n\nDesign a cozy, emotionally expressive product called HeartLink Plush—a soft, huggable plush toy for international college students that simulates a loved one’s heartbeat. The plush is medium-sized, made with calming, textured fabric (like velvety or knitted material), in gentle tones such as pastel pink, soft gray, or lavender. It has a subtle heart-shaped glow area on its chest, gently pulsing with warm light to mimic a heartbeat. Next to the plush is a phone with a companion app on screen, showing a “connected” status with another plush across the globe. The setting is a dorm room at night, softly lit, with blankets, books, and maybe a cup of tea, evoking feelings of warmth, connection, and comfort during homesickness or study stress.\n\n\n\nDesign a sleek and modern wearable wristband called PulseBand, intended to help college students maintain focus while studying. The band is minimalist, made of soft silicone or woven fabric, in calming colors like matte black, sand beige, or slate blue. The wristband features a small, subtle LED indicator that glows gently during active focus sessions, and a tiny tactile pattern on the inside for discreet haptic feedback. Place the wristband on a student’s wrist as they study at a clean desk with a laptop, notebook, and a coffee nearby. A phone beside them shows the connected PulseBand app with “Deep Work Mode: 23 min remaining” on screen. The overall setting is calm and tech-savvy, emphasizing productivity and personal focus without distractions.\n\n\nWhat I also found fascinating was the accuracy in detail changes and the effectiveness of style transformations. After my spring break trip to Vienna, I asked GPT to generate a product idea for a Vienna-themed souvenir. Here’s the prompt it came up with:\n\nDesign a charming, photo-realistic souvenir product called “Sound of Vienna – Mini Pop-Up Stage.” The product is a flat, postcard-sized item that unfolds into a detailed, 3D paper pop-up scene. When opened, the stage reveals illustrated paper cutouts of a classical orchestra mid-performance—string musicians, a conductor, and a grand piano—set inside an ornate Viennese concert hall inspired by the Vienna State Opera.\nThe backdrop features intricate details: golden balconies, velvet red curtains, Baroque arches, and chandeliers. At the base of the stage is a small button that plays a 10-second snippet of classical music (e.g., Strauss’s Blue Danube), with a small speaker subtly embedded into the cardboard.\nThe overall aesthetic is elegant yet whimsical. The color palette includes warm golds, soft creams, deep reds, and touches of hand-drawn texture. Show the item halfway opened on a wooden table, with another folded version beside it in a flat protective sleeve that reads “Sound of Vienna – Strauss Edition.” Nearby, place a hand or travel bag to suggest how easily it fits into luggage.\nEmphasize portability, craftsmanship, and the magical storytelling of Vienna’s musical legacy in a physical keepsake.\n\nIntially this is what GPT gave me: \nI was not very satisfied with the style, wanting the postcard to be more three-dimensional. So I used another image as a reference:\n\nThis time, GPT did an extremely well job in replicating the style of the postcard without changing too much of the original design.\n\nTo continue experimenting with the style transformation feature, I asked GPT to render the postcard design in the style of a Monet painting—and here’s what it produced:\n\nI can further prompt it to make it more 3D, but I believe these examples should be sufficient to demonstrate the powerful capability of 4o Image Gen.\n\n\nAlthough the GPT-4o model does a solid job generating high-quality product images, the resulting design may not always align perfectly with the user’s expectations. If the user has specific ideas about the structure, material, shape, or style of the product, it’s important to state those details explicitly in the prompt. This helps ensure the model stays faithful to the original vision.\nAs a result, this process requires more intentionality in prompt engineering, especially when describing physical products for image generation. The clearer and more detailed the description, the more accurate and useful the output will be.\n\n\n\n\nA more specific feature I’d like to develop in this product is the ability to generate images that merge two or more seemingly unrelated concepts. Many innovative product ideas are born from conceptual recombination, and I believe adding visual stimulation to this process could significantly boost creativity. To explore this, I selected several pairs of distant or contrasting concepts, and challenged the LLM to combine them into coherent product ideas, which were then visualized through image generation.\nConclusion goes first: After several trials, I was genuinely impressed by GPT-4o’s ability to integrate distant concepts into compelling product prototypes. Its text generation feature makes it incredibly easy to combine abstract or unrelated ideas, and the image generation feature faithfully visualizes the described details, even when the object isn’t something we’d typically encounter in real life. However, physical products still have a better image quality than digital products in terms of its plausibility and design.\nNext, I asked GPT to generate the image prompt, which was then used to create the final visual. The overall vibe of the image aligns well with the product concept, and I’m quite satisfied with the level of detail it was able to capture.\n\nNext, I asked GPT to generate the image prompt, which was then used to create the final visual. The overall vibe of the image aligns well with the product concept, and I’m quite satisfied with the level of detail it was able to capture.\n\nThat wasn’t quite enough—I wanted to make quick design changes to test the consistency of the image generation. To start, I prompted it to change the texture to transparent frosted glass. It did a great job! The result was both visually consistent and true to the updated prompt.\n\nFinally, I asked GPT to add some “puzzle” elements to make the product more interactive and playful. While the resulting concept made less practical sense to me—the structure felt somewhat arbitrary and lacked a clear functional rationale—I was still impressed by the image’s visual consistency and aesthetic appeal. To fix the problem of implausible design, refining the prompt itself would likely be a more effective solution."
  },
  {
    "objectID": "posts/016_post16/index.html#testing-the-affordances-and-constraints-of-4o-image-gen",
    "href": "posts/016_post16/index.html#testing-the-affordances-and-constraints-of-4o-image-gen",
    "title": "Explore OpenAI 4o image generation model",
    "section": "",
    "text": "Picking up from where I left off in the last post, I started exploring the idea of integrating multi-modal output into the LLM ideation partner. In this post, I’ll be testing the capabilities of that feature to see whether it can meaningfully enhance text-based ideation by opening up new visual pathways for creativity.\nRight now, this feature is going viral online, largely because of its ability to generate images in the Studio Ghibli style. But generating visuals for innovative product ideas goes far beyond applying stylistic filters. It requires the language model to accurately interpret and visualize the details of potentially unconventional tools or concepts—many of which may not even exist in its training data.\nThat’s why I’ve decided to test it today—to better understand whether incorporating multi-modal output into my LLM application is a strategic and valuable move for supporting creative ideation.\n\n\n\nConclusion goes first: The GPT-4o model is surprisingly effective at generating highly detailed product images with accurate text. The key to this capability lies in crafting an extremely detailed image generation prompt—something that can also be efficiently produced within GPT-4o itself.\nThe trick lies in crafting a high-quality prompt beforehand. As long as all the necessary details are clearly specified, GPT-4o does an impressive job visualizing them accurately. That’s why ensuring the clarity and completeness of the prompt is the most essential part of the process.\nTo gain a more comprehensive understanding of this feature, I created a rubric for systematically testing the capabilities of GPT-4o’s image generation in the context of product visualization. For each attribute, I ran multiple tests, asking the LLM to generate high-quality images. I then scored each attribute and included notes with tips and observations for effectively using the tool.\nHere’s a snapshot of my rubric.\n\nHere are a few practice trial runs worth highlighting. My typical workflow looks like this: First, I ask GPT to generate a product idea (which will eventually be the result of human-AI collaboration in my product). Then, I prompt it to translate the idea into a detailed image generation prompt, making edits if needed. Finally, I ask GPT to generate the product image based on that prompt.\nI found this feature particularly effective for generating physical product visuals. Since most of us aren’t professional industrial designers—-and physical products tend to involve more visual variety than digital ones—-having GPT generate reference images is incredibly helpful for bringing ideas to life. Based on my trial runs, the results are visually impressive, as long as the prompt is well-crafted and pointed in the right direction.\nHere are a few examples of prompts and the corresponding images:\n\nDesign a cozy, emotionally expressive product called HeartLink Plush—a soft, huggable plush toy for international college students that simulates a loved one’s heartbeat. The plush is medium-sized, made with calming, textured fabric (like velvety or knitted material), in gentle tones such as pastel pink, soft gray, or lavender. It has a subtle heart-shaped glow area on its chest, gently pulsing with warm light to mimic a heartbeat. Next to the plush is a phone with a companion app on screen, showing a “connected” status with another plush across the globe. The setting is a dorm room at night, softly lit, with blankets, books, and maybe a cup of tea, evoking feelings of warmth, connection, and comfort during homesickness or study stress.\n\n\n\nDesign a sleek and modern wearable wristband called PulseBand, intended to help college students maintain focus while studying. The band is minimalist, made of soft silicone or woven fabric, in calming colors like matte black, sand beige, or slate blue. The wristband features a small, subtle LED indicator that glows gently during active focus sessions, and a tiny tactile pattern on the inside for discreet haptic feedback. Place the wristband on a student’s wrist as they study at a clean desk with a laptop, notebook, and a coffee nearby. A phone beside them shows the connected PulseBand app with “Deep Work Mode: 23 min remaining” on screen. The overall setting is calm and tech-savvy, emphasizing productivity and personal focus without distractions.\n\n\nWhat I also found fascinating was the accuracy in detail changes and the effectiveness of style transformations. After my spring break trip to Vienna, I asked GPT to generate a product idea for a Vienna-themed souvenir. Here’s the prompt it came up with:\n\nDesign a charming, photo-realistic souvenir product called “Sound of Vienna – Mini Pop-Up Stage.” The product is a flat, postcard-sized item that unfolds into a detailed, 3D paper pop-up scene. When opened, the stage reveals illustrated paper cutouts of a classical orchestra mid-performance—string musicians, a conductor, and a grand piano—set inside an ornate Viennese concert hall inspired by the Vienna State Opera.\nThe backdrop features intricate details: golden balconies, velvet red curtains, Baroque arches, and chandeliers. At the base of the stage is a small button that plays a 10-second snippet of classical music (e.g., Strauss’s Blue Danube), with a small speaker subtly embedded into the cardboard.\nThe overall aesthetic is elegant yet whimsical. The color palette includes warm golds, soft creams, deep reds, and touches of hand-drawn texture. Show the item halfway opened on a wooden table, with another folded version beside it in a flat protective sleeve that reads “Sound of Vienna – Strauss Edition.” Nearby, place a hand or travel bag to suggest how easily it fits into luggage.\nEmphasize portability, craftsmanship, and the magical storytelling of Vienna’s musical legacy in a physical keepsake.\n\nIntially this is what GPT gave me: \nI was not very satisfied with the style, wanting the postcard to be more three-dimensional. So I used another image as a reference:\n\nThis time, GPT did an extremely well job in replicating the style of the postcard without changing too much of the original design.\n\nTo continue experimenting with the style transformation feature, I asked GPT to render the postcard design in the style of a Monet painting—and here’s what it produced:\n\nI can further prompt it to make it more 3D, but I believe these examples should be sufficient to demonstrate the powerful capability of 4o Image Gen.\n\n\nAlthough the GPT-4o model does a solid job generating high-quality product images, the resulting design may not always align perfectly with the user’s expectations. If the user has specific ideas about the structure, material, shape, or style of the product, it’s important to state those details explicitly in the prompt. This helps ensure the model stays faithful to the original vision.\nAs a result, this process requires more intentionality in prompt engineering, especially when describing physical products for image generation. The clearer and more detailed the description, the more accurate and useful the output will be.\n\n\n\n\nA more specific feature I’d like to develop in this product is the ability to generate images that merge two or more seemingly unrelated concepts. Many innovative product ideas are born from conceptual recombination, and I believe adding visual stimulation to this process could significantly boost creativity. To explore this, I selected several pairs of distant or contrasting concepts, and challenged the LLM to combine them into coherent product ideas, which were then visualized through image generation.\nConclusion goes first: After several trials, I was genuinely impressed by GPT-4o’s ability to integrate distant concepts into compelling product prototypes. Its text generation feature makes it incredibly easy to combine abstract or unrelated ideas, and the image generation feature faithfully visualizes the described details, even when the object isn’t something we’d typically encounter in real life. However, physical products still have a better image quality than digital products in terms of its plausibility and design.\nNext, I asked GPT to generate the image prompt, which was then used to create the final visual. The overall vibe of the image aligns well with the product concept, and I’m quite satisfied with the level of detail it was able to capture.\n\nNext, I asked GPT to generate the image prompt, which was then used to create the final visual. The overall vibe of the image aligns well with the product concept, and I’m quite satisfied with the level of detail it was able to capture.\n\nThat wasn’t quite enough—I wanted to make quick design changes to test the consistency of the image generation. To start, I prompted it to change the texture to transparent frosted glass. It did a great job! The result was both visually consistent and true to the updated prompt.\n\nFinally, I asked GPT to add some “puzzle” elements to make the product more interactive and playful. While the resulting concept made less practical sense to me—the structure felt somewhat arbitrary and lacked a clear functional rationale—I was still impressed by the image’s visual consistency and aesthetic appeal. To fix the problem of implausible design, refining the prompt itself would likely be a more effective solution."
  },
  {
    "objectID": "posts/016_post16/index.html#conclusion",
    "href": "posts/016_post16/index.html#conclusion",
    "title": "Explore OpenAI 4o image generation model",
    "section": "Conclusion",
    "text": "Conclusion\nOverall, I’m quite satisfied with the capabilities demonstrated by GPT-4o’s Image Generation, and I believe it will be a valuable addition to my ideation tool. In the next post, I’ll start bringing everything together and begin writing some actual code to integrate the components."
  }
]